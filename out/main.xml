<?xml version="1.0" encoding="UTF-8"?>

<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" lang="es">
<info>
<title>Geolocalización de usuarios en medios sociales mediante análisis de contenidos</title>
<date>2014-06-05</date>
<author>
<personname>
<firstname>Sergio</firstname>
<othername>Álvarez</othername>
<surname>Suárez</surname>
</personname>
</author>
<authorinitials>SÁS</authorinitials>
</info>
<preface xml:id="_resumen">
<title>Resumen</title>
<simpara>Aquí el resumen en español.</simpara>
<simpara><?asciidoc-pagebreak?></simpara>
</preface>
<preface xml:id="_abstract">
<title>Abstract</title>
<simpara>Here the abstract in english.</simpara>
</preface>
<preface xml:id="_agradecimientos">
<title>Agradecimientos</title>
<simpara>Aquí los agradecimientos.</simpara>
</preface>
<chapter xml:id="_introducción">
<title>Introducción</title>
<section xml:id="_motivación_del_proyecto">
<title>Motivación del proyecto</title>
<simpara>Aquí la motivación del proyecto</simpara>
</section>
<section xml:id="_alcance">
<title>Alcance</title>
<simpara>Aquí el alcance del proyecto.</simpara>
</section>
<section xml:id="_estado_del_arte">
<title>Estado del arte</title>
<simpara>El crecimiento exponencial de las redes sociales durante los últimos años ha despertado un gran interés en los diferentes ámbitos de la informática, siendo un claro objetivo comercial para profesionales del sector, así como un nuevo campo de investigación para los investigadores universitarios.</simpara>
<simpara>Como consecuencia de todo ello, durante los últimos años han ido apareciendo diversas aplicaciones que, de una u otra manera, se centran en estudiar ciertos aspectos de las redes sociales para poder extraer información acerca de sus usuarios gracias a las diversas publicaciones que estos mismos realizan en sus perfiles.</simpara>
<simpara>El estudio de la geolocalización de un usuario a partir de su contenido, sin embargo, es una de las pocas áreas que <emphasis>tan sólo</emphasis> agrupa un pequeño número de estudios teóricos, pero en donde no han proliferado herramientas que comprueben de manera empírica los resultados teóricos emitidos por diversos investigadores.</simpara>
<simpara>Por ello, en este capítulo se recopilan algunos estudios teóricos y artículos de investigación que han servido como punto de arranque para este proyecto.</simpara>
<section xml:id="_estudios_y_artículos_de_investigación">
<title>Estudios y artículos de investigación</title>
<simpara>Se pueden encontrar varias tendencias en la investigación de la geolocalización de los tuits y usarios de Twitter. Por un lado, existen investigadores que basan sus estudios en encontrar términos o entidades con significado geográfico propio, que se puedan cotejar con un gazetteer<footnote><simpara>Conjunto de nombres geográficos que, junto con un mapa, constituye una importante referencia sobre lugares y sus nombres</simpara></footnote> con el objetivo de poder identificar nombres de lugares o coordenadas geográficas, intuyendo que aquellas que más veces aparezcan en el <emphasis>timeline</emphasis> del usuario son aquellas más próximas a su localización real.</simpara>
<simpara>Por otro lado, grupos de investigadores han optado por estudiar únicamente el contenido de un tuit con el objetivo de encontrar aquellas palabras más características de un lugar y que puedan ser lo suficientemente discriminativas. En estos casos, se suele tender a buscar aquellos términos con una alta frecuencia y una baja dispersión geográfica.</simpara>
<simpara>Existen también enfoques donde se ha optado por una estrategia basada en estudiar el comportamiento de un usuario, utilizando técnicas de los dos métodos anteriores y combinándola con la información obtenida a través de servicios de terceros que puedan realizar publicaciones en el <emphasis>timeline</emphasis> del usuario y además adjuntar información geolocalizada (<emphasis role="strong">Foursquare</emphasis> sería un claro ejemplo de este tipo de servicios).</simpara>
<section xml:id="_tweets_from_justin_bieber_s_heart_the_dynamics_of_the_location_field_in_user_profiles">
<title>Tweets from Justin Bieber’s Heart: The Dynamics of the "Location" Field in User Profiles</title>
<simpara><emphasis>Por Brent Hecht et al. Northwestern University y Palo Alto Research Center</emphasis></simpara>
<simpara>El estudio liderado por Brent Hecht demuestra de manera científica como el uso del campo de Localización disponible en los perfiles de los usuarios en Twitter no es un indicador válido para obtener su posición geográfica real.</simpara>
<simpara>Entre los datos obtenidos por Brent y su equipo se pueden destacar los siguientes:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Sólo un 66% de los usuarios utiliza el campo de Localización para aportar información geográfica válida (es decir, localizaciones reales pero que <emphasis role="strong">no tienen porqué indicar su situación real</emphasis>. Por ejemplo, si una persona de <emphasis role="strong">Oviedo</emphasis> escribe <emphasis role="strong">California</emphasis> en el campo de Localización de su perfil, este estudio lo incluye dentro del 66% anterior).</simpara>
</listitem>
<listitem>
<simpara>Algunos de los usos que los usuarios dentro del 34% restante aplican al campo de Localización se puede observar en <xref linkend="location-use-type"/>.</simpara>
</listitem>
<listitem>
<simpara>La manera de distinguir entre localización real y ficticia supuso de un proceso manual en el que dos miembros del equipo debieron revisar tuit a tuit el campo de Localización, debido a la habilidad de los usuarios para poder expresar sarcasmo o ironía, así como expresiones comunes que pueden tener asociado un componente geográfico inherente (un ejemplo puede ser referirse al Principado de Asturias como <emphasis>la tierrina</emphasis>).</simpara>
</listitem>
</orderedlist>
<table xml:id="location-use-type" frame="all" rowsep="1" colsep="1">
<title>Tipos de uso del campo de Localización por parte de los usuarios de Twitter</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Cultura popular</simpara></entry>
<entry align="left" valign="top"><simpara>12.9%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Referencias a su propia privacidad</simpara></entry>
<entry align="left" valign="top"><simpara>1.2%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Insultos o contenido violento</simpara></entry>
<entry align="left" valign="top"><simpara>4.6%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Localizaciones no terráqueas</simpara></entry>
<entry align="left" valign="top"><simpara>5.0%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Emociones negativas hacia su localización real</simpara></entry>
<entry align="left" valign="top"><simpara>3.2%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Naturaleza sexual</simpara></entry>
<entry align="left" valign="top"><simpara>3.2%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>Como alternativa a los resultados anteriores, y buscando una manera de automatizar el proceso y encontrar resultados más fiables y exhaustivos, se propusieron hacer un primer experimento para comprobar si el estudio de los contenidos publicados por un usuario pueden aportar la información necesaria para permitir inferir su ubicación geográfica.</simpara>
<simpara>Para ello, utilizaron un software de aprendizaje automático y un clasificador Bayesiano multinomial que en base a un conjunto de datos obtenidos a partir de aplicar el algoritmo <emphasis>CALGARI</emphasis> (de implementación propia), fuese capaz de predecir a qué área (País y Estado) pertenece un tuit en base a su contenido.</simpara>
<note>
<title>CALGARI</title>
<simpara>El algoritmo CALGARI tiene como objetivo normalizar la frecuencia con la que un término ha aparecido dentro de un dataset de tuits para priorizar aquellos que son más específicas de un área (ciudad o estado) en concreto, penalizando palabras comunes como <emphasis>ya, hola, adiós, etc.</emphasis></simpara>
</note>
<simpara>Entre los resultados ofrecidos por el estudio destacan un <emphasis role="strong">72.7% de acierto para inferir el país</emphasis> de un usuario pero tan <emphasis role="strong">sólo un 30% de acierto a nivel de estado</emphasis>.</simpara>
<simpara>Como apunte adicional, es interesante una afirmación que se enuncia en el artículo cuando se hace referencia al nuevo campo de localización que la API Streaming de Twitter ofrece para adjuntar información geolocalizada (siempre que el usuario lo permita):</simpara>
<blockquote>
<simpara>First, our focus is on the geographic information revealed in the “location” field of user profiles, a type of geographic information that is prevalent across the Web 2.0 world. <emphasis role="strong">Second, we found that only 0.77% of our 62 million tweets contained this embedded location information</emphasis>.</simpara>
</blockquote>
<simpara>De 62 millones de tuits, únicamente un 0.77% (~= 477.400) contenían información geográfica adjunta.</simpara>
</section>
<section xml:id="_where_is_this_tweet_from_inferring_home_locations_of_twitter_users">
<title>Where Is This Tweet From? Inferring Home Locations of Twitter Users</title>
<simpara><emphasis>Por Jalal Mahmud et al. IBM Research</emphasis></simpara>
<simpara>Con el objetivo de poder identificar un tuit a diferentes granularidades: ciudad o estado, el estudio plantea la posibilidad de analizar tres tipos de términos diferentes para localizar una publicación en Twitter:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><emphasis role="strong">Palabras</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Hashtags</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Nombres de lugares</emphasis> (utilizando un gazetteer geográfico). Puesto que estos términos podía estar compuestos por más de una palabra, se utilizaron bigramas y trigamas, así como un heurístico especializado en reconocer nombres de lugares expresados mediante vocabulario común (un ejemplo sería <emphasis>Red Sox</emphasis> para referirse a la ciudad de Boston).</simpara>
</listitem>
</orderedlist>
<simpara>Es interesante observar como empiezan a aparecer pequeñas diferencias entre términos, considerando que en función de su categoría, pueden ofrecer más o menos información geográfica. Esta misma estrategia será también utilizada en el presente proyecto, mediante la extracción de Hashtags, Menciones y N-gramas.</simpara>
<simpara>Con el objetivo de minimizar la aparición de ruido, normalizaron el contenido de cada tuit eliminando signos de puntuación (a excepción de aquellos que indican una entidad propia cuando se encuentran al principio de una palabra, como <literal>#</literal> para indicar <emphasis>hashtags</emphasis>) y palabras vacías.</simpara>
<simpara>También se hace mención a la utilización de un <emphasis role="strong">software de aprendizaje automático</emphasis>, en este caso WEKA, y su conjunción con un modelo estadístico que realice los cálculos necesarios para el clasificador. El modelo que seleccionaron de manera empírica fue un clasificador Bayesiano multinomial.</simpara>
<simpara>La estrategia propuesta en este trabajo para inferir la localización de un usuario en Twitter fue:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>A lo largo de sus tuits, mencionará más veces su ciudad o estado de origen que el resto de ciudades o estados.</simpara>
</listitem>
<listitem>
<simpara>Visitará más lugares de su ciudad o estado de origen que del resto de ciudades o estados (para detectar este tipo de visitas, se guardan todas las URLs generadas a partir de <emphasis>check-ins</emphasis> compartidos a través de <emphasis role="strong">Foursquare</emphasis> para luego comprobar su información asociada a través de la propia API de Foursquare).</simpara>
</listitem>
</orderedlist>
<simpara>A partir de estas premisas y de las decisiones anteriores, se crearon 3 modelos diferentes para poder entrenar sobre cada uno de los términos que se quieren extraer: palabras, hashtags y nombres de lugares. Los resultados presentados a nivel de ciudad no fueron realmente positivos, y sólo presentan niveles de precisión superiores al 70% cuando se permiten márgenes de error superiores a 200 millas (~= 322 kilómetros).</simpara>
<simpara>Por último, no se especifica con exactitud cómo actúa realmente el algoritmo cuando se trabaja con usuarios que no tienen contenido generado por Foursquare o no hacen una referencia explícita a su ciudad, estado o país.</simpara>
</section>
<section xml:id="_tweolocator_a_non_intrusive_geographical_locator_system_for_twitter">
<title>TweoLocator: A Non-Intrusive Geographical Locator System for Twitter</title>
<simpara><emphasis>Por Yi-Shin Chen et al. National Tsing Hua University</emphasis></simpara>
<simpara>En este estudio, Yi-Shin Chen diseña un sistema que a través de diferentes etapas y aglutinando varios procesos es capaz de inferir la localización de un usuario en Twitter en función del contenido de sus tuits.</simpara>
<variablelist>
<varlistentry>
<term>Baseline Classification</term>
<listitem>
<simpara>A partir de un gran dataset de usuarios de Twitter, en esta fase se realiza un análisis para comprobar qué perfiles puede ser potencialmente válidos para realizar un análisis de contenidos, eliminando aquellos que puedan pertenecer a <emphasis>bots</emphasis> automáticos o sean perfiles de spam. Una vez se obtiene una masa de usuarios válidos se procede, dentro aún de esta etapa, a analizar todos sus tuits (a excepción de aquellos con información de geolocalización asociada) para volver a categorizarlos en 3 tipos:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Direct subject</emphasis>: Tuits que hacen referencia al usuario en primera persona.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Anonymous subject</emphasis>: Tuits que no hacen una referencia directa al usuario, pero utilizan otros pronombres personales o la primera secuencia de palabras es un verbo que no es una palabra vacía.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Others</emphasis>: Tuits descartados por no pertenecer a ninguna de las 2 categorías anteriores.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Rule Generation</term>
<listitem>
<simpara>Una vez todos los tuits anteriores han sido analizados semánticamente se realiza una normalización de los mismos aplicando técnicas de análisis de texto (utilizando un tokenizador y un stemmer) para luego poder formar n-gramas como los mismos. Durante esta etapa, se intentan inferir reglas que permitan asociar términos comunes a localizaciones específicas como aeropuertos, parques, estaciones de tren, etc.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Location Discovery</term>
<listitem>
<simpara>A partir de los términos de cada tuit, se generan trigramas, bigramas y unigramas y se comparan sobre un gazetteer y las reglas generadas en el paso anterior, obteniendo localizaciones que se pueden agrupar en:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Explicit Specific</emphasis>: Nombres que hacen una referencia directa a una ciudad o lugar determinado, como por ejemplo «The White House» or «Los Angeles».</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Explicit</emphasis>: Nombres que hacen referencia a localizaciones generales como parques o gimnasios.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Implicit</emphasis>: Combinaciones de palabras que implícitamente sugieren una localización. Estos resultaos se obtienen a partir de las reglas generadas en el paso anterior.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Toponym Removal</term>
<listitem>
<simpara>Mediante la utilización de un clúster, y partiendo de la premisa de que un usuario nombrará con mayor frecuencia lugares cercanos a su lugar de origen, en esta fase se analizan las menciones realizadas por el usuario sobre ciudades, lugares, países y se refinarán los datos para obtener su lugar de origen.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Timeline Sorting</term>
<listitem>
<simpara>Es el último paso en el refinamiento de los datos. En esta fase se intenta minimizar la aparición de ruido detectando aquellas ocasiones en las que el usuario hace referencia a una localización geográfica sin aportar una información real acerca de su posición. Por ejemplo, es habitual que alguien situado en Asturias pueda nombrar la ciudad de Nueva York para hablar de alguna noticia o para mostrar sus ganas por conocer la ciudad, sin que esa mención indique que se encuentre realmente allí. Para resolver este problema, y aceptando que en algunos casos sólo se podrían resolver dichas inconsistencias de manera manual mediante la intervención humana, se diseñó un sistema que a partir de dos tuits con contenido geolocalizado consecutivos (del mismo usuario) compruebe si su diferencia en el tiempo es acorde a la posibilidad de haberse movido entre ambos puntos a una velocidad normal de transporte.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Location Inferred</term>
<listitem>
<simpara>De acuerdo a los resultados obtenidos en todas las fases anteriores y de acuerdo al nivel sobre que el que se haya podido inferir su localización, los usuarios son clasificados en los siguientes grupos:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">No information</emphasis>: Si no se ha podido obtener información geográfica válida para inferir la localización del usuario.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Just country</emphasis>: Si sólo se ha podido inferir el país del usuario.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Timeline</emphasis>: Se han podido detectar ubicaciones actuales y previas del usuario, pero no su lugar de origen.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Hometown</emphasis>: Se han podido detectar ubincaciones actuales y previas del usuario y <emphasis role="strong">también</emphasis> su lugar de origen. Es el grupo con información más completa.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<simpara>En las conclusiones que se exponen en el artículo se muestran unos resultados bastante aceptables, donde hay porcentajes de acierto cercanos al 80%. Al igual que en el caso anterior, TweoLocator tiene una gran dependencia de que los usuarios incluyan en el contenido de sus tuits información explícitamente geolocalizable.</simpara>
</section>
<section xml:id="_a_multi_indicator_approach_for_geolocalization_of_tweets">
<title>A Multi-Indicator Approach for Geolocalization of Tweets</title>
<simpara><emphasis>Por Axel Schulz et al. SAP Research</emphasis></simpara>
<simpara>En este artículo, un equipo de investigación de <emphasis role="strong">SAP AG</emphasis> presenta un enfoque muy interesante para inferir la localización de un usuario mediante la utilización de formas poligonales en 3D. Los polígonos se superponen, y la intersección de mayor altura es el área con más probabilidades de contener al usuario analizado.</simpara>
<simpara>La altura de cada polígono viene determinada por pesos específicos que se aplican en función de la fuente utilizada para obtener esa localización. Cada fuente tiene sus propios estándares de calidad y sus propias métricas para indicar más o menos fiabilidad.</simpara>
<simpara>Para obtener las coordenadas o posiciones geográficas que deben ocupar los polígonos, los investigadores extraen información de los siguientes campos:</simpara>
<variablelist>
<varlistentry>
<term>Contenido del tuit</term>
<listitem>
<simpara>Se optó por utilizar <emphasis role="strong">DBPedia Spotlight</emphasis> para extraer las entidades que existían en el tuit. Con los resultados de la extracción, se seleccionaban únicamente aquellas que tenían coordenadas asociadas. Además, se utilizó como calidad de cada predicción la propia confianza aportada por DBPedia Spotlight en su resultado. También se utilizaron las publicaciones realizadas a través de servicios como Foursquare, Flickr o Ubisoft, las cuales tienen adjunta información geográfica precisa mediante la utilización de coordenadas geográficas.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Localización</term>
<listitem>
<simpara>Se hizo uso de gazetteers que permitiesen buscar coincidencias textuales en el campo de Localización. Además, se volvió a utilizar DBPedia Spotlight para conseguir trabajar con expresiones comunes como «La gran manzana» y expresiones regulares para detectar si algún usuario incluía coordenadas geográficas directamente en su campo de Localización.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Web del usuario</term>
<listitem>
<simpara>Para aquellos usuarios que añaden en su perfil su página web personal se aplican dos estrategias:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Extraer el dominio de la página (.com, .es, etc).</simpara>
</listitem>
<listitem>
<simpara>Utilizar la dirección IP y obtener las coordenadas a través del servicio IPInfoDB.</simpara>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Zona horaria</term>
<listitem>
<simpara>Se asume como cierto que la zona horaria asociada al usuario es la capital de su país de origen.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Los resultados de este estudio presentan mejorías respecto a otras investigaciones basadas en inferir la localidad de un usuario mediante el uso de <emphasis role="strong">múltiples indicadores</emphasis> con un 37% de acierto con una distancia de error de 10km y un 48% para 25km; así como un 54% cuando el margen se amplía a 50km.</simpara>
</section>
<section xml:id="_inferring_the_origin_locations_of_tweets_with_quantitative_confidence">
<title>Inferring the Origin Locations of Tweets with Quantitative Confidence</title>
<simpara><emphasis>Por Reid Priedhorsky et al. Los Alamos National Laboratory y Northeastern Illinois University</emphasis></simpara>
<simpara>El artículo parte de la premisa de que no es posible obtener la localización de un tuit con una exactitud total, si no que lo más acertado es ofrecer un modelo probabilístico que muestre las diferentes localizaciones a las que un tuit puede pertenecer asociadas a un grado de confianza (probabilidad).</simpara>
<simpara>Para obtener un dataset de entrenamiento, se utilizó la API Streaming de Twitter para después realizar un procesamiento de cada tuit extrayendo información de los campos: descripción del usuario, idioma del perfil seleccionado, campo de localización, zona horaria y contenido del tuit. Sobre esta información, se extrajeron bigramas para todos los términos adyacentes (a excepción del campo de zona horaria). Además, también se almacenó la información geográfica adjunta al tuit para poder realizar los experimentos y el entrenamiento del modelo.</simpara>
<simpara>Un ejemplo de la extracción de bigramas sería:</simpara>
<blockquote>
<simpara>Obviamente todo esto se hace para acabar con la costumbre de nuestra infancia de los álbumes de cromos.</simpara>
</blockquote>
<simpara>Que se traduciría a:</simpara>
<blockquote>
<simpara>Obviamente todo, todo esto, esto se, se hace, hace para, para acabar, acabar con, con la, la costumbre, costumbre de, de nuestra, nuestra infancia, infancia de, de los, los álbumes, álbumes de, de cromos.</simpara>
</blockquote>
<simpara>Una vez con toda esta información almacenada, se utilizó una técnica de estimación denominada «gaussian mixture models» en donde cada bigrama que aparezca más de un mínimo número de veces se asocia a las coordenadas del tuit que lo contiene. Cada asociación, va vinculada a un peso específico en función del bigrama y la suma de todos los pesos asociados a un tuit es su probabilidad total de pertenecer a esas coordenadas.</simpara>
<simpara>Para calcular el peso que se le debe dar a cada a cada bigrama, el equipo de Reid Priedhorsky desarrolló tres métodos diferentes:</simpara>
<itemizedlist>
<listitem>
<simpara>Peso por propiedades de calidad</simpara>
</listitem>
<listitem>
<simpara>Peso por error inverso</simpara>
</listitem>
<listitem>
<simpara>Peso por optimización</simpara>
</listitem>
</itemizedlist>
<simpara>Todos ellos con una alta dosis de componente algorítmico y matemático.</simpara>
<simpara>Los resultados del estudio revelaron un acierto del 83% para aquellos tuits que contenían bigramas con contenido explícitamente localizable (nombres de lugares) frente a un 57% de acierto sobre tuits sin información geográfica.</simpara>
</section>
<section xml:id="_you_are_where_you_tweet_a_content_based_approach_to_geo_locating_twitter_users">
<title>You Are Where You Tweet: A Content-Based Approach to Geo-locating Twitter Users</title>
<simpara><emphasis>Por Zhiyuan Cheng et al. Texas A&amp;M University</emphasis></simpara>
<simpara>En este estudio, Zhiyuan Cheng y su equipo aportan datos interesantes en la investigación de la geolocalización en Twitter. Centrados en buscar un algoritmo capaz de inferir la localización de un usuario <emphasis role="strong">únicamente</emphasis> con el contenido de sus tuits, establecen tres criterios que serán ampliamente utilizados por el resto de investigaciones posteriores:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Se deben buscar términos con un fuerte componente discriminativo mediante la aplicación de algoritmos que sirvan para normalizar la frecuencia de apariciones de un término.</simpara>
</listitem>
<listitem>
<simpara>El test Likelihood ratio es capaz de obtener probabilidades bastante acertadas para este dominio específico.</simpara>
</listitem>
<listitem>
<simpara>Los términos más altamente discriminativos se caracterizan por una alta frecuencia y una baja dispersión.</simpara>
</listitem>
</orderedlist>
<simpara>Entre los resultados que presentaron, afirman ser capaces de localizar correctamente el 51% de los tuits dentro de un radio de error de 100 millas (~= 161 km.).</simpara>
</section>
<section xml:id="_otros_emphasis_papers_emphasis_de_interés">
<title>Otros <emphasis>papers</emphasis> de interés</title>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Location Type Classification Using Tweet Content</emphasis> <emphasis>por Haibin Liu et al. The Pennsylvania State University</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">TweetLocalize: Inferring Author Location in Social Media</emphasis> <emphasis>por Evan Sparks et al. University of California-Berkeley</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Inferring the Location of Twitter Messages Based on User Relationships</emphasis> <emphasis>por Clodoveu A. Davis Jr. et al. Universidade Federal de Minas Gerais</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Geolocation Prediction in Social Media Data by Finding Location Indicative Words</emphasis> <emphasis>por HAN Bo et al. University of Melbourne</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Home Location Identification of Twitter Users</emphasis> <emphasis>por Jalal Mahmud et al. IBM Research</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Geotagging One Hundred Million Twitter Accounts with Total Variation Minimization</emphasis> <emphasis>por Ryan Compton et al. HRL Laboratories (Malibu)</emphasis></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_algoritmos_y_aspectos_teóricos">
<title>Algoritmos y aspectos teóricos</title>
<section xml:id="_log_likelihood_ratio_test">
<title>Log Likelihood-Ratio test</title>
<simpara>El test Log Likelihood-Ratio (<link xlink:href="http://www.itl.nist.gov/div898/handbook/apr/section2/apr233.htm">http://www.itl.nist.gov/div898/handbook/apr/section2/apr233.htm</link>) es un método estadístico ampliamente utilizado en problemas donde se pretenden comparar dos conjuntos de datos a través de una serie de supuestos.</simpara>
<simpara>En el caso del presente proyecto, la idea principal es aplicar LLR sobre los dos conjuntos de datos que contienen, por un lado, los tuits que se han recogido para el área local sobre el que se quieren comenzar a inferir tuits, y por el otro, un conjunto de tuits localizado en un área que se podría entender como global respecto al conjunto de datos local (por lo general, las áreas globales son localizaciones donde se habla el mismo idioma que en el área local, pero abarca otros territorios).</simpara>
<simpara>En este caso, partimos del <emphasis role="strong">supuesto</emphasis> de que el área local es un caso especializado del área global, el cual se puede diferenciar por los términos que contiene. Sobre este supuesto, el test Log Likelihood-Ratio nos devolverá un valor con la probabilidad de que cada término del área local sea realmente discriminativo comparando su frecuencia también en el conjunto de datos globales. Cuanto más discriminativo sea el término, mayor será su valor LLR. Sin embargo, todos aquellos que no sirvan para diferenciar al conjunto de datos especializado, tendrán un valor negativo.</simpara>
<simpara>El valor LLR de cada término se utilizará para calcular la probabilidad de que los nuevos tuits pertenezcan o no al área local mediante un sumatorio de todos los valores para cada término que forma el tuit.</simpara>
<simpara>La consideración acerca de qué es un <emphasis role="strong">término</emphasis> se explica en <xref linkend="_términos_discriminativos"/>, pero se podría definir resumidamente como todo aquel lexema que pueda contener información geolocalizable asociada.</simpara>
<section xml:id="_log_likelihood_ratio_test_normalizado">
<title>Log Likelihood-Ratio test normalizado</title>
<simpara>La implementación del algoritmo Log Likelihood-Ratio utilizado se basa en la interpretación propuesta en el artículo: Java, Akshay, et al. «Why we twitter: understanding microblogging usage and communities». Perteneciente a la novena WebKDD y primer taller SNA-KDD 2007 en minería de datos web y análisis de redes sociales.<footnote><simpara>Disponible en: <link xlink:href="http://aisl.umbc.edu/resources/369.pdf">http://aisl.umbc.edu/resources/369.pdf</link> (ver tablas y ecuaciones en la página 7)</simpara></footnote></simpara>
<simpara>Tras los primeros experimentos, aunque se vislumbraron resultados esperanzadores, se observó también como algunos términos que se sabía eran discruminativos, no obtenían una puntuación LLR lo suficientemente alta como para que el sistema los pudiese considerar discriminativos en el futuro. Por tanto, se realizó una nueva implementación <emphasis>normalizada</emphasis> sobre el algoritmo anterior, basada en la siguiente sospecha:</simpara>
<blockquote>
<simpara>Términos como «españa», los cuales son muy representativos de tuits españoles, también son relativamente comunes en tuits chilenos, y por tanto no se puede interpretar que vayan a tener la misma importancia para discrimar entre ambos, por mucho que su uso sea mayor en España. Sin embargo, términos como «culín sidra», los cuales no tienen una gran representatividad en tuits españoles y es casi seguro que no tengan ninguna aparición en tuits procedentes de Chile, tienen un gran componente geográfico implícito que hace pensar que tuits que contenga ese término es <emphasis role="strong">muy posible</emphasis> que provengan de España.</simpara>
</blockquote>
<simpara>Para tratar de demostrar la intuición anterior, se desarrolló una normalización basada en tres pasos:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>La puntuación LLR de cada término, es normalizada contra el mínimo (negativo) y máximo (positivo) valor teórico que LLR puede alcanzar dados dos datasets.</simpara>
</listitem>
<listitem>
<simpara>La frecuencia del término en el dataset local y global, es normalizada frente al total de frecuencias en ambos datasets.</simpara>
</listitem>
<listitem>
<simpara>Por último, el valor LLR normalizado es dividido por el valor correspondiente de la frecuencia local o global dependiendo de si el valor obtenido es positivo o negativo (es decir, si es más posible que pertenezca al dataset A o al dataset B).</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Algoritmo para normalizar el valor LLR de un término</title>
<para>
<programlisting language="java" linenumbering="unnumbered">  /*
   * @param a  frequency of token of interest in dataset A
   * @param b  frequency of token of interest in dataset B
   * @param c  total number of observations in dataset A
   * @param d  total number of observations in dataset B
   */
  public double normalizedLLR (long a, long b, long c, long d) {
    double min, max, a_norm, b_norm, llr;
    min    = getLLR(0, d, c, d);
    max    = getLLR(c, 0, c, d);
    a_norm = a / (double)c;
    b_norm = b / (double)d;
    llr    = getLLR(a, b, c, d);

    return (llr&gt;0) ? (llr/(max*a_norm)) : (-llr/(min*b_norm));
  }</programlisting>
</para>
</formalpara>
<simpara>De esta manera, el valor LLR normalizado se puede usar como <emphasis>proxy</emphasis> para el nivel de confianza que tendremos a la hora de asignar cada término a uno u otro dataset. Por ejemplo, en el supuesto anterior, «españa» obtendría un peso relativamente bajo, mientras que si aparece «culín sidra» su peso sería muy alto. En el caso contrario, donde se querría identificar cuando un término no es lo suficientemente discriminativo para el área local, se aplicaría la misma lógica, y términos como «chile» tendría un peso negativo moderado, mientras que «achunchar» obtendría un peso mucho más negativo para indicar que el vocablo es más discriminativo para el área global.</simpara>
<simpara>A continuación, se muestran los 10 resultados más discriminativos tras aplicar LLR y LLR normalizado sobre keywords que sirvan para diferenciar tuits de Asturias del resto de España:</simpara>
<formalpara>
<title>LLR sin normalizar</title>
<para>
<screen>5.673174267765713    avilés
5.786668335161142    muches
6.55625607825904     gracies
6.9163915469140225   gijon
7.082198151712459    olmo
7.650261596901131    ye
7.8221070414197245   besin
8.673885864488826    gijón
11.355768634976933   asturias
13.541645339239054   oviedo</screen>
</para>
</formalpara>
<formalpara>
<title>LLR normalizado</title>
<para>
<screen>34.03033586645537    economia
34.83206427341024    escalera
35.896146032001866   moreda
40.13311132890776    héctor
40.13311132890776    celebralos
40.13311132890776    sidra
41.63987088300992    avilés
41.65857127377122    simón
45.94585510324283    presta</screen>
</para>
</formalpara>
<simpara>Se puede observar como en el caso del LLR normalizado, se penalizan aquellos términos que podrían aparecer frecuentemente en otros tuits del territorio español: <emphasis>oviedo</emphasis>, <emphasis>gijón</emphasis> y <emphasis>asturias</emphasis>; mientras que se favorecen otros más específicos como <emphasis>sidra</emphasis>, <emphasis>presta</emphasis> o <emphasis>celebralos</emphasis> (escrito como se pronunciaría en asturiano).</simpara>
<simpara>La implementación de ambos algoritmos Root Log Likelihood-Ratio fue desarrollada por el doctor Daniel Gayo Avello.</simpara>
</section>
</section>
<section xml:id="_términos_discriminativos">
<title>Términos discriminativos</title>
<simpara>Se consideran términos discriminativos aquellos que son capaces de aportar información muy geolocalizable de manera implícita. Un ejemplo son aquellas palabras muy propias de una localización en concreto, como el caso del término <emphasis>sidra</emphasis> o <emphasis>carbayu</emphasis>, que con mucha probabilidad indican un contenido que ha sido generado en Asturias.</simpara>
<simpara>La estrategia planteada en este proyecto está basada en descubrir este tipo de términos a través de los tuits que los usuarios publican para una determinada región (país, estado, ciudad, etc.). Para ello, la premisa básica es aceptar que los términos más discriminativos tenderán a tener un epicentro muy significativo donde su frecuencia es muy elevada, para después, no tener apenas dispersión y ser muy poco frecuentes en el resto.</simpara>
<simpara>Con el objetivo de poder aplicar los algoritmos anteriores, será necesario trabajar siempre con dos datasets sobre los que establecer la comparativa. Por un lado, se trabajará con un conjunto de datos localizados en el área en concreto que se quiera analizar, y por otro lado, otro conjunto de datos que se establezcan en un área más global para ese mismo idioma.</simpara>
<simpara>A partir de ahí, el análisis de cada dataset extraerá los siguientes términos:</simpara>
<variablelist>
<varlistentry>
<term>Menciones</term>
<listitem>
<simpara>Se considerán menciones todos aquellos términos que comienzan con el literal <literal>@</literal>. En Twitter, se utilizan para hacer referencia a otro usuario en el contenido que se está publicando.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Hashtags</term>
<listitem>
<simpara>Un hashtag es un término que comienza con el literal <literal>#</literal> y sirve para categorizar el contenido de un tuit. Un ejemplo claro es durante los partidos de fútbol del Fútbol Club Barcelona, donde los aficionados que se encuentran comentando el partido en Twitter, suelen acompañar cada publicación con el hashtag <literal>#fcblive</literal> de manera que clasifican manualmente el contenido de su tuit para poder ser agrupado en una misma conversación.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Bigramas</term>
<listitem>
<simpara>En este proyecto, hemos considerado como bigramas todas aquellas combinaciones de 2 palabras que se puedan hacer con el contenido de un tuit. Al contrario que en algunos artículos de investigación anteriores donde sólo se consideran términos consecutivos, en este caso hemos realizado todas las combinaciones posibles para cada tuit.</simpara>
<simpara>Algunas consideraciones importantes sobre esto son:</simpara>
<itemizedlist>
<listitem>
<simpara>Se han eliminado todos aquellos bigramas que contienen 2 veces la misma palabra.</simpara>
</listitem>
<listitem>
<simpara>Se han eliminado todos aquellos bigramas que contienen al menos una palabra vacía.</simpara>
</listitem>
<listitem>
<simpara>Se han eliminado todos aquellos bigramas con términos inferiores a 2 caracteres.</simpara>
</listitem>
<listitem>
<simpara>Se han ordenado alfabéticamente todos los bigramas de acuerdo a las 2 palabras que contienen, facilitando así el control de bigramas repetidos.</simpara>
<simpara>Un ejemplo de los bigramas que obtendríamos tras analizar un tuit en base a las condiciones anteriores sería:</simpara>
<blockquote>
<simpara>Buenos días vamos a trabajar todo el día</simpara>
</blockquote>
<simpara>Que generaría las siguientes combinaciones</simpara>
<screen>(buenos días), (buenos vamos), (buenos trabajar), (buenos todo), (buenos día), (días vamos), (días trabajar), (días todo), (día días), (trabajar vamos), (todo vamos), (día vamos), (todo trabajar), (día trabajar), (día todo)</screen>
<simpara>Como se puede observar, la generación de bigramas para cada tuit provoca una explosión de términos que fue necesario controlar (explicado en <xref linkend="_utilización_de_algoritmos_de_streaming"/>) para evitar sobrepasar la memoria del sistema.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Keywords</term>
<listitem>
<simpara>Las keywords son unigramas formandos, obviamente, por un único término, cuyo resultado se asemeja a realizar una tokenización sobre el tuit pero aplicando reglas que también se utilizaban en la extracción de bigramas (palabra vacía, longitud inferior a 2 caracteres, etc.).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Keywords en el campo de Localización</term>
<listitem>
<simpara>Son el resultado de aplicar la extracción anterior sobre el campo de Localización del perfil del usuario.</simpara>
</listitem>
</varlistentry>
</variablelist>
<section xml:id="_sistema_de_filtros">
<title>Sistema de filtros</title>
<simpara>Con el objetivo de poder realizar las extracciones de los términos anteriores de forma flexible, se diseñó un pequeño sistema de filtros que ayudara a combinar varios filtros en una misma ejecución. La implementación de este sistema está basada en el patrón de diseño Decorator<footnote><simpara><link xlink:href="http://perldesignpatterns.com/?word=decorator+pattern">http://perldesignpatterns.com/?word=decorator+pattern</link></simpara></footnote>, aunque con la diferencia de que en este caso, la extracción de cada filtro se realiza sobre el tuit original y no sobre el resultado de las extracciones de filtros anteriores (una «decoración» incremental no tendría sentido dado el dominio del problema).</simpara>
<figure>
<title>Representación del patrón Decorator que ilustra el sistema de filtros</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/extractor-filter.png" align="center"/>
</imageobject>
<textobject><phrase>Representación del patrón Decorator que ilustra el sistema de filtros</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="_utilización_de_algoritmos_de_streaming">
<title>Utilización de algoritmos de Streaming</title>
<simpara>Como se ha visto en secciones anteriores, el proceso de extracción de términos genera una gran cantidad de datos que será necesario gestionar en memoria. Para solucionar este problema, se hizo uso de técnicas propias de los algoritmos de Streaming, los cuales tienen varios puntos en común con el problema actual.</simpara>
<note>
<title>Algoritmos de Streaming</title>
<simpara>Se conocen como algoritmos de Streaming aquellos problemas donde la capacidad de memoria o procesamiento es menor a la cantidad de datos que se reciben como entrada. Estos datos, se procesan de uno en uno y una única vez, manteniendo un orden secuencial e incremental que implica que sea necesario conocer el dato anterior para poder procesar correctamente el dato actual.</simpara>
</note>
<simpara>La solución, por tanto, pasa por controlar el número de elementos que se gestionan en cada momento en memoria por el sistema, y plantear una estrategia que sea capaz de liberar memoria sin el riesgo de perder información que pueda adulterar los resultados. Los pasos seguidos en este proyecto para lidiar con el problema fueron los siguientes:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Establecer un número máximo de <emphasis>keys</emphasis> que podrán ser gestionadas por el hash. Este valor deberá ser configurado por el desarrollador en función de las características del hardware sobre el que se ejecute el sistema. En las pruebas realizadas en este proyecto, el número máximo de elementos se situó en 500.000.</simpara>
<simpara>Esto también implica que el sistema de generación de puntuaciones LLR podrá trabajar únicamente sobre los <literal>n</literal> términos que se seleccionen aquí.</simpara>
</listitem>
<listitem>
<simpara>Una vez determinado el umbral máximo de elementos, será necesario definir que porcentaje de términos se eliminarán una vez alcanzado el límite anterior. En este caso, se ha optado por seguir una estrategia de poda agresiva en la que se eliminan de manera constante un % de los elementos con menos frecuencia del hash.</simpara>
<simpara>Esta estrategia implica que siempre que se produzca una situación de poda, se deba ordenar el hash de acuerdo a la frecuencia de sus elementos. De manera experimental, se ha comprobado como la eliminación constante de un 40% de los elementos con menor frecuencia, a pesar de parecer demasiado agresiva, da resultados muy positivos sin existir riesgo de eliminar términos con una frecuencia muy elevada (por supuesto, todo esto dentro del dominio del problema actual).</simpara>
</listitem>
<listitem>
<simpara>En el momento de realizar la poda, se debe guardar qué frecuencia es la mayor del grupo de elementos a eliminar. De esta manera, se consigue que términos que vuelvan a aparecer tras la poda, partan de su frecuencia original en vez de volver a empezar de 0. Esto provoca también que muchos términos nuevos, empiecen con una frecuencia más elevada de lo esperado. Sin embargo, la frecuencia mínima que se utilizará después para seleccionar sobre qué términos se aplica el LLR, será lo suficientemente elevada como para evitar situaciones donde este problema pueda adulterar los resultados.</simpara>
</listitem>
</orderedlist>
<simpara>En <xref linkend="_pseudocódigo_para_ilustrar_el_proceso_completo_de_análisis_de_tuits"/>, se muestra un esbozo de la implementación del algoritmo anterior. El control de memoria y proceso de poda agresiva se ilustra a través de los métodos <literal>check_memory_status</literal> y <literal>reduce_map_load</literal>.</simpara>
</section>
</section>
<section xml:id="_pseudocódigo_para_ilustrar_el_proceso_completo_de_análisis_de_tuits">
<title>Pseudocódigo para ilustrar el proceso completo de análisis de tuits</title>
<simpara>Los siguientes fragmentos de <emphasis>pseudocódigo</emphasis> muestran los diferentes algoritmos que se han utilizado para obtener la frecuencia de términos en los diferentes datasets, así como el proceso para realizar el cálculo de su Log Likelihood-Ratio asociado y la manera de computar la puntuación total de cada tuit en función de la puntuación de cada uno de los términos que contiene.</simpara>
<formalpara>
<title>Algoritmo para extraer la frecuencia de cada término</title>
<para>
<screen>for each tweet in tweets do
  terms = apply_extractor_filter(tweet)
  check_memory_status()
  for each term in terms do
    if frequencies[term].is_defined then
      frequencies[term] += 1
    else
      frequencies[term] = minimum_frequency
    end
  end
end

def check_memory_status
  if frequencies.size &gt;= MAXIMUM_EXTRACTED_TERMS then
    reduce_map_load()
  end
end

def reduce_map_load
  items_to_remove   = frequencies.size * FACTOR_TO_REMOVE
  ordered_map       = frequencies.order_by_frequency
  minimum_frequency = ordered_map.get(items_to_remove - 1)

  ordered_map.slice from items_to_remove - 1 to ordered_map.size
end</screen>
</para>
</formalpara>
<formalpara>
<title>Algoritmo para calcular la puntuación LLR de cada término</title>
<para>
<screen>for each term in locals do
  freq = locals[term]
  if freq &gt; min_frequency then
    global_freq = get_global_freq(term)
    if global_freq &gt; 0 then
      k11 = freq
      k12 = global_freq
      k21 = total_local_frequencies
      k22 = total_global_frequencies
      llr = Dunning.normalized_llr(k11, k12, k21, k22)
      results[term] =&gt; llr
    end
  end
end

def get_global_freq(term)
  globals[term].is_defined ? globals[term] : avg_global_freq(term)
end

def avg_global_freq(term)
  same_freq_in_local = locals.select(
    t =&gt; globals[t].is_defined &amp;&amp; locals[t] == locals[term]
  )
  acc = same_freq_in_local.map(t =&gt; globals[t]).reduce(
    (previous, current) =&gt; previous + current
  )
  return acc / same_freq_in_local.size
end</screen>
</para>
</formalpara>
<formalpara>
<title>Algoritmo para calcular la puntuación LLR de cada tuit</title>
<para>
<screen>for each tweet in tweets do
  terms = extract_terms(tweet)
  score = terms.reduce(
    (previous, current) =&gt; previous + get_llr_score(current)
  )
  results[tweet] = score
end</screen>
</para>
</formalpara>
</section>
<section xml:id="_procesando_xml_con_emphasis_pull_parsing_emphasis">
<title>Procesando XML con <emphasis>pull parsing</emphasis></title>
<simpara>Debido al intenso uso que se hace en el proyecto de datos en XML, fue necesario desarrollar un sistema de análisis basado en estrategias que evitasen tener que cargar el contenido completo del XML en memoria.</simpara>
<simpara>Dos de las estrategias más conocidas en contraposición a DOM para analizar grandes colecciones de XML son: <emphasis role="strong">Simple API XML (o SAX)<footnote><simpara><link xlink:href="http://www.saxproject.org/">http://www.saxproject.org/</link></simpara></footnote></emphasis> y <emphasis role="strong">pulling parsing<footnote><simpara><link xlink:href="http://www.xmlpull.org/">http://www.xmlpull.org/</link></simpara></footnote></emphasis>.</simpara>
<simpara>En este proyecto, se decidió utilizar la segunda debido a su soporte nativo por parte de las APIs de Scala para trabajar con XML a través del paquete <literal>xml.pull</literal>. Pulling parser se caracteriza por analizar el fichero XML mediante la utilización de eventos. A medida que se van leyendo líneas del documento, van saltando una serie de eventos de acuerdo a la especificación: <emphasis>comienzo de elemento, fin de elemento, comienzo de nodo de texto, fin de nodo de texto, etc</emphasis>. Cada uno de estos eventos, puede tener asignado un <emphasis>listener</emphasis> que contenga la lógica a ejecutar.</simpara>
<simpara>El fragmento de código posterior muestra un ejemplo de uso para analizar un fichero XML mediante <emphasis>pulling parser</emphasis>. En primer lugar se define el fichero sobre el que realizar el análisis y se crea una nueva variable que nos ayudar a controlar cuando estamos dentro del elemento que queremos analizar. Utilizando <emphasis>pattern matching</emphasis> nos aseguramos de seleccionar aquellos elementos cuya etiqueta coincida con la del campo sobre el que debe actuar el filtro que deseamos aplicar. En caso de encontrar un elemento de apertura para ese nodo, indicamos en la variable booleana <literal>in</literal> que nos encontramos en el elemento deseado y los eventos encargados de detectar nodos de texto la consultarán para saber si sobre ese texto deben aplicar o no el filtro.</simpara>
<simpara>El evento para detectar que hemos salido del nodo deseado se encarga de volver a <emphasis>setear</emphasis> el valor de la variable <literal>in</literal> a <literal>false</literal>.</simpara>
<formalpara>
<title>Ejemplo de código para analizar un documento XML utilizando <emphasis>pulling parser</emphasis></title>
<para>
<programlisting language="java" linenumbering="unnumbered">val reader = new XMLEventReader(Source.fromFile(xmlFile))
var in = false
reader.foreach({
  case e: EvElemStart if e.label == _filter.field =&gt; in = true
  case EvText(text) if in =&gt; applyFilter(text)
  case e: EvElemEnd if e.label == _filter.field =&gt; in = false
  case _ =&gt; ;
})</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_entrenamiento_del_clasificador">
<title>Entrenamiento del clasificador</title>

</section>
</chapter>
<chapter xml:id="_experimentos_y_resultados">
<title>Experimentos y resultados</title>
<simpara>Para comprobar la validez de los algortimos anteriores se plantearon una serie de experimentos que trabajasen sobre conjuntos de tuits escritos tanto en inglés como en español, con el objetivo de poder obtener resultados positivos a nivel de país, provincia, área metropolitana y barrio.</simpara>
<simpara>La selección de los idiomas inglés y español como base para los experimentos se realizó a tenor de ser los dos lenguajes, a priori, más complejos de analizar debido a que son hablados en muchas partes del mundo. Por ejemplo, haber escogido como idioma el italiano, haría que las posibilidades de geolocalización se viesen reducidas prácticamente a Italia; ídem en el caso de haber utilizado alemán, que tan sólo es hablado en Alemania, Austria, Suiza y parte de Luxemburgo.</simpara>
<section xml:id="_primeros_experimentos_trabajando_con_grandes_volúmenes_de_datos">
<title>Primeros experimentos: trabajando con grandes volúmenes de datos</title>
<simpara>Los primeros experimentos se basaron en establecer diferencias a nivel de país. Para ello, se realizó un proceso de recolección de tuits durante una semana para los siguientes casos:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Tuits de España escritos en español</simpara>
</listitem>
<listitem>
<simpara>Tuits del mundo escritos en español</simpara>
</listitem>
<listitem>
<simpara>Tuits del Reino Unido escritos en inglés</simpara>
</listitem>
<listitem>
<simpara>Tuits del mundo escritos en inglés</simpara>
</listitem>
</orderedlist>
<simpara>En total esta fase duró 4 semanas, recolectando en torno a <literal>2 x 8GB</literal> de datos para los tuits globales, así como <literal>2 x 1GB</literal> para los tuits localizados en España y el Reino Unido.</simpara>
<simpara>Estos primeros conjuntos de datos permitieron que se desarrollasen los algoritmos de puntuación LLR y se empezase a comprobar empíricamente qué resultados aportaban los términos utilizados (menciones, hashtags, bigramas) así como investigar que otros campos o términos del perfil del usuario podrían ayudar a encontrar pistas acerca de su geolocalización. Durante esta fase se añadió el uso de <emphasis>keywords</emphasis> tanto a nivel de contenido de un tuit como sobre el campo de localización del usuario, sospechando que podrían aportar más información útil en base a los resultados LLR que se estaban observando a nivel de bigramas.</simpara>
<simpara>Sobre estos conjuntos de datos no llegaron a establecerse pruebas que involucrasen software de aprendizaje automático, puesto que el uso de grandes volúmenes de datos quedó descartado al comprobar empíricamente que los términos que implican contenido geolocalizable no tienen que ser uniformes en el tiempo para una determinada región. Esta apreciación se realizó al trabajar con nuevos conjuntos de datos más pequeños, donde aparecieron como términos muy significativos aquellos que tenían que ver con un evento temporal muy concreto, el fallecimiento del seleccionador de fútbol nacional, Luis Aragonés.</simpara>
<formalpara>
<title>Puntuación LLR para algunos términos recogidos en conjuntos de datos entre el 1 y 2 de febrero de 2014</title>
<para>
<screen>48.958529065415576  aragonés gran
49.17093149031471   aragonés selección
50.049053227619474  aragonés descanse
50.049053227619474  grandes luis
50.739417608310795  luis roja
50.97596629207029   luis nunca
50.97596629207029   luis mas
51.215854517258855  aragones hoy
51.215854517258855  aragonés historia
51.45916160893309   fútbol hombre
51.95636312422304   aragonés hombre
52.21042998468123   siempre aragones
52.46826083659376   luis mejor
52.46826083659376   dep sabio
56.28098057747021   descanse paz
81.59614077426396   fútbol grande
102.31617863438613  entrenador gran
107.56036254304512  bernabéu santiago
107.5697695101089   noticia triste
107.87375147152346  hoy madrid
111.68234684538564  hizo hombre
111.93538868096397  dep triste
115.39967536884996  fútbol va
116.50155357908382  futbol hoy
119.45187311362997  día fútbol</screen>
</para>
</formalpara>
<simpara>Como se puede observar, términos vinculados al fallecimiento del seleccionador aportaban una gran información geográfica que hacía que todos los tuits que los contenían prácticamente fuesen vinculados a España (lo cual en este caso sería correcto). Pero, ¿qué sucede si ocurre sobre un evento que se pueda aplicar sobre cualquier otro lugar del mundo en cualquier momento?</simpara>
<simpara>Por ejemplo, es habitual que durante los meses de verano en el hemisferio sur, términos relativos a la estación estival sean más habituales en países como Argentina, Chile, etc. que en España. Lo cual sería justamente al revés en épocas de verano en el hemisferio norte. Por tanto, es importante tener en cuenta que los términos y el vocabulario tienen una importancia temporal, y el hecho de obtener grandes volúmenes de datos no tiene porque beneficiar el entrenamiento de un modelo de aprendizaje automático para inferir la localización de un tuit.</simpara>
<simpara>Por ello, se optó por trabajar siempre con conjuntos de datos recogidos en periodos de 24 horas, para evaluarlos contra colecciones de tuits que se recogerían en las 24 horas siguientes. De esta manera se puede aprovechar la repercursión de ciertos términos ante cualquier evento.</simpara>
</section>
<section xml:id="_trabajando_con_diferentes_niveles_de_granularidad">
<title>Trabajando con diferentes niveles de granularidad</title>
<section xml:id="_conceptos_previos">
<title>Conceptos previos</title>
<section xml:id="_acierto_y_exhaustividad">
<title>Acierto y exhaustividad</title>

</section>
<section xml:id="_umbral_de_acierto">
<title>Umbral de acierto</title>

</section>
</section>
<section xml:id="_resultados_a_nivel_de_país">
<title>Resultados a nivel de país</title>
<simpara>De acuerdo con los objetivos establecidos al comienzo del proyecto, se realizaron dos experimentos a nivel de país para poder trabajar tanto con tuits escritos en español como con aquellos escritos en inglés.</simpara>
<simpara>Por un lado, se realizó un experimento que pudiese discriminar tuits procedentes de España en un conjunto de tuits globales escritos en español. Por el otro, se realizó un segundo experimento enfocado en detectar qué tuits escritos en inglés desde las islas británicas procedían de la República Irlandesa.</simpara>
<simpara>Los resultados para ambos experimentos se pueden observar en las tablas <xref linkend="experiment-spain-global"/> y <xref linkend="experiment-uk-ireland"/>.</simpara>
<table xml:id="experiment-spain-global" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes España</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>93.16%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>89.48%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>96.13%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>90.35%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-uk-ireland" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de la República Irlandesa.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>98.17%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>71.17%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>97.56%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>98.20%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_resultados_a_nivel_de_provincia">
<title>Resultados a nivel de provincia</title>
<simpara>Para realizar los experimentos a nivel de provincia se seleccionaron las siguientes tres regiones del estado español: <emphasis role="strong">Principado de Asturias</emphasis>, <emphasis role="strong">Madrid y</emphasis> <emphasis role="strong">Barcelona</emphasis>.</simpara>
<simpara>En este caso, se consideró como área local cada una de las tres regiones anteriores y como área global, el total del territorio español a excepción de la provincia evaluada. Así pues, para realizar el experimento sobre el Principado de Asturias se consideró:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Área local</emphasis>: Principado de Asturias</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Área global</emphasis>: Todo el territorio español a excepción del Principado de Asturias.</simpara>
</listitem>
</itemizedlist>
<simpara>Siguiendo el mismo mecanismo en el resto de provincias.</simpara>
<simpara>Las tablas <xref linkend="experiment-spain-asturias"/>, <xref linkend="experiment-spain-madrid"/> y <xref linkend="experiment-spain-barcelona"/> muestran los resultados de la ejecución para esta granularidad.</simpara>
<table xml:id="experiment-spain-asturias" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes del Pricipado de Asturias.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>98.89%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>53.53%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>89.83%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>99.00%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-spain-madrid" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Madrid.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>92.27%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>72.70%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>86.78%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>93.39%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-spain-barcelona" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Barcelona.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>98.25%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>65.73%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>97.09%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>98.29%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="_barcelona_y_el_impacto_del_catalán">
<title>Barcelona y el impacto del catalán</title>
<simpara>Uno de los aspectos más curiosos de los resultados y puntuaciones generadas por el algoritmo Log Likelihood-Ratio fue comprobar como entre los términos más discriminativos para la provincia de Barcelona se encontraban, en su mayoría, vocablos en catalán.</simpara>
<simpara>Esto certifica de manera empírica las intuiciones acerca de la validez del algoritmo para este tipo de problema, siendo capaz de detectar que los términos en catalán, con una frecuencia muy baja a nivel global y relativamente alta en Barcelona, son los más significativos para identificar al territorio.</simpara>
<formalpara>
<title>Bigramas más discriminativos para encontrar tuits procedentes de la provincia de Barcelona</title>
<para>
<screen>94.52274667047315   per tres
100.19650868429694  per dos
100.19650868429694  salón barcelona
100.19650868429694  ara per
100.19650868429694  acuerdo leo
100.19650868429694  gracias horas
100.19650868429694  debat els
101.79780787750204  buenos barcelona
111.11339666895931  catalunya és
111.11339666895931  avui amb</screen>
</para>
</formalpara>
</section>
</section>
<section xml:id="_resultados_a_nivel_de_área_metropolitana">
<title>Resultados a nivel de área metropolitana</title>
<simpara>Para la granularidad a nivel de área metropolitana, se utilizaron las tres metrópolis más importantes del Reino Unido: <emphasis role="strong">Manchester</emphasis>, <emphasis role="strong">Birminghan</emphasis> y <emphasis role="strong">Londres</emphasis>. El proceso para generar los conjuntos de datos utilizados para el análisis se basa en los mecanismos utilizados a nivel de provincia.</simpara>
<simpara>Las tablas <xref linkend="experiment-uk-london"/>, <xref linkend="experiment-uk-manchester"/> y <xref linkend="experiment-uk-birminghan"/> muestran los resultados para esta granularidad.</simpara>
<table xml:id="experiment-uk-london" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Londres.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>91.69%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>70.98%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>86.43%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>92.77%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-uk-manchester" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Manchester.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>96.68%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>51.10%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>88.53%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>96.97%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-uk-birminghan" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Birminghan.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>97.22%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>47.05%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>97.95%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>97.20%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_resultados_a_nivel_de_barrio">
<title>Resultados a nivel de barrio</title>
<simpara>Por último, se realizó un experimento a nivel de barrio, siendo la granularidad más baja analizada en el proyecto. Para ello, se utilizaron los 3 barrios más importantes de Londres como área local (<emphasis role="strong">Wandsworth</emphasis>, <emphasis role="strong">Lambeth</emphasis> y <emphasis role="strong">Southwark</emphasis>), así como la ciudad de Londres como área global. Los resultados de este experimento se pueden consultar en las tablas <xref linkend="experiment-london-wandsworth"/>, <xref linkend="experiment-london-lambeth"/> y <xref linkend="experiment-london-southwark"/>.</simpara>
<table xml:id="experiment-london-wandsworth" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Wandsworth.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>99.54%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>18.18%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>100.0%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>99.54%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-london-lambeth" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Lambeth.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>99.20%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>24.13%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>87.5%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>99.23%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-london-southwark" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Southwark.</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Acierto</simpara></entry>
<entry align="left" valign="top"><simpara>98.98%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>25.0%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>90.0%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Acierto en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>99.01%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
</chapter>
<chapter xml:id="_conclusiones_del_sistema_de_geolocalización">
<title>Conclusiones del sistema de geolocalización</title>

</chapter>
<chapter xml:id="_aplicación_web_de_geolocalización_como_servicio">
<title>Aplicación web de geolocalización como servicio</title>
<section xml:id="_análisis_del_sistema">
<title>Análisis del sistema</title>
<section xml:id="_diseño_de_emphasis_mockups_emphasis_para_interfaces_de_usuario">
<title>Diseño de <emphasis>mockups</emphasis> para interfaces de usuario</title>
<figure>
<title>Pantalla de login de la aplicación.</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/mockup/home.png" align="center"/>
</imageobject>
<textobject><phrase>home</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Pantalla que muestra el dashboard del usuario en la sección de proyectos.</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/mockup/dashboard-projects.png" align="center"/>
</imageobject>
<textobject><phrase>dashboard projects</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Pantalla que muestra el dashboard del usuario en la sección de ejecuciones.</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/mockup/dashboard-executions.png" align="center"/>
</imageobject>
<textobject><phrase>dashboard executions</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Lightboxes de creación de un nuevo proyecto.</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/mockup/new-project.png" contentdepth="650px" align="center"/>
</imageobject>
<textobject><phrase>new project</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Pantalla de ejecución de un proyecto en tiempo real.</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/mockup/execution.png" align="center"/>
</imageobject>
<textobject><phrase>execution</phrase></textobject>
</mediaobject>
</figure>
</section>
</section>
</chapter>
<appendix xml:id="_scala">
<title>Scala</title>
<section xml:id="_sbt">
<title>sbt</title>

</section>
</appendix>
<appendix xml:id="_software_de_aprendizaje_automático_y_vowpal_wabbit">
<title>Software de aprendizaje automático y Vowpal Wabbit</title>
<section xml:id="_software_de_aprendizaje_automático">
<title>Software de aprendizaje automático</title>

</section>
<section xml:id="_vowpal_wabbit">
<title>Vowpal Wabbit</title>
<simpara><emphasis role="strong">Vowpal Wabbit<footnote><simpara><link xlink:href="http://hunch.net/~vw/">http://hunch.net/~vw/</link></simpara></footnote></emphasis> es un proyecto creado originalmente por <emphasis>Yahoo! Research</emphasis> que fue posteriormente continuado por <emphasis>Microsoft Research</emphasis>, consistente en la creación de un nuevo algoritmo de aprendizaje automático rápido y escalable que permita trabajar con grandes cantidades de datos. A partir de una serie de información específicamente preparada para ser consumida por Vowpal Wabbit, este es capaz de crear un propio modelo de datos que sirva como base de entrenamiento para después, mediante la aplicación de diversos algoritmos, predecir un resultado en base al conocimiento adquirido en ejecuciones anteriores.</simpara>
<simpara>Además de la velocidad y precisión de los resultados que puede ofrecer Vowpal Wabbit, una de sus características más importantes es la capacidad de funcionar como un demonio del sistema e ir aprendiendo <emphasis>en caliente</emphasis> a través de nuevos modelos de datos. Esto, a diferencia de otros software de aprendizaje automático, permite que el sistema pueda adquirir un conocimiento incremental y ofrecer mejores resultados a medida que pasa el tiempo.</simpara>
<section xml:id="_normalización_de_datos">
<title>Normalización de datos</title>
<simpara>Para aprovechar toda la potencia y velocidad que ofrece Vowpal Wabbit, es necesario generar ficheros de entrada que estén estructurados de acuerdo a un formato optimizado para el clasificador. En este caso, Vowpal Wabbit espera datos de entrada estructurados de la siguiente manera:</simpara>
<screen>[Label] [Importance [Tag]]|Namespace Features |Namespace Features ... |Namespace Features <co xml:id="CO1-1"/></screen>
<calloutlist>
<callout arearefs="CO1-1">
<para><link xlink:href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format">https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format</link></para>
</callout>
</calloutlist>
<simpara>En el presente proyecto, esta estructura generaría datos de la siguiente manera:</simpara>
<screen>1 |Tweet @adrianzenb scl rainer wirth óscar amigos radio quillota
0 |Tweet @thomasuribe medellín colombia celebra día hombre
1 |Tweet @fvminajx @cursiperono mujer ruega punto</screen>
<simpara>Debido a que un primer momento se consideró la opción de trabajar con modelos multiclase, el sistema que realiza la traducción entre los ficheros de puntuación generados por el sistema Puma y los datos que espera recibir Vowpal Wabbit, debía de ser parametrizable para poder cubrir los siguientes dos casos:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Entrenar un modelo de clasificación binario donde sólo se indicara si un tuit pertenece o no a un conjunto de coordenadas (útil para modelos en los que se quiera conocer de manera binaria si una dato pertenece o no a una localización en concreto).</simpara>
</listitem>
<listitem>
<simpara>Entrenar un modelo de clasificación multiclase donde se agruparan las clases en torno a un número de decimales para la latitud y longitud. Esto haría que se considerasen de la misma clase todos los tuits cuyas coordenadas con 3 decimales sean las mismas, permitiendo obtener predicciones con un grado de precisión de 10 kilómetros (en caso de agrupar por 2 decimales, la precisión sería de 100 y con 1, de 1000 kilómetros).</simpara>
</listitem>
</orderedlist>
<simpara>Para ello, se creo un sistema que mediante una interfaz de línea de comandos pudiese ser configurable mediante los siguientes argumentos:</simpara>
<screen>vw-input-translator 1.0
Usage: vw-input-translator [options] &lt;file&gt;

  --inout
        Flag for creating an input file for binary classification
  -d &lt;value&gt; | --decimals &lt;value&gt;
        Creates an input file for multi-class classification. Each sample will have the selected number of decimals on latitude and longitude coordinates
  &lt;file&gt;
        Source file</screen>
</section>
<section xml:id="_división_de_datos_en_conjuntos_de_entrenamiento_y_test">
<title>División de datos en conjuntos de entrenamiento y test</title>
<simpara>Con el objetivo de entrenar al clasificador, se desarrolló un script capaz de, a partir de los datos de entrada que recibiría Vowpal Wabbit, dividir el conjunto en dos para dedicar una parte al proceso de entrenamiento y otra a probar el modelo de datos generados.</simpara>
<simpara>Con el objetivo de generar dos conjuntos consistentes, la división se realizó en base a los usuarios, haciendo que un mismo usuario no pudiese formar parte de ambos grupos. Un 80% de los usuarios sería destinado al grupo de entrenamiento, mientras que el 20% restante iría a parar al conjunto de test.</simpara>
</section>
</section>
</appendix>
<appendix xml:id="_falcon_sistema_para_coleccionar_datos_de_la_api_streaming_de_twitter">
<title>Falcon, sistema para coleccionar datos de la API Streaming de Twitter</title>
<simpara>Para crear los diferentes datasets que fueron necesarios para desarrollar los prototipos y modelos de datos, se diseñó un sistema capaz de conectarse a la <emphasis role="strong">API Streaming de Twitter</emphasis> (<link xlink:href="https://dev.twitter.com/docs/api/streaming">https://dev.twitter.com/docs/api/streaming</link>) para descargar y almacenar tuits en tiempo real.</simpara>
<simpara>Este sistema debía ser parametrizable, con el objetivo de poder configurar en cada ejecución el tipo de tuits que se querían obtener de acuerdo a los filtros disponibles a través de la API de Twitter:</simpara>
<itemizedlist>
<listitem>
<simpara>Filtro por idioma del tuit</simpara>
</listitem>
<listitem>
<simpara>Filtro por localización del tuit (mediante el uso de <emphasis>bounding boxes</emphasis>)</simpara>
</listitem>
</itemizedlist>
<simpara>Para realizar la conexión entre el sistem Falcon y la API de Twitter se utilizó la biblioteca <emphasis role="strong">Twitter4j</emphasis> (<link xlink:href="http://twitter4j.org/en/index.html">http://twitter4j.org/en/index.html</link>). La ventajas de utilizar una biblioteca construida sobre la API original es que algunos de los problemas más habituales se solucionan a través de nuevas capas de abstracción:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Autenticación OAuth2 simplificada mediante clases propias de la biblioteca</simpara>
</listitem>
<listitem>
<simpara>Simplificación del proceso para poder utilizar la API de Streaming, aislando al desarrollador de la complejidad para mantener activa la conexión con el servidor de Twitter.</simpara>
</listitem>
</orderedlist>
<figure>
<title>Comunicación entre un cliente y la API Streaming de Twitter<footnote><simpara><link xlink:href="https://dev.twitter.com/docs/api/streaming">https://dev.twitter.com/docs/api/streaming</link></simpara></footnote></title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/twitter-streaming-api.png" align="center"/>
</imageobject>
<textobject><phrase>Modelo de comunicación entre un cliente y la API Streaming de Twitter</phrase></textobject>
</mediaobject>
</figure>
<section xml:id="_almacenamiento_de_datos">
<title>Almacenamiento de datos</title>
<simpara>Uno de los puntos más importantes que planteó el sistema para recolectar tuits era en qué formato sería más adecuado serializar los datos obtenidos.</simpara>
<simpara>En un primer momento se barajó la posibilidad de utilizar el formato CSV, el cual permitiría acceder de manera rápida al número de tuits guardados y realizar operaciones sencillas en línea de comandos mediante operaciones <literal>grep</literal>. Esta decisión fue descartada al realizar los primeros experimentos y comprobar como el guardado de ciertos datos en formato CSV presenta muchas dificultades para poder solventar todos los casos esquina que se presentan con la aparición de contenido complejo que pueda incluir comas, comillas y otros signos de puntuación (aún en el caso de utilizar bibliotecas especializadas como OpenCSV - <link xlink:href="http://opencsv.sourceforge.net/">http://opencsv.sourceforge.net/</link> ) combinados con caracteres extraños como Emoji (<link xlink:href="http://www.unicode.org/faq/emoji_dingbats.html">http://www.unicode.org/faq/emoji_dingbats.html</link>).</simpara>
<simpara>Como consecuencia de los resultados anteriores, y apoyado en el soporte nativo ofrecido por Scala, se utilizó XML como el lenguaje de estructuración de datos que mejor podría serializar la información obtenida a través de Twitter4j. El siguiente fragmento de código permite ver lo sencillo que es serializar un objeto en Scala a XML:</simpara>
<programlisting language="scala" linenumbering="unnumbered">class Tweet(id:String, username: String, name:String, location: String, timezone: String, createdAt:String, latitude: String, longitude: String, text: String) {
  def toXML =
    &lt;tweet&gt;
      &lt;id&gt;
        {id}
      &lt;/id&gt;
      &lt;username&gt;
        {username}
      &lt;/username&gt;
      &lt;name&gt;
        {name}
      &lt;/name&gt;
      &lt;location&gt;
        {location}
      &lt;/location&gt;
      &lt;timezone&gt;
        {timezone}
      &lt;/timezone&gt;
      &lt;createdAt&gt;
        {createdAt}
      &lt;/createdAt&gt;
      &lt;latitude&gt;
        {latitude}
      &lt;/latitude&gt;
      &lt;longitude&gt;
        {longitude}
      &lt;/longitude&gt;
      &lt;text&gt;
        {text}
      &lt;/text&gt;
    &lt;/tweet&gt;
}</programlisting>
</section>
<section xml:id="_parámetros_del_sistema">
<title>Parámetros del sistema</title>
<simpara>Debido a que Falcon es un sistema sin ánimo de ejecutarse a través de una GUI, la manera de parametrizar la ejecución ha sido a través de una interfaz de línea de comandos. Para ello, se ha utilizado la biblioteca <emphasis role="strong">scopt</emphasis> (<link xlink:href="https://github.com/scopt/scopt">https://github.com/scopt/scopt</link>).</simpara>
<simpara>scopt permite parsear de manera sencilla los argumentos que se le pasan al programa en el momento de su ejecución. Para ello, simplemente hay que definir un objeto <literal>ScoptParser</literal> que contenga las reglas necesarias para especificar qué parámetros se esperan, qué tipo deben tener (<literal>String</literal>, <literal>Integer</literal>, <literal>Boolean</literal>, etc.) y si son requeridos u opcionales.</simpara>
<simpara>A continuación se muestra el mensaje de ayuda de la aplicación:</simpara>
<screen>Falcon 1.0
Usage: Falcon [options]

  -l &lt;value&gt; | --language &lt;value&gt;
        Specifies the language, in ISO 639-1 format, for the tweets to collect.
  -k &lt;value&gt; | --keywords &lt;value&gt;
        Specifies the file for the keywords.
  -t &lt;value&gt; | --time-in &lt;value&gt;
        The time measure for collecting tweets (SECONDS, MINUTES, HOURS, DAYS).
  -n &lt;value&gt; | --timestamp &lt;value&gt;
        Units of time for collecting tweets.
  -o &lt;value&gt; | --output &lt;value&gt;
        The output filename where store the collection results.
  -c &lt;value&gt; | --credentials &lt;value&gt;
        Properties file with the Twitter credentials
  -b &lt;value&gt; | --bounding-boxes &lt;value&gt;
        Specifies the file which contains the bounding boxes.</screen>
<itemizedlist>
<listitem>
<simpara><literal>--time-in</literal> y <literal>--timestamp</literal>: permiten establecer al recolector un tiempo de ejecución representado en diferentes magnitudes.</simpara>
</listitem>
<listitem>
<simpara><literal>--output</literal>: nombre del fichero de salida.</simpara>
</listitem>
<listitem>
<simpara><literal>--language</literal>: idioma en el que se desean obtener los tuits.</simpara>
</listitem>
<listitem>
<simpara><literal>--stopwords</literal>: debido a restricciones de Twitter, es necesario proveer una lista de términos cuando se intenta realizar un filtrado por idioma. Con el objetivo de restringir lo mínimo posible el número de tuits a obtener, se provee una lista de <emphasis>stop words</emphasis> del idioma por el que se esté filtrando.</simpara>
</listitem>
<listitem>
<simpara><literal>--bounding-boxes</literal>: fichero <literal>XML</literal> que contiene los <emphasis>bounding boxes</emphasis> sobre los que se realizará el filtrado.</simpara>
</listitem>
<listitem>
<simpara><literal>--credentials</literal>: indica el fichero que contiene las credenciales que se utilizarán para conectarse con la API de Twitter y utilizar su servicio de Streaming.</simpara>
</listitem>
</itemizedlist>
<simpara>Un ejemplo de uso para obtener datos en español sobre los bounding boxes de España durante un día, podría ser algo como esto:</simpara>
<screen>falcon.jar -l es -s es_stop_words.txt -t DAYS -n 1 -o es_tweets_collection -c credentials.properties -b spain_bounding_boxes.xml</screen>
</section>
<section xml:id="_ejemplo_de_resultados">
<title>Ejemplo de resultados</title>
<simpara>Un ejemplo de los resultados obtenidos por el recolector sería el siguiente:</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;tweet&gt;
  &lt;id&gt;
    469499350327255040
  &lt;/id&gt;
  &lt;username&gt;
    jessfhickey
  &lt;/username&gt;
  &lt;name&gt;
    Jessica Hickey
  &lt;/name&gt;
  &lt;location&gt;
    Ireland
  &lt;/location&gt;
  &lt;timezone&gt;
    London
  &lt;/timezone&gt;
  &lt;createdAt&gt;
    2014-05-22 15:25
  &lt;/createdAt&gt;
  &lt;latitude&gt;
    53.3444086
  &lt;/latitude&gt;
  &lt;longitude&gt;
    -6.2649497
  &lt;/longitude&gt;
  &lt;text&gt;
    In the most beautiful cafe ever. #boulevardcafe
  &lt;/text&gt;
&lt;/tweet&gt;</programlisting>
</section>
</appendix>
<appendix xml:id="_b2pick_aplicación_web_para_seleccionar_emphasis_bounding_boxes_emphasis">
<title>B2pick, aplicación web para seleccionar <emphasis>bounding boxes</emphasis></title>
<simpara>Uno de los procesos más habituales durante el desarrollo del proyecto fue la selección de bounding boxes para aplicar filtros sobre el Streaming de Twitter. Un bounding box, se puede definir como un rectángulo que abarca un área geográfica en concreto que queda definida por sus coordenadas suroeste y noreste.</simpara>
<simpara><emphasis role="strong">B2pick</emphasis> (<link xlink:href="https://github.com/sergio-alvarez/b2pick">https://github.com/sergio-alvarez/b2pick</link>), viene a cubrir la necesidad de poder automatizar el proceso mediante una sencilla aplicación web escrita puramente en JavaScript en la capa de cliente, que mediante la utilización de la API de Google Maps permita dibujar al usuario rectángulos sobre las zonas geográficas que desee.</simpara>
<simpara>Actualmente, no se conoce ninguna herramienta que ofrezca un servicio similar.</simpara>
<simpara>Una vez el usuario ha seleccionado los bounding boxes que desea, B2pick ofrece la posibilidad de descargarlos en formato XML (que es el formato que se ha utilizado a la hora de desarrollar los prototipos), siendo posible en el futuro adaptar fácilmente el formato de salida a otros también populares como JSON o CSV.</simpara>
<formalpara>
<title>Ejemplo de salida XML para un conjunto de Bounding Boxes</title>
<para>
<programlisting language="xml" linenumbering="unnumbered">&lt;boundingBoxes&gt;
  &lt;boundingBox&gt;
    &lt;sw&gt;
      &lt;latitude&gt;35.75&lt;/latitude&gt;
      &lt;longitude&gt;-12.04&lt;/longitude&gt;
    &lt;/sw&gt;
    &lt;ne&gt;
      &lt;latitude&gt;44.53&lt;/latitude&gt;
      &lt;longitude&gt;4.22&lt;/longitude&gt;
    &lt;/ne&gt;
  &lt;/boundingBox&gt;
  &lt;boundingBox&gt;
    &lt;sw&gt;
      &lt;latitude&gt;50.23&lt;/latitude&gt;
      &lt;longitude&gt;-13.89&lt;/longitude&gt;
    &lt;/sw&gt;
    &lt;ne&gt;
      &lt;latitude&gt;59.27&lt;/latitude&gt;
      &lt;longitude&gt;1.85&lt;/longitude&gt;
    &lt;/ne&gt;
  &lt;/boundingBox&gt;
  &lt;boundingBox&gt;
    &lt;sw&gt;
      &lt;latitude&gt;54.72&lt;/latitude&gt;
      &lt;longitude&gt;6.24&lt;/longitude&gt;
    &lt;/sw&gt;
    &lt;ne&gt;
      &lt;latitude&gt;57.61&lt;/latitude&gt;
      &lt;longitude&gt;12.83&lt;/longitude&gt;
    &lt;/ne&gt;
  &lt;/boundingBox&gt;
&lt;/boundingBoxes&gt;</programlisting>
</para>
</formalpara>
<figure>
<title>Imagen de la pantalla principal de B2pick</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/b2pick-main-screen.png" align="center"/>
</imageobject>
<textobject><phrase>Imagen de la pantalla principal de b2pick</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Ejemplo del lightbox de salida con los bounding boxes seleccionados</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/b2pick-lightbox.png" align="center"/>
</imageobject>
<textobject><phrase>Ejemplo del lightbox de salida con los bounding boxes seleccionados</phrase></textobject>
</mediaobject>
</figure>
</appendix>
<appendix xml:id="_tweetheat_mapa_de_calor_sobre_ficheros_tsv_de_puntuación">
<title>TweetHeat, mapa de calor sobre ficheros TSV de puntuación</title>

</appendix>
</book>