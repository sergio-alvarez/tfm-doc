<?xml version="1.0" encoding="UTF-8"?>

<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" lang="es">
<info>
<title>Geolocalización de usuarios en medios sociales mediante análisis de contenidos</title>
<date>2014-06-29</date>
<author>
<personname>
<firstname>Sergio</firstname>
<othername>Álvarez</othername>
<surname>Suárez</surname>
</personname>
</author>
<authorinitials>SÁS</authorinitials>
</info>
<preface xml:id="_resumen">
<title>Resumen</title>
<simpara>El presente proyecto recoge la implementación de un sistema capaz de inferir la localización de un usuario a través de sus publicaciones en Twitter combinando métodos estadísticos y aprendizaje automático.</simpara>
<simpara>Para demostrar su efectividad, se han realizado experimentos a distintos niveles de granularidad (país, provincia, área metropolitana y barrio) y con dos idiomas diferentes (inglés y español). El algoritmo ha mostrado niveles de precisión bastante altos (prácticamente siempre superiores al 90%) y un grado de exhaustividad (especialmente para las granularidades de más alto nivel) muy aceptable, identificando aproximadamente un 70% ~ 80% del total de tuits locales recibidos.</simpara>
<simpara>Para realizar el desarrollo del sistema, el proyecto ha hecho uso de métodos estadísticos como Log-Likelihood Ratio y el software de aprendizaje automático Vowpal Wabbit.</simpara>
<simpara>Por último, el proyecto recoge el diseño de una aplicación web que utilizaría la tecnología previamente desarrollada para ofrecer un servicio de geolocalización de publicaciones en redes sociales (en este caso, centrado únicamente en Twitter) mediante análisis de contenidos.</simpara>
</preface>
<acknowledgements xml:id="_agradecimientos">
<title>Agradecimientos</title>
<simpara>Me gustaría dar las gracias, en primer lugar, a mi familia, mis padres, mi hermana y mi novia por el apoyo incondicional que me han brindado durante estos últimos meses para ser capaz de finalizar con éxito mi Trabajo Fin de Máster.</simpara>
<simpara>También quisiera agradecer a Daniel Gayo Avello, director del proyecto, su ayuda constante durante todo el proceso de creación y desarrollo del proyecto así como su disponibilidad y paciencia para aportar soluciones a todas aquellas dificultades que me pude ir encontrando durante el transcurso del mismo.</simpara>
</acknowledgements>
<preface xml:id="_palabras_clave">
<title>Palabras clave</title>
<simpara>twitter, log likelihood ratio test, vowpal wabbit, palabras indicativas de posición, scala, geolocalización, play framework</simpara>
</preface>
<chapter xml:id="_introducción">
<title>Introducción</title>
<section xml:id="_motivación_del_proyecto">
<title>Motivación del proyecto</title>
<simpara>El crecimiento exponencial de las redes sociales durante los últimos años ha despertado un gran interés en los diferentes ámbitos de la informática, siendo un claro objetivo comercial para profesionales del sector, así como un nuevo campo de investigación para los investigadores universitarios.</simpara>
<simpara>Como consecuencia de todo ello, durante los últimos años han ido apareciendo diversas aplicaciones que, de una u otra manera, se centran en estudiar ciertos aspectos de las redes sociales para poder extraer información acerca de sus usuarios gracias a las diversas publicaciones que estos mismos realizan en sus perfiles.</simpara>
<simpara>El estudio de la geolocalización de un usuario a partir del contenido publicado por él mismo, sin embargo, es una de las pocas áreas que <emphasis>tan sólo</emphasis> agrupa un pequeño número de estudios, pero en donde no han proliferado herramientas que comprueben de manera empírica los resultados obtenidos por diversos investigadores.</simpara>
<simpara>La motivación de este proyecto, por tanto, es crear un sistema basado en las investigaciones previas, que permita inferir la geolocalización de un usuario mediante el análisis del contenido de sus publicaciones en Twitter y demostrar empíricamente si las soluciones propuestas pueden ser aceptadas como válidas.</simpara>
</section>
<section xml:id="_alcance">
<title>Alcance</title>
<simpara>El objetivo principal del proyecto es desarrollar un sistema capaz de inferir la geolocalización de un usuario en base al estudio de sus publicaciones en Twitter. Para ello, se implementará una solución basada en los estudios llevados a cabo por diversos investigadores (tanto universitarios como empresariales) a lo largo de los últimos años.</simpara>
<simpara>El sistema deberá ser capaz de trabajar sobre diversas granularidades (país, provincia, área metropolitana y barrio) y utilizar tuits escritos en diversos idiomas (como mínimo, deberá funcionar satisfactoriamente con inglés y español). Para inferir la gelocalización de un tuit se basará únicamente en el contenido de la publicación, evitando la comunicación con sistemas de geolocalización externos y siendo capaz de detectar términos discriminativos que ayuden a identificar un tuit como perteneciente a un área geográfica en concreto.</simpara>
<simpara>Finalmente, el proyecto deberá plantear un sistema que pueda ser integrable en una aplicación web que se aproveche de sus capacidades para ofrecer un servicio de geolocalización de tuits basado en su contenido.</simpara>
</section>
<section xml:id="_estado_del_arte">
<title>Estado del arte</title>
<simpara>Desde la aparición de Twitter en 2006 y su irrupción como servicio de masas en torno al año 2008, han ido apareciendo de manera exponencial diferentes servicios y vías de investigación centradas en estudiar el comportamiento de los usuarios en la red social en base a sus publicaciones, con el objetivo de poder extraer conclusiones acerca de su tendencia política, opiniones personales y otras características individuales.</simpara>
<note>
<title>Twitter</title>
<simpara>Twitter es una red social (también clasificada como servicio de <emphasis>microblogging</emphasis>) creada en 2006 por Jack Dorsey. Su principal característica es el envío de <emphasis>posts</emphasis> de 140 caracteres por parte de los usuarios y la capacidad de que estos mismos formen su propio <emphasis>timeline</emphasis> suscribiéndose a las publicaciones realizadas por el resto de usuarios.</simpara>
</note>
<simpara>Sin embargo, un problema habitual a la hora de trabajar con datos extraídos de Twitter es su falta de estructuración. Si bien es habitual que los usuarios en redes sociales como Facebook mantengan un perfil completo y actualizado con datos reales (en parte gracias al buen hacer de dicha red social por conseguir que así sea), en Twitter lo más habitual es carecer de dicha información. Por tanto, muchos de los esfuerzos realizados en el análisis de contenidos en Twitter vienen para cubrir esa necesidad de obtener información real acerca de los usuarios que realizan las publicaciones.</simpara>
<simpara>Por poner un ejemplo, la figura <xref linkend="fb-profile"/> muestra una captura del perfil del autor en Facebook. En ella, se puede observar como aparece una gran cantidad de información perfectamente estructurada: centros de estudio, lugar de nacimiento, lugar de residencia actual, fecha de nacimiento, sexo e incluso campos que, aunque en este caso no estén completos, podrían aportar muchísima más información sobre el usuario: relación sentimental, ideología política o religiosa, etc.</simpara>
<figure xml:id="fb-profile">
<title>Captura del perfil del autor en Facebook</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/introduction/fb_profile.png" align="center"/>
</imageobject>
<textobject><phrase>fb profile</phrase></textobject>
</mediaobject>
</figure>
<simpara>Sin embargo, los perfiles en Twitter son mucho más limitados. Por utilizar el mismo símil, en <xref linkend="twitter-profile"/> se muestra una captura del perfil del autor en Twitter. Como se puede observar, en este caso tan sólo se muestra una pequeña biografía en la que el usuario aprovecha para reflejar su perfil más profesional.</simpara>
<figure xml:id="twitter-profile">
<title>Captura del perfil del autor en Twitter</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/introduction/twitter_profile.png" contentdepth="200px" align="center"/>
</imageobject>
<textobject><phrase>twitter profile</phrase></textobject>
</mediaobject>
</figure>
<simpara>En el caso de la geolocalización de usuarios, Bren Hercht <emphasis>et al.</emphasis> en su artículo <emphasis role="strong">Tweets from Justin Bieber’s Heart: The Dynamics of the "Location" Field in User Profiles</emphasis>, centrado en estudiar el campo de <emphasis role="strong">Localización</emphasis> de los perfiles de usuario, realiza la siguiente afirmación acerca de la cantidad de tuits geoposicionados (es decir, con información geográfica adjunta como latitud y longitud) que contenían en su dataset de ejemplo:</simpara>
<blockquote>
<simpara>First, our focus is on the geographic information revealed in the “location” field of user profiles, a type of geographic information that is prevalent across the Web 2.0 world. <emphasis role="strong">Second, we found that only 0.77% of our 62 million tweets contained this embedded location information</emphasis>.</simpara>
</blockquote>
<simpara>De 62 millones de tuits, únicamente un 0.77% (~= 477.400) contenían información geográfica adjunta. Como se puede deducir de esta afirmación, el problema para geolocalizar usuarios en Twitter es real y su solución sería de gran interés para poder contribuir a un mejor estudio de las publicaciones de los usuarios en la red social, obteniendo la capacidad de situarlos en un espacio geográfico concreto.</simpara>
<simpara>A continuación, se recogen una serie de artículos de investigación que abordan la problemática anterior y proponen mecanismos para inferir la localización de un usuario en base al contenido de sus publicaciones.</simpara>
<section xml:id="_estudios_y_artículos_de_investigación">
<title>Estudios y artículos de investigación</title>
<simpara>Se pueden encontrar varias tendencias en la investigación de la geolocalización de los tuits y usuarios de Twitter. Por un lado, existen investigadores que basan sus estudios en encontrar términos o entidades con significado geográfico propio, que se puedan cotejar con un <emphasis>gazetteer</emphasis> con el objetivo de poder identificar nombres de lugares o coordenadas geográficas, intuyendo que aquellas que más veces aparezcan en el <emphasis>timeline</emphasis> del usuario son aquellas más próximas a su localización real.</simpara>
<note>
<title>Timeline de Twitter</title>
<simpara>El timeline de un usuario en Twitter se puede entender como la vista principal del usuario en la que puede observar, en tiempo real, las publicaciones realizadas por aquellos usuarios a los que está suscrito (o a los que <emphasis>sigue</emphasis>, si se prefiere utilizar la terminología propuesta por Twitter).</simpara>
</note>
<simpara>Por otro lado, grupos de investigadores han optado por estudiar únicamente el contenido de un tuit con el objetivo de encontrar aquellas palabras más características de un lugar y que puedan ser lo suficientemente discriminativas. En estos casos, se suele tender a buscar aquellos términos con una alta frecuencia y una baja dispersión geográfica.</simpara>
<note>
<title>Gazetteer</title>
<simpara>Un gazetteer (o <emphasis role="strong">nomenclátor</emphasis> en español) es un diccionario geográfico que contiene los nombres de varias localizaciones geográficas referentes a un lugar. Por lo general, es habitual disponer de gazetteers a nivel de país para indicar el conjunto de sus ciudades, comunidades autónomas o provincias. Así como para casi cualquier otra unidad administrativa.</simpara>
</note>
<simpara>Existen también enfoques donde se ha optado por una estrategia basada en estudiar el comportamiento de un usuario, utilizando técnicas de los dos métodos anteriores y combinándola con la información obtenida a través de servicios de terceros que puedan realizar publicaciones en el <emphasis>timeline</emphasis> del usuario y además adjuntar información geolocalizada (<emphasis role="strong">Foursquare</emphasis> sería un claro ejemplo de este tipo de servicios).</simpara>
<note>
<title>Foursquare</title>
<simpara>Foursquare es una aplicación desarrollada originalmente para dispositivos móviles, que funciona en parte con las características propias de cualquier red social, permitiendo a cada usuario publicar su ubicación actual al resto de sus contactos. A diferencia de otras redes sociales más generales (como Facebook o Twitter), Foursquare tiene como objetivo compartir localizaciones concretas que tienen adjunta información geográfica. Su explosión como servicio se basó en su interoperabilidad con el resto de servicios como Facebook, Twitter o Google+ permitiendo que las publicaciones en Foursquare se pueden observar desde el resto de redes sociales del usuario.</simpara>
</note>
<section xml:id="_tweets_from_justin_bieber_s_heart_the_dynamics_of_the_location_field_in_user_profiles">
<title>Tweets from Justin Bieber’s Heart: The Dynamics of the "Location" Field in User Profiles</title>
<simpara><emphasis>Por Brent Hecht et al. Northwestern University y Palo Alto Research Center</emphasis></simpara>
<simpara>El estudio liderado por Brent Hecht demuestra como el uso del campo de Localización disponible en los perfiles de los usuarios en Twitter no es un indicador válido para obtener su posición geográfica real.</simpara>
<simpara>Entre los datos obtenidos por Brent y su equipo se pueden destacar los siguientes:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Sólo un 66% de los usuarios utiliza el campo de Localización para aportar información geográfica válida (es decir, localizaciones reales pero que <emphasis role="strong">no tienen porqué indicar su situación real</emphasis>. Por ejemplo, si una persona de <emphasis role="strong">Oviedo</emphasis> escribe <emphasis role="strong">California</emphasis> en el campo de Localización de su perfil, este estudio lo incluye dentro del 66% anterior).</simpara>
</listitem>
<listitem>
<simpara>Algunos de los usos que los usuarios dentro del 34% restante aplican al campo de Localización se puede observar en <xref linkend="location-use-type"/>.</simpara>
</listitem>
<listitem>
<simpara>La manera de distinguir entre localización real y ficticia supuso de un proceso manual en el que dos miembros del equipo debieron revisar tuit a tuit el campo de Localización, debido a la habilidad de los usuarios para poder expresar sarcasmo o ironía, así como expresiones comunes que pueden tener asociado un componente geográfico inherente (un ejemplo puede ser referirse al Principado de Asturias como <emphasis>la tierrina</emphasis>).</simpara>
</listitem>
</orderedlist>
<table xml:id="location-use-type" frame="all" rowsep="1" colsep="1">
<title>Tipos de uso del campo de Localización por parte de los usuarios de Twitter</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Cultura popular</simpara></entry>
<entry align="left" valign="top"><simpara>12.9%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Referencias a su propia privacidad</simpara></entry>
<entry align="left" valign="top"><simpara>1.2%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Insultos o contenido violento</simpara></entry>
<entry align="left" valign="top"><simpara>4.6%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Localizaciones no terráqueas</simpara></entry>
<entry align="left" valign="top"><simpara>5.0%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Emociones negativas hacia su localización real</simpara></entry>
<entry align="left" valign="top"><simpara>3.2%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Naturaleza sexual</simpara></entry>
<entry align="left" valign="top"><simpara>3.2%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>Como alternativa a los resultados anteriores, y buscando una manera de automatizar el proceso y encontrar resultados más fiables y exhaustivos, se propusieron hacer un primer experimento para comprobar si el estudio de los contenidos publicados por un usuario pueden aportar la información necesaria para permitir inferir su ubicación geográfica.</simpara>
<note>
<title>Clasificador Bayesiano</title>
<simpara>De manera simplificada, se podría definir un <emphasis role="strong">clasificador Bayesiano</emphasis> como un clasificador probabilístico basado en el teorema de Bayes, que permite trabajar con una serie de características de manera independiente, sin asumir que la ausencia o presencia de cada una influya en el valor que se otorga a las demás para calcular la predicción final.</simpara>
</note>
<simpara>Para ello, utilizaron un software de aprendizaje automático y un <emphasis role="strong">clasificador Bayesiano</emphasis> multinomial que en base a un conjunto de datos obtenidos a partir de aplicar el algoritmo <emphasis>CALGARI</emphasis> (de implementación propia), fuese capaz de predecir a qué área (País y Estado) pertenece un tuit en base a su contenido.</simpara>
<note>
<title>CALGARI</title>
<simpara>El algoritmo CALGARI tiene como objetivo normalizar la frecuencia con la que un término ha aparecido dentro de un dataset de tuits para priorizar aquellos que son más específicas de un área (ciudad o estado) en concreto, penalizando palabras comunes como <emphasis>ya, hola, adiós, etc.</emphasis></simpara>
</note>
<simpara>Entre los resultados ofrecidos por el estudio destacan un <emphasis role="strong">72.7% de precisión para inferir el país</emphasis> de un usuario pero tan <emphasis role="strong">sólo un 30% de precisión a nivel de estado</emphasis>.</simpara>
</section>
<section xml:id="_where_is_this_tweet_from_inferring_home_locations_of_twitter_users">
<title>Where Is This Tweet From? Inferring Home Locations of Twitter Users</title>
<simpara><emphasis>Por Jalal Mahmud et al. IBM Research</emphasis></simpara>
<simpara>Con el objetivo de poder identificar un tuit a diferentes granularidades: ciudad o estado, el estudio plantea la posibilidad de analizar tres tipos de términos diferentes para localizar una publicación en Twitter:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><emphasis role="strong">Palabras</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Hashtags</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Nombres de lugares</emphasis> (utilizando un gazetteer geográfico). Puesto que estos términos podía estar compuestos por más de una palabra, se utilizaron bigramas y trigamas, así como un heurístico especializado en reconocer nombres de lugares expresados mediante vocabulario común (un ejemplo sería <emphasis>Red Sox</emphasis> para referirse a la ciudad de Boston).</simpara>
</listitem>
</orderedlist>
<note>
<title>Hashtag</title>
<simpara>Un hashtag es un término que comienza con el literal <literal>#</literal> y sirve para categorizar el contenido de un tuit.</simpara>
</note>
<simpara>Es interesante observar como empiezan a aparecer pequeñas diferencias entre términos, considerando que en función de su categoría, pueden ofrecer más o menos información geográfica. Esta misma estrategia será también utilizada en el presente proyecto, mediante la extracción de Hashtags, Menciones y N-gramas.</simpara>
<simpara>Con el objetivo de minimizar la aparición de ruido, normalizaron el contenido de cada tuit eliminando signos de puntuación (a excepción de aquellos que indican una entidad propia cuando se encuentran al principio de una palabra, como <literal>#</literal> para indicar <emphasis>hashtags</emphasis>) y palabras vacías.</simpara>
<simpara>También se hace mención a la utilización de un <emphasis role="strong">software de aprendizaje automático</emphasis>, en este caso WEKA, y su conjunción con un modelo estadístico que realice los cálculos necesarios para el clasificador. El modelo que seleccionaron de manera empírica fue un clasificador Bayesiano multinomial.</simpara>
<note>
<title>WEKA</title>
<simpara>WEKA es una implementación de software de aprendizaje automático realizada en Java por la Universidad de Waikato en Nueva Zelanda. Es uno de los sistemas más utilizados debido a su soporte para aplicar un gran número de algoritmos de aprendizaje automático.</simpara>
</note>
<simpara>La estrategia propuesta en este trabajo para inferir la localización de un usuario en Twitter fue:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>A lo largo de sus tuits, mencionará más veces su ciudad o estado de origen que el resto de ciudades o estados.</simpara>
</listitem>
<listitem>
<simpara>Visitará más lugares de su ciudad o estado de origen que del resto de ciudades o estados (para detectar este tipo de visitas, se guardan todas las URLs generadas a partir de <emphasis>check-ins</emphasis> compartidos a través de <emphasis role="strong">Foursquare</emphasis> para luego comprobar su información asociada a través de la propia API de Foursquare).</simpara>
</listitem>
</orderedlist>
<simpara>A partir de estas premisas y de las decisiones anteriores, se crearon 3 modelos diferentes para poder entrenar sobre cada uno de los términos que se quieren extraer: palabras, hashtags y nombres de lugares. Los resultados presentados a nivel de ciudad no fueron realmente positivos, y sólo presentan niveles de precisión superiores al 70% cuando se permiten márgenes de error superiores a 200 millas (~= 322 kilómetros).</simpara>
<simpara>Por último, no se especifica con exactitud cómo actúa realmente el algoritmo cuando se trabaja con usuarios que no tienen contenido generado por Foursquare o no hacen una referencia explícita a su ciudad, estado o país.</simpara>
</section>
<section xml:id="_tweolocator_a_non_intrusive_geographical_locator_system_for_twitter">
<title>TweoLocator: A Non-Intrusive Geographical Locator System for Twitter</title>
<simpara><emphasis>Por Yi-Shin Chen et al. National Tsing Hua University</emphasis></simpara>
<simpara>En este estudio, Yi-Shin Chen diseña un sistema que a través de diferentes etapas y aglutinando varios procesos es capaz de inferir la localización de un usuario en Twitter en función del contenido de sus tuits.</simpara>
<variablelist>
<varlistentry>
<term>Clasificación base</term>
<listitem>
<simpara>A partir de un gran dataset de usuarios de Twitter, en esta fase se realiza un análisis para comprobar qué perfiles puede ser potencialmente válidos para realizar un análisis de contenidos, eliminando aquellos que puedan pertenecer a <emphasis>bots</emphasis> automáticos o sean perfiles de spam. Una vez se obtiene una masa de usuarios válidos se procede, dentro aún de esta etapa, a analizar todos sus tuits (a excepción de aquellos con información de geolocalización asociada) para volver a categorizarlos en 3 tipos:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Direct subject</emphasis>: Tuits que hacen referencia al usuario en primera persona.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Anonymous subject</emphasis>: Tuits que no hacen una referencia directa al usuario, pero utilizan otros pronombres personales o la primera secuencia de palabras es un verbo que no es una palabra vacía.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Others</emphasis>: Tuits descartados por no pertenecer a ninguna de las 2 categorías anteriores.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Generación de reglas</term>
<listitem>
<simpara>Una vez todos los tuits anteriores han sido analizados semánticamente se realiza una normalización de los mismos aplicando técnicas de análisis de texto (utilizando un tokenizador y un stemmer) para luego poder formar n-gramas como los mismos. Durante esta etapa, se intentan inferir reglas que permitan asociar términos comunes a localizaciones específicas como aeropuertos, parques, estaciones de tren, etc.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Descubrimiento de localizaciones</term>
<listitem>
<simpara>A partir de los términos de cada tuit, se generan trigramas, bigramas y unigramas y se comparan sobre un gazetteer y las reglas generadas en el paso anterior, obteniendo localizaciones que se pueden agrupar en:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Explicit Specific</emphasis>: Nombres que hacen una referencia directa a una ciudad o lugar determinado, como por ejemplo «The White House» o «Los Ángeles».</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Explicit</emphasis>: Nombres que hacen referencia a localizaciones generales como parques o gimnasios.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Implicit</emphasis>: Combinaciones de palabras que implícitamente sugieren una localización. Estos resultaos se obtienen a partir de las reglas generadas en el paso anterior.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Eliminación de topónimos</term>
<listitem>
<simpara>Mediante la utilización de un clúster, y partiendo de la premisa de que un usuario nombrará con mayor frecuencia lugares cercanos a su lugar de origen, en esta fase se analizan las menciones realizadas por el usuario sobre ciudades, lugares, países y se refinarán los datos para obtener su lugar de origen.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Ordenación temporal</term>
<listitem>
<simpara>Es el último paso en el refinamiento de los datos. En esta fase se intenta minimizar la aparición de ruido detectando aquellas ocasiones en las que el usuario hace referencia a una localización geográfica sin aportar una información real acerca de su posición. Por ejemplo, es habitual que alguien situado en Asturias pueda nombrar la ciudad de Nueva York para hablar de alguna noticia o para mostrar sus ganas por conocer la ciudad, sin que esa mención indique que se encuentre realmente allí. Para resolver este problema, y aceptando que en algunos casos sólo se podrían resolver dichas inconsistencias de manera manual mediante la intervención humana, se diseñó un sistema que a partir de dos tuits con contenido geolocalizado consecutivos (del mismo usuario) compruebe si su diferencia en el tiempo es acorde a la posibilidad de haberse movido entre ambos puntos a una velocidad normal de transporte.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Localización inferida</term>
<listitem>
<simpara>De acuerdo a los resultados obtenidos en todas las fases anteriores y de acuerdo al nivel sobre que el que se haya podido inferir su localización, los usuarios son clasificados en los siguientes grupos:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">No information</emphasis>: Si no se ha podido obtener información geográfica válida para inferir la localización del usuario.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Just country</emphasis>: Si sólo se ha podido inferir el país del usuario.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Timeline</emphasis>: Se han podido detectar ubicaciones actuales y previas del usuario, pero no su lugar de origen.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Hometown</emphasis>: Se han podido detectar ubicaciones actuales y previas del usuario y <emphasis role="strong">también</emphasis> su lugar de origen. Es el grupo con información más completa.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<simpara>En las conclusiones que se exponen en el artículo se muestran unos resultados bastante aceptables, donde hay porcentajes de precisión cercanos al 80%. Al igual que en el caso anterior, TweoLocator tiene una gran dependencia de que los usuarios incluyan en el contenido de sus tuits información explícitamente geolocalizable.</simpara>
</section>
<section xml:id="_a_multi_indicator_approach_for_geolocalization_of_tweets">
<title>A Multi-Indicator Approach for Geolocalization of Tweets</title>
<simpara><emphasis>Por Axel Schulz et al. SAP Research</emphasis></simpara>
<simpara>En este artículo, un equipo de investigación de <emphasis role="strong">SAP AG</emphasis> presenta un enfoque muy interesante para inferir la localización de un usuario mediante la utilización de formas poligonales en 3D. Los polígonos se superponen, y la intersección de mayor altura es el área con más probabilidades de contener al usuario analizado.</simpara>
<simpara>La altura de cada polígono viene determinada por pesos específicos que se aplican en función de la fuente utilizada para obtener esa localización. Cada fuente tiene sus propios estándares de calidad y sus propias métricas para indicar más o menos fiabilidad.</simpara>
<simpara>Para obtener las coordenadas o posiciones geográficas que deben ocupar los polígonos, los investigadores extraen información de los siguientes campos:</simpara>
<variablelist>
<varlistentry>
<term>Contenido del tuit</term>
<listitem>
<simpara>Se optó por utilizar <emphasis role="strong">DBPedia Spotlight</emphasis> para extraer las entidades que existían en el tuit. Con los resultados de la extracción, se seleccionaban únicamente aquellas que tenían coordenadas asociadas. Además, se utilizó como calidad de cada predicción la propia confianza aportada por DBPedia Spotlight en su resultado. También se utilizaron las publicaciones realizadas a través de servicios como Foursquare, Flickr o Ubisoft, las cuales tienen adjunta información geográfica precisa mediante la utilización de coordenadas geográficas.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Localización</term>
<listitem>
<simpara>Se hizo uso de gazetteers que permitiesen buscar coincidencias textuales en el campo de Localización. Además, se volvió a utilizar DBPedia Spotlight para conseguir trabajar con expresiones comunes como «La gran manzana» y expresiones regulares para detectar si algún usuario incluía coordenadas geográficas directamente en su campo de Localización.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Web del usuario</term>
<listitem>
<simpara>Para aquellos usuarios que añaden en su perfil su página web personal se aplican dos estrategias:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Extraer el dominio de la página (.com, .es, etc).</simpara>
</listitem>
<listitem>
<simpara>Utilizar la dirección IP y obtener las coordenadas a través del servicio IPInfoDB.</simpara>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Zona horaria</term>
<listitem>
<simpara>Se asume como cierto que la zona horaria asociada al usuario es la capital de su país de origen.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Los resultados de este estudio presentan mejorías respecto a otras investigaciones basadas en inferir la localidad de un usuario mediante el uso de <emphasis role="strong">múltiples indicadores</emphasis> con un 37% de precisión con una distancia de error de 10km y un 48% para 25km; así como un 54% cuando el margen se amplía a 50km.</simpara>
</section>
<section xml:id="_inferring_the_origin_locations_of_tweets_with_quantitative_confidence">
<title>Inferring the Origin Locations of Tweets with Quantitative Confidence</title>
<simpara><emphasis>Por Reid Priedhorsky et al. Los Alamos National Laboratory y Northeastern Illinois University</emphasis></simpara>
<simpara>El artículo parte de la premisa de que no es posible obtener la localización de un tuit con una exactitud total, sino que lo más acertado es ofrecer un modelo probabilístico que muestre las diferentes localizaciones a las que un tuit puede pertenecer asociadas a un grado de confianza (probabilidad).</simpara>
<simpara>Para obtener un dataset de entrenamiento, se utilizó la API Streaming de Twitter para después realizar un procesamiento de cada tuit extrayendo información de los campos: descripción del usuario, idioma del perfil seleccionado, campo de localización, zona horaria y contenido del tuit. Sobre esta información, se extrajeron bigramas para todos los términos adyacentes (a excepción del campo de zona horaria). Además, también se almacenó la información geográfica adjunta al tuit para poder realizar los experimentos y el entrenamiento del modelo.</simpara>
<note>
<title>Twitter Streaming API</title>
<simpara>Conjunto de APIs ofrecidas por Twitter que permiten acceder al streaming real de tuits que gestiona la aplicación. Se ofrecen 3 tipos de streaming en función del contenido que se desee analizar:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="66*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Stream público</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Ofrece un 1% del total de tuits públicos gestionados por la aplicación. Se suele utilizar principalmente para tareas de <emphasis>data mining</emphasis> donde no se requiere establecer ningún filtro en concreto en base a ningún usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Stream de usuarios</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Permite obtener información en tiempo real acerca de los eventos recibidos por un único usuario de Twitter.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Stream de <emphasis>Sites</emphasis></emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Ofrece la capacidad de obtener información en tiempo real de los eventos recibidos por un conjunto de usuarios. Está pensado para aplicación web o móviles que requieren monitorizar la actividad en Twitter de un conjunto de sus usuarios.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>La página web de documentación en Twitter acerca de las APIs de Streaming contiene una mayor información acerca de sus capacidades y la manera de interactuar con ellas: <link xlink:href="https://dev.twitter.com/docs/api/streaming">https://dev.twitter.com/docs/api/streaming</link></simpara>
</note>
<simpara>Una vez con toda esta información almacenada, se utilizó una técnica de estimación denominada «gaussian mixture models» en donde cada bigrama que aparezca más de un mínimo número de veces se asocia a las coordenadas del tuit que lo contiene. Cada asociación va vinculada a un peso específico en función del bigrama y la suma de todos los pesos relativos a un mismo tuit resulta en la probabilidad total de que pertenezca a dichas coordenadas.</simpara>
<note>
<title>Gaussian mixture models</title>
<simpara>Un modelo Gaussiano mixto es un modelo probabilístico que asume que todos sus datos fueron generados a partir de la unión de un número finito de distribuciones Gaussianas con parámetros desconocidos.</simpara>
<simpara>— <link xlink:href="http://scikit-learn.org/stable/modules/mixture.html">http://scikit-learn.org/stable/modules/mixture.html</link></simpara>
</note>
<simpara>Para calcular el peso total de un tuit a partir del peso de cada bigrama, el equipo de Reid Priedhorsky desarrolló tres métodos diferentes:</simpara>
<variablelist>
<varlistentry>
<term>Peso por propiedades de calidad</term>
<listitem>
<simpara>Se realiza la suma de todos los pesos asignados para cada bigrama y se utiliza su total como el total del tuit.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Peso por error inverso</term>
<listitem>
<simpara>Se calcula el error de cada bigrama utilizando el conjunto de datos de entrenamiento y se realiza la suma de todos los errores obtenidos. Después, se aplica el inverso sobre el error total, para obtener un valor que implique que cuanto mayor sea el error, menor sea el peso del tuit.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Peso por optimización</term>
<listitem>
<simpara>Aprovechando los dos casos anteriores, se plantea una nueva característica que pueda aportar el peso a cada bigrama utilizando la información que pueden dar tanto sus propiedades de calidad como su valor de error sobre el error total. De esta manera, se calcularía un nuevo valor para cada bigrama que, en conjunto con el del resto de bigramas que componen el tuit, resultaría en el peso total para dicho tuit.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Todos los algoritmos anteriores contienen una alta dosis de componente matemático.</simpara>
<note>
<title>N-Grama</title>
<simpara>En este contexto, un n-grama es una combinación de <emphasis role="strong">n</emphasis> palabras contenidas en un tuit. A lo largo del proyecto se hará referencia a dos tipos de n-gramas: <emphasis role="strong">unigramas</emphasis> (combinaciones de 1 palabra) y <emphasis role="strong">bigramas</emphasis> (combinaciones de 2 palabras).</simpara>
<simpara>Un ejemplo de la extracción de bigramas sobre el contenido de un tuit sería:</simpara>
<simpara>«Obviamente todo esto se hace para acabar con la costumbre de nuestra infancia de los álbumes de cromos.»</simpara>
<simpara>Que se traduciría a:</simpara>
<screen>Obviamente todo, todo esto, esto se, se hace,
hace para, para acabar, acabar con, con la,
la costumbre, costumbre de, de nuestra, nuestra infancia,
infancia de, de los, los álbumes, álbumes de, de cromos</screen>
</note>
<simpara>Los resultados del estudio revelaron un precisión del 83% para aquellos tuits que contenían bigramas con contenido explícitamente localizable (nombres de lugares) frente a un 57% de precisión sobre tuits sin información geográfica.</simpara>
</section>
<section xml:id="_you_are_where_you_tweet_a_content_based_approach_to_geo_locating_twitter_users">
<title>You Are Where You Tweet: A Content-Based Approach to Geo-locating Twitter Users</title>
<simpara><emphasis>Por Zhiyuan Cheng et al. Texas A&amp;M University</emphasis></simpara>
<simpara>En este estudio, Zhiyuan Cheng y su equipo aportan datos interesantes en la investigación de la geolocalización en Twitter. Centrados en buscar un algoritmo capaz de inferir la localización de un usuario <emphasis role="strong">únicamente</emphasis> con el contenido de sus tuits, establecen tres criterios que serán ampliamente utilizados por el resto de investigaciones posteriores:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Se deben buscar términos con un fuerte componente discriminativo mediante la aplicación de algoritmos que sirvan para normalizar la frecuencia de apariciones de un término.</simpara>
</listitem>
<listitem>
<simpara>El test Likelihood Ratio es capaz de obtener puntuaciones bastante acertadas para este dominio específico.</simpara>
</listitem>
<listitem>
<simpara>Los términos más altamente discriminativos se caracterizan por una alta frecuencia y una baja dispersión.</simpara>
</listitem>
</orderedlist>
<note>
<title>Likelihood Ratio test</title>
<simpara>El test Likelihood-Ratio es un método estadístico ampliamente utilizado en problemas donde se pretenden comparar dos conjuntos de datos a través de una serie de supuestos. En <xref linkend="_log_likelihood_ratio_test"/> se exponen de manera detallada sus principales características y su aplicación en el proyecto actual.</simpara>
</note>
<simpara>Entre los resultados que presentaron, afirman ser capaces de localizar correctamente el 51% de los tuits dentro de un radio de error de 100 millas (~= 161 km.).</simpara>
</section>
<section xml:id="_otros_artículos_de_interés">
<title>Otros artículos de interés</title>
<simpara>Los artículos anteriores han sido seleccionados como los más representativos de las principales vías de investigación para inferir la localización de un usuario en redes sociales en base al contenido de sus publicaciones. A continuación, se enumeran otros artículos de interés que pueden ayudar a conocer vías alternativas sobre las investigaciones propuestas así como nuevos resultados.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Location Type Classification Using Tweet Content</emphasis> <emphasis>por Haibin Liu et al. The Pennsylvania State University</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">TweetLocalize: Inferring Author Location in Social Media</emphasis> <emphasis>por Evan Sparks et al. University of California-Berkeley</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Inferring the Location of Twitter Messages Based on User Relationships</emphasis> <emphasis>por Clodoveu A. Davis Jr. et al. Universidade Federal de Minas Gerais</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Geolocation Prediction in Social Media Data by Finding Location Indicative Words</emphasis> <emphasis>por HAN Bo et al. University of Melbourne</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Home Location Identification of Twitter Users</emphasis> <emphasis>por Jalal Mahmud et al. IBM Research</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Geotagging One Hundred Million Twitter Accounts with Total Variation Minimization</emphasis> <emphasis>por Ryan Compton et al. HRL Laboratories (Malibu)</emphasis></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_algoritmos_y_aspectos_teóricos">
<title>Algoritmos y aspectos teóricos</title>
<simpara>Las siguientes secciones explican en profundidad los conceptos teóricos clave que se utilizan a lo largo del proyecto. Además, se hace un análisis detallado de los algoritmos aplicados para obtener términos discriminativos y su posterior comunicación con el software de aprendizaje automático.</simpara>
<section xml:id="_visión_general_del_sistema">
<title>Visión general del sistema</title>
<simpara>Antes de empezar a detallar los conceptos y algoritmos utilizados a lo largo de todo el proceso, esta sección pretende dar una visión general acerca de los pasos que se deberán realizar en el proyecto para la identificación de términos discriminativos y la posterior ejecución de un modelo de aprendizaje automático que permita inferir la geolocalización de una publicación en Twitter.</simpara>
<simpara>El primer paso, será la identificación de términos discriminativos. Para ello, se utilizará una implementación del método estadístico Log Likelihood Ratio, el cual nos devolverá la puntuación para cada término indicando si pertenecen al conjunto de datos local (es decir, que sea lo suficientemente discriminativo para poder ser etiquetado como perteneciente al área de interés). Esta fase devolverá un conjunto de términos que podrán ayudar a inferir la localización de un tuit en base a los términos que contiene. Para la identificación de términos, será necesario trabajar con dos conjuntos de tuits con características diferentes:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Un conjunto de tuits para un área global que utilice el mismo lenguaje que el área local que se quiere analizar.</simpara>
</listitem>
<listitem>
<simpara>Un conjunto de tuits para el área local sobre el que se quiere realizar el análisis.</simpara>
</listitem>
</orderedlist>
<figure>
<title>Visión simplificada del proceso de identificación de términos discriminativos</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/theoretical-aspects/overview/identify-terms.png" align="center"/>
</imageobject>
<textobject><phrase>identify terms</phrase></textobject>
</mediaobject>
</figure>
<simpara>Una vez se ha conseguido extraer el conjunto de términos discriminativos y su puntuación obtenida de aplicar el método Log Likehood Ratio, se deberá trabajar con un nuevo conjunto de tuits con coordenadas geográficas adjuntas que permita crear el modelo de entrenamiento para el software de aprendizaje automático. En <xref linkend="training-vw-process"/> se ilustra el proceso necesario para calcular la puntuación de cada tuit y desarrollar el modelo de entrenamiento en base a la división de los resultados en conjuntos de entrenamiento y test.</simpara>
<figure xml:id="training-vw-process">
<title>Flujo de trabajo para procesar los datos y entrenar al clasificador automático</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/theoretical-aspects/overview/training_vw_process.png" align="center"/>
</imageobject>
<textobject><phrase>training vw process</phrase></textobject>
</mediaobject>
</figure>
<simpara>Las siguientes secciones entrarán a explicar todos los puntos anteriores en un mayor grado de profundidad.</simpara>
</section>
<section xml:id="_log_likelihood_ratio_test">
<title>Log Likelihood-Ratio test</title>
<simpara>El test Log Likelihood-Ratio (<link xlink:href="http://www.itl.nist.gov/div898/handbook/apr/section2/apr233.htm">http://www.itl.nist.gov/div898/handbook/apr/section2/apr233.htm</link>) es un método estadístico ampliamente utilizado en problemas donde se pretenden comparar dos conjuntos de datos a través de una serie de supuestos.</simpara>
<simpara>En el caso del presente proyecto, la idea principal es aplicar LLR sobre los dos conjuntos de datos que contienen, por un lado, los tuits que se han recogido para el área local sobre el que se quieren comenzar a inferir tuits, y por el otro, un conjunto de tuits localizado en un área que se podría entender como global respecto al conjunto de datos local (por lo general, las áreas globales son localizaciones donde se habla el mismo idioma que en el área local, pero abarca otros territorios).</simpara>
<simpara>En este caso, partimos del <emphasis role="strong">supuesto</emphasis> de que el área local es un caso especializado del área global, el cual se puede diferenciar por los términos que contiene. Sobre este supuesto, el test Log Likelihood-Ratio nos devolverá una puntuación para cada término del área local donde se reflejará su valor discriminativo sobre el conjunto de términos globales. Cuanto más discriminativo sea el término, mayor será su valor LLR. Sin embargo, todos aquellos que no sirvan para diferenciar al conjunto de datos especializado, tendrán un valor negativo.</simpara>
<simpara>El valor LLR de cada término se utilizará para calcular la puntuación que indique si los nuevos tuits pertenecen o no al área local mediante un sumatorio de todos los valores para cada término que forma el tuit.</simpara>
<simpara>La explicación acerca de qué se considera <emphasis role="strong">término</emphasis> en el contexto del presente proyecto se explica en <xref linkend="_términos_discriminativos"/>, pero se podría definir resumidamente como todo aquel lexema que pueda contener información geolocalizable asociada.</simpara>
<section xml:id="_log_likelihood_ratio_test_normalizado">
<title>Log Likelihood-Ratio test normalizado</title>
<simpara>La implementación del algoritmo Log Likelihood-Ratio utilizado se basa en la interpretación propuesta en el artículo: Java, Akshay, et al. «Why we twitter: understanding microblogging usage and communities». Perteneciente a la novena WebKDD y primer taller SNA-KDD 2007 en minería de datos web y análisis de redes sociales.<footnote><simpara>Disponible en: <link xlink:href="http://aisl.umbc.edu/resources/369.pdf">http://aisl.umbc.edu/resources/369.pdf</link> (ver tablas y ecuaciones en la página 7)</simpara></footnote></simpara>
<simpara>Tras los primeros experimentos, aunque se vislumbraron resultados esperanzadores, se observó también como algunos términos que se sabía eran discriminativos, no obtenían una puntuación LLR lo suficientemente alta como para que el sistema los pudiese considerar discriminativos en el futuro. Por tanto, se realizó una nueva implementación <emphasis>normalizada</emphasis> sobre el algoritmo anterior, basada en la siguiente sospecha:</simpara>
<blockquote>
<attribution>
Comunicación personal del director del proyecto.
</attribution>
<simpara>Términos como «españa», los cuales son muy representativos de tuits españoles, también son relativamente comunes en tuits chilenos, y por tanto no se puede interpretar que vayan a tener la misma importancia para discrimar entre ambos, por mucho que su uso sea mayor en España. Sin embargo, términos como «culín sidra», los cuales no tienen una gran representatividad en tuits españoles y es casi seguro que no tengan ninguna aparición en tuits procedentes de Chile, tienen un gran componente geográfico implícito que hace pensar que tuits que contenga ese término es <emphasis role="strong">muy posible</emphasis> que provengan de España.</simpara>
</blockquote>
<simpara>Para tratar de demostrar la intuición anterior, se desarrolló una normalización basada en tres pasos:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>La puntuación LLR de cada término, es normalizada contra el mínimo (negativo) y máximo (positivo) valor teórico que LLR puede alcanzar dados dos datasets.</simpara>
</listitem>
<listitem>
<simpara>La frecuencia del término en el dataset local y global, es normalizada frente al total de frecuencias en ambos datasets.</simpara>
</listitem>
<listitem>
<simpara>Por último, el valor LLR normalizado es dividido por el valor correspondiente de la frecuencia local o global dependiendo de si el valor obtenido es positivo o negativo (es decir, si es más posible que pertenezca al dataset A o al dataset B).</simpara>
</listitem>
</orderedlist>
<simpara><?asciidoc-pagebreak?></simpara>
<formalpara>
<title>Algoritmo para normalizar el valor LLR de un término</title>
<para>
<programlisting language="java" linenumbering="unnumbered">  /*
   * @param a  frequency of token of interest in dataset A
   * @param b  frequency of token of interest in dataset B
   * @param c  total number of observations in dataset A
   * @param d  total number of observations in dataset B
   */
  public double normalizedLLR (long a, long b, long c, long d) {
    double min, max, a_norm, b_norm, llr;
    min    = getLLR(0, d, c, d);
    max    = getLLR(c, 0, c, d);
    a_norm = a / (double)c;
    b_norm = b / (double)d;
    llr    = getLLR(a, b, c, d);

    return (llr&gt;0) ? (llr/(max*a_norm)) : (-llr/(min*b_norm));
  }</programlisting>
</para>
</formalpara>
<simpara>De esta manera, el valor LLR normalizado se puede usar como <emphasis>proxy</emphasis> para el nivel de confianza que tendremos a la hora de asignar cada término a uno u otro dataset. Por ejemplo, en el supuesto anterior, «españa» obtendría un peso relativamente bajo, mientras que si aparece «culín sidra» su peso sería muy alto. En el caso contrario, donde se querría identificar cuando un término no es lo suficientemente discriminativo para el área local, se aplicaría la misma lógica, y términos como «chile» tendría un peso negativo moderado, mientras que «achunchar» obtendría un peso mucho más negativo para indicar que el vocablo es más discriminativo para el área global.</simpara>
<simpara>A continuación, se muestran los 10 resultados más discriminativos tras aplicar LLR y LLR normalizado sobre keywords que sirvan para diferenciar tuits de Asturias del resto de España:</simpara>
<formalpara>
<title>LLR sin normalizar</title>
<para>
<screen>5.673174267765713    avilés
5.786668335161142    muches
6.55625607825904     gracies
6.9163915469140225   gijon
7.082198151712459    olmo
7.650261596901131    ye
7.8221070414197245   besin
8.673885864488826    gijón
11.355768634976933   asturias
13.541645339239054   oviedo</screen>
</para>
</formalpara>
<simpara><?asciidoc-pagebreak?></simpara>
<formalpara>
<title>LLR normalizado</title>
<para>
<screen>34.03033586645537    economia
34.83206427341024    escalera
35.896146032001866   moreda
40.13311132890776    héctor
40.13311132890776    celebralos
40.13311132890776    sidra
41.63987088300992    avilés
41.65857127377122    simón
45.94585510324283    presta</screen>
</para>
</formalpara>
<simpara>Se puede observar como en el caso del LLR normalizado, se penalizan aquellos términos que podrían aparecer frecuentemente en otros tuits del territorio español: <emphasis>oviedo</emphasis>, <emphasis>gijón</emphasis> y <emphasis>asturias</emphasis>; mientras que se favorecen otros más específicos como <emphasis>sidra</emphasis>, <emphasis>presta</emphasis> o <emphasis>celebralos</emphasis> (escrito como se pronunciaría en asturiano).</simpara>
<simpara>Como también es evidente, existe cierto ruido a la hora de identificar términos como muy discriminativos. En el caso anterior, a las palabras <emphasis>escalera</emphasis> o <emphasis>economia</emphasis> se les ha otorgado una puntuación LLR bastante elevada pero que puede ser tolerable, por el momento, al tratarse de un proceso totalmente automático que no necesita de ningún conocimiento experto.</simpara>
</section>
</section>
<section xml:id="_términos_discriminativos">
<title>Términos discriminativos</title>
<simpara>Se consideran términos discriminativos aquellos que son capaces de aportar información muy geolocalizable de manera implícita. Un ejemplo son aquellas palabras muy propias de una localización en concreto, como el caso del término <emphasis>sidra</emphasis> o <emphasis>carbayu</emphasis>, que con mucha probabilidad indican un contenido que ha sido generado en Asturias.</simpara>
<simpara>La estrategia planteada en este proyecto está basada en descubrir este tipo de términos a través de los tuits que los usuarios publican para una determinada región (país, estado, ciudad, etc.). Para ello, la premisa básica es aceptar que los términos más discriminativos tenderán a tener un epicentro muy significativo donde su frecuencia es muy elevada, para después, no tener apenas dispersión y ser muy poco frecuentes en el resto.</simpara>
<simpara>Con el objetivo de poder aplicar los algoritmos anteriores, será necesario trabajar siempre con dos datasets sobre los que establecer la comparativa. Por un lado, se trabajará con un conjunto de datos localizados en el área en concreto que se quiera analizar, y por otro lado, otro conjunto de datos que se establezcan en un área más global para ese mismo idioma.</simpara>
<simpara>A partir de ahí, el análisis de cada dataset extraerá los siguientes términos:</simpara>
<variablelist>
<varlistentry>
<term>Menciones</term>
<listitem>
<simpara>Se considerán menciones todos aquellos términos que comienzan con el literal <literal>@</literal>. En Twitter, se utilizan para hacer referencia a otro usuario en el contenido que se está publicando.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Hashtags</term>
<listitem>
<simpara>Un hashtag es un término que comienza con el literal <literal>#</literal> y sirve para categorizar el contenido de un tuit. Un ejemplo claro es durante los partidos de fútbol del Fútbol Club Barcelona, donde los aficionados que se encuentran comentando el partido en Twitter, suelen acompañar cada publicación con el hashtag <literal>#fcblive</literal> de manera que clasifican manualmente el contenido de su tuit para poder ser agrupado en una misma conversación.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Bigramas</term>
<listitem>
<simpara>En este proyecto, hemos considerado como bigramas todas aquellas combinaciones de 2 palabras que se puedan formar con el contenido de un tuit. Al contrario que en algunos artículos de investigación anteriores donde sólo se consideran términos consecutivos, en este caso hemos realizado todas las combinaciones posibles para cada tuit.</simpara>
<simpara>Algunas consideraciones importantes sobre esto son:</simpara>
<itemizedlist>
<listitem>
<simpara>Se han eliminado todos aquellos bigramas que contienen 2 veces la misma palabra.</simpara>
</listitem>
<listitem>
<simpara>Se han eliminado todos aquellos bigramas que contienen al menos una palabra vacía.</simpara>
</listitem>
<listitem>
<simpara>Se han eliminado todos aquellos bigramas con términos inferiores a 2 caracteres.</simpara>
</listitem>
<listitem>
<simpara>Se han ordenado alfabéticamente todos los bigramas de acuerdo a las 2 palabras que contienen, facilitando así el control de bigramas repetidos.</simpara>
<simpara>Un ejemplo de los bigramas que obtendríamos tras analizar un tuit en base a las condiciones anteriores sería:</simpara>
<blockquote>
<simpara>Buenos días vamos a trabajar todo el día</simpara>
</blockquote>
<simpara>Que generaría las siguientes combinaciones</simpara>
<screen>(buenos días), (buenos vamos), (buenos trabajar),
(buenos todo), (buenos día), (días vamos),
(días trabajar), (días todo), (día días),
(trabajar vamos), (todo vamos), (día vamos),
(todo trabajar), (día trabajar), (día todo)</screen>
<simpara>Como se puede observar, la generación de bigramas para cada tuit provoca una explosión de términos que fue necesario controlar (explicado en <xref linkend="_utilización_de_algoritmos_de_streaming"/>) para evitar sobrepasar la memoria del sistema.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Keywords</term>
<listitem>
<simpara>Las keywords son unigramas formados, obviamente, por un único término, cuyo resultado se asemeja a realizar una tokenización sobre el tuit pero aplicando reglas que también se utilizaban en la extracción de bigramas (palabra vacía, longitud inferior a 2 caracteres, etc.).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Keywords en el campo de Localización</term>
<listitem>
<simpara>Son el resultado de aplicar la extracción anterior sobre el campo de Localización del perfil del usuario.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>A continuación se muestran dos ejemplos de tuits que reflejan como existen ciertos términos que pueden poseer información geográfica implícita a pesar de no estar relacionados a ningunas coordenadas concretas. Los tuits seleccionados pertenecen a un conjunto recogido entre los días 13 y 14 de mayo de 2014 para la ciudad de Madrid.</simpara>
<simpara>El primer tuit contiene mucha información potencialmente geolocalizable. Nombra varias veces la ciudad de Madrid y, además, hace uso del término <emphasis role="strong">champions</emphasis> el cual fue, en conjunción con el término <emphasis role="strong">madrid</emphasis>, uno de los biagramas que localizaban con más fuerza los tuits procedentes de la capital española.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="66*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Campo de Localización</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Madrid</emphasis></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Contenido del tuit</simpara></entry>
<entry align="left" valign="top"><simpara>Primera vez que dos equipos <emphasis role="strong">Madrileños</emphasis>, se erigen como finalistas de la  <emphasis role="strong">Champions</emphasis>!! Gane quién gane, <emphasis role="strong">Madrid</emphasis> gana!! La Copa se queda aquí.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>El siguiente tuit, se podría considerar como un claro ejemplo de tuit difícilmente localizable (en base a su contenido). Su campo de localización no hace ninguna referencia a un lugar geográfico y el contenido del tuit tampoco contiene ninguna entidad geográfica:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="66*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Campo de Localización</simpara></entry>
<entry align="left" valign="top"><simpara>Skateboarding saved my life.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Contenido del tuit</simpara></entry>
<entry align="left" valign="top"><simpara>Que <emphasis role="strong">Ana Botella</emphasis> cobre 100.000€ al año en bruto y no sepa hablar inglés no tiene derecho.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>Sin embargo, la presencia del bigrama <emphasis role="strong">ana botella</emphasis> puede aportar alguna pista para identificar que el tuit procede de Madrid. De hecho, los experimentos realizados sobre este mismo conjunto de datos, detectaron que el bigrama <emphasis role="strong">ana botella</emphasis> efectivamente era bastante significativo para identificar tuits procedentes de Madrid sobre aquellos procedentes del resto de España.<footnote><simpara><emphasis role="strong">ana botella</emphasis> obtuvo una puntuación LLR de 134.13897 sobre un máximo de 175.30144</simpara></footnote></simpara>
<section xml:id="_sistema_de_filtros">
<title>Sistema de filtros</title>
<simpara>Con el objetivo de poder realizar las extracciones de los términos anteriores de forma flexible, se diseñó un pequeño sistema de filtros que ayudara a combinar varios filtros en una misma ejecución. La implementación de este sistema está basada en el patrón de diseño Decorator<footnote><simpara><link xlink:href="http://perldesignpatterns.com/?word=decorator+pattern">http://perldesignpatterns.com/?word=decorator+pattern</link></simpara></footnote>, aunque con la diferencia de que en este caso, la extracción de cada filtro se realiza sobre el tuit original y no sobre el resultado de las extracciones de filtros anteriores (una «decoración» incremental no tendría sentido dado el dominio del problema).</simpara>
<figure>
<title>Representación del patrón Decorator que ilustra el sistema de filtros</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/extractor-filter.png" align="center"/>
</imageobject>
<textobject><phrase>Representación del patrón Decorator que ilustra el sistema de filtros</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="_utilización_de_algoritmos_de_streaming">
<title>Utilización de algoritmos de Streaming</title>
<simpara>Como se ha visto en secciones anteriores, el proceso de extracción de términos genera una gran cantidad de datos que será necesario gestionar en memoria. Para solucionar este problema, se hizo uso de técnicas propias de los algoritmos de Streaming, los cuales tienen varios puntos en común con el problema actual.</simpara>
<note>
<title>Algoritmos de Streaming</title>
<simpara>Se conocen como algoritmos de Streaming aquellos problemas donde la capacidad de memoria o procesamiento es menor a la cantidad de datos que se reciben como entrada. Estos datos, se procesan de uno en uno y una única vez, manteniendo un orden secuencial e incremental que implica que sea necesario conocer el dato anterior para poder procesar correctamente el dato actual.</simpara>
</note>
<simpara>La solución, por tanto, pasa por controlar el número de elementos que se gestionan en cada momento en memoria por el sistema, y plantear una estrategia que sea capaz de liberar memoria sin el riesgo de perder información que pueda adulterar los resultados. Los pasos seguidos en este proyecto para lidiar con el problema fueron los siguientes:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Establecer un número máximo de <emphasis>keys</emphasis> que podrán ser gestionadas por el hash. Este valor deberá ser configurado por el desarrollador en función de las características del hardware sobre el que se ejecute el sistema. En las pruebas realizadas en este proyecto, el número máximo de elementos se situó en 500.000.</simpara>
<simpara>Esto también implica que el sistema de generación de puntuaciones LLR podrá trabajar únicamente sobre los <literal>n</literal> términos que se seleccionen aquí.</simpara>
</listitem>
<listitem>
<simpara>Una vez determinado el umbral máximo de elementos, será necesario definir que porcentaje de términos se eliminarán una vez alcanzado el límite anterior. En este caso, se ha optado por seguir una estrategia de poda agresiva en la que se eliminan de manera constante un porcentaje de los elementos con menos frecuencia del hash.</simpara>
<simpara>Esta estrategia implica que siempre que se produzca una situación de poda, se deba ordenar el hash de acuerdo a la frecuencia de sus elementos. De manera experimental, se ha comprobado como la eliminación constante de un 40% de los elementos con menor frecuencia, a pesar de parecer demasiado agresiva, da resultados muy positivos sin existir riesgo de eliminar términos con una frecuencia muy elevada (por supuesto, todo esto dentro del dominio del problema actual).</simpara>
</listitem>
<listitem>
<simpara>En el momento de realizar la poda, se debe guardar qué frecuencia es la mayor del grupo de elementos a eliminar. De esta manera, se consigue que términos que vuelvan a aparecer tras la poda, partan de su frecuencia original en vez de volver a empezar de 0. Esto provoca también que muchos términos nuevos, empiecen con una frecuencia más elevada de lo esperado. Sin embargo, la frecuencia mínima que se utilizará después para seleccionar sobre qué términos se aplica el LLR, será lo suficientemente elevada como para evitar situaciones donde este problema pueda adulterar los resultados.</simpara>
</listitem>
</orderedlist>
<simpara>En <xref linkend="_pseudocódigo_para_ilustrar_el_proceso_completo_de_análisis_de_tuits"/>, se muestra un esbozo de la implementación del algoritmo anterior. El control de memoria y proceso de poda agresiva se ilustra a través de los métodos <literal>check_memory_status</literal> y <literal>reduce_map_load</literal>.</simpara>
</section>
</section>
<section xml:id="_pseudocódigo_para_ilustrar_el_proceso_completo_de_análisis_de_tuits">
<title>Pseudocódigo para ilustrar el proceso completo de análisis de tuits</title>
<simpara>Los siguientes fragmentos de <emphasis>pseudocódigo</emphasis> muestran los diferentes algoritmos que se han utilizado para obtener la frecuencia de términos en los diferentes datasets, así como el proceso para realizar el cálculo de su Log Likelihood-Ratio asociado y la manera de computar la puntuación total de cada tuit en función de la puntuación de cada uno de los términos que contiene.</simpara>
<simpara><?asciidoc-pagebreak?></simpara>
<formalpara>
<title>Algoritmo para extraer la frecuencia de cada término</title>
<para>
<programlisting language="ruby" linenumbering="unnumbered">for each tweet in tweets do
  terms = apply_extractor_filter(tweet)
  check_memory_status()
  for each term in terms do
    if frequencies[term].is_defined then
      frequencies[term] += 1
    else
      frequencies[term] = minimum_frequency
    end
  end
end

def check_memory_status
  if frequencies.size &gt;= MAXIMUM_EXTRACTED_TERMS then
    reduce_map_load()
  end
end

def reduce_map_load
  items_to_remove   = frequencies.size * FACTOR_TO_REMOVE
  # Ordenamos el mapa de menor a mayor en función de la frecuencia
  ordered_map       = frequencies.order_by_frequency
  minimum_frequency = ordered_map.get(items_to_remove - 1)

  ordered_map.slice from items_to_remove - 1 to ordered_map.size
end</programlisting>
</para>
</formalpara>
<simpara><?asciidoc-pagebreak?></simpara>
<formalpara>
<title>Algoritmo para calcular la puntuación LLR de cada término</title>
<para>
<programlisting language="ruby" linenumbering="unnumbered">for each term in locals do
  freq = locals[term]
  if freq &gt; min_frequency then
    global_freq = get_global_freq(term)
    if global_freq &gt; 0 then
      k11 = freq
      k12 = global_freq
      k21 = total_local_frequencies
      k22 = total_global_frequencies
      llr = Dunning.normalized_llr(k11, k12, k21, k22)
      results[term] =&gt; llr
    end
  end
end

def get_global_freq(term)
  globals[term].is_defined ? globals[term] : avg_global_freq(term)
end

def avg_global_freq(term)
  same_freq_in_local = locals.select(
    t =&gt; globals[t].is_defined &amp;&amp; locals[t] == locals[term]
  )
  acc = same_freq_in_local.map(t =&gt; globals[t]).reduce(
    (previous, current) =&gt; previous + current
  )
  return acc / same_freq_in_local.size
end</programlisting>
</para>
</formalpara>
<formalpara>
<title>Algoritmo para calcular la puntuación LLR de cada tuit</title>
<para>
<programlisting language="ruby" linenumbering="unnumbered">for each tweet in tweets do
  terms = extract_terms(tweet)

  score = 0
  for each term in terms do
    score += llr_by_term[term]
  end

  results[tweet] = score
end</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_procesando_xml_con_emphasis_pull_parsing_emphasis">
<title>Procesando XML con <emphasis>pull parsing</emphasis></title>
<simpara>Debido al intenso uso que se hace en el proyecto de datos en XML (ver <xref linkend="_almacenamiento_de_datos"/>), fue necesario desarrollar un sistema de análisis basado en estrategias que evitasen tener que cargar el contenido completo del XML en memoria.</simpara>
<simpara>Dos de las estrategias más conocidas en contraposición a DOM para analizar grandes colecciones de XML son: <emphasis role="strong">Simple API XML (o SAX)<footnote><simpara><link xlink:href="http://www.saxproject.org/">http://www.saxproject.org/</link></simpara></footnote></emphasis> y <emphasis role="strong">pulling parsing<footnote><simpara><link xlink:href="http://www.xmlpull.org/">http://www.xmlpull.org/</link></simpara></footnote></emphasis>.</simpara>
<simpara>En este proyecto, se decidió utilizar la segunda debido a su soporte nativo por parte de las APIs de Scala (en <xref linkend="_scala"/> se realiza una breve descripción acerca de este lenguaje de programación así como de las razones que influyeron en su utilización en el desarrollo del presente proyecto) para trabajar con XML a través del paquete <literal>xml.pull</literal>. Pulling parser se caracteriza por analizar el fichero XML mediante la utilización de eventos. A medida que se van leyendo líneas del documento, van saltando una serie de eventos de acuerdo a la especificación: <emphasis>comienzo de elemento, fin de elemento, comienzo de nodo de texto, fin de nodo de texto, etc</emphasis>. Cada uno de estos eventos, puede tener asignado un <emphasis>listener</emphasis> que contenga la lógica a ejecutar.</simpara>
<simpara>El fragmento de código posterior muestra un ejemplo de uso para analizar un fichero XML mediante <emphasis>pulling parser</emphasis>. En primer lugar se define el fichero sobre el que realizar el análisis y se crea una nueva variable que nos ayudar a controlar cuando estamos dentro del elemento que queremos analizar. Utilizando <emphasis>pattern matching</emphasis> nos aseguramos de seleccionar aquellos elementos cuya etiqueta coincida con la del campo sobre el que debe actuar el filtro que deseamos aplicar. En caso de encontrar un elemento de apertura para ese nodo, indicamos en la variable booleana <literal>in</literal> que nos encontramos en el elemento deseado y los eventos encargados de detectar nodos de texto la consultarán para saber si sobre ese texto deben aplicar o no el filtro.</simpara>
<simpara>El evento para detectar que hemos salido del nodo deseado se encarga de volver a <emphasis>setear</emphasis> el valor de la variable <literal>in</literal> a <literal>false</literal>.</simpara>
<formalpara>
<title>Ejemplo de código para analizar un documento XML utilizando <emphasis>pulling parser</emphasis></title>
<para>
<programlisting language="java" linenumbering="unnumbered">val reader = new XMLEventReader(Source.fromFile(xmlFile))
var in = false
reader.foreach({
  case e: EvElemStart if e.label == _filter.field =&gt; in = true
  case EvText(text) if in =&gt; applyFilter(text)
  case e: EvElemEnd if e.label == _filter.field =&gt; in = false
  case _ =&gt; ;
})</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_entrenamiento_del_clasificador">
<title>Entrenamiento del clasificador</title>
<simpara>Después de todos los algoritmos y procesos anteriores, el último paso es entrenar al software de aprendizaje automático (ver <xref linkend="_software_de_aprendizaje_automático_y_vowpal_wabbit"/>) para proporcionarle la información necesaria para que pueda inferir la localización de un tuit de manera automática.</simpara>
<simpara>Partiendo del punto en el que ya se tienen una serie de ficheros TSV con la puntuación LLR de cada término y que han servido para generar un último fichero TSV con la puntuación LLR asociada para cada tuit (mediante el sumatorio del conjunto de términos que lo forman) de una nueva colección de 24 horas, será necesario traducir esa información a un input válido para Vowpal Wabbit. Para ello, se utiliza el sistema explicado en <xref linkend="_normalización_de_datos"/>.</simpara>
<simpara>Una vez hecho esto, y como se explica también en <xref linkend="_división_de_datos_en_conjuntos_de_entrenamiento_y_test"/> será necesario dividir el fichero de entrada en un 80% de usuarios para entrenar al clasificador, y un 20% para realizar tests. Serán este 20% restantes los que se usarán para medir la precisión y exhaustividad de los modelos que se creen para realizar los experimentos a diferentes granularidades.</simpara>
<simpara>Es importante señalar que los ficheros de entrada que se utilizan para entrenar a Vowpal Wabbit han sido refinados para eliminar aquellos tuits cuya puntuación era negativa aunque se sabía (ya que contiene información geográfica adjunta por tratarse de tuits utilizados en procesos de entrenamiento) que su localización pertenecía al área local y viceversa. De este modo, se garantiza que el clasificador se entrene siempre en base a resultados correctos.</simpara>
</section>
</chapter>
<chapter xml:id="_experimentos_y_resultados">
<title>Experimentos y resultados</title>
<simpara>Para comprobar la validez de los algoritmos anteriores se plantearon una serie de experimentos que trabajasen sobre conjuntos de tuits escritos tanto en inglés como en español, con el objetivo de poder obtener resultados positivos a nivel de país, provincia, área metropolitana y barrio.</simpara>
<simpara>La selección de los idiomas inglés y español como base para los experimentos se realizó a tenor de ser los dos lenguajes, a priori, más complejos de analizar debido a que son hablados en muchas partes del mundo. Por ejemplo, haber escogido como idioma el italiano, haría que las posibilidades de geolocalización se viesen reducidas prácticamente a Italia; ídem en el caso de haber utilizado alemán, que tan sólo es hablado en Alemania, Austria, Suiza y parte de Luxemburgo.</simpara>
<section xml:id="_primeros_experimentos_trabajando_con_grandes_volúmenes_de_datos">
<title>Primeros experimentos: trabajando con grandes volúmenes de datos</title>
<simpara>Los primeros experimentos se basaron en establecer diferencias a nivel de país. Para ello, se realizó un proceso de recolección de tuits durante una semana para los siguientes casos:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Tuits de España escritos en español</simpara>
</listitem>
<listitem>
<simpara>Tuits del mundo escritos en español</simpara>
</listitem>
<listitem>
<simpara>Tuits del Reino Unido escritos en inglés</simpara>
</listitem>
<listitem>
<simpara>Tuits del mundo escritos en inglés</simpara>
</listitem>
</orderedlist>
<simpara>En total esta fase duró 4 semanas, recolectando en torno a <literal>2 x 8GB</literal> de datos para los tuits globales, así como <literal>2 x 1GB</literal> para los tuits localizados en España y el Reino Unido.</simpara>
<simpara>Estos primeros conjuntos de datos permitieron que se desarrollasen los algoritmos de puntuación LLR y se empezase a comprobar empíricamente qué resultados aportaban los términos utilizados (menciones, hashtags, bigramas) así como investigar que otros campos o términos del perfil del usuario podrían ayudar a encontrar pistas acerca de su geolocalización. Durante esta fase se añadió el uso de <emphasis>keywords</emphasis> tanto a nivel de contenido de un tuit como sobre el campo de localización del usuario, sospechando que podrían aportar más información útil en base a los resultados LLR que se estaban observando a nivel de bigramas.</simpara>
<simpara>Sobre estos conjuntos de datos no llegaron a establecerse pruebas que involucrasen software de aprendizaje automático, puesto que el uso de grandes volúmenes de datos quedó descartado al comprobar empíricamente que los términos que implican contenido geolocalizable no tienen que ser uniformes en el tiempo para una determinada región. Esta apreciación se realizó al trabajar con nuevos conjuntos de datos más pequeños, donde aparecieron como términos muy significativos aquellos que tenían que ver con un evento temporal muy concreto, el fallecimiento del seleccionador de fútbol nacional, Luis Aragonés.</simpara>
<formalpara>
<title>Puntuación LLR para algunos términos recogidos en conjuntos de datos entre el 1 y 2 de febrero de 2014</title>
<para>
<screen>48.958529065415576  aragonés gran
49.17093149031471   aragonés selección
50.049053227619474  aragonés descanse
50.049053227619474  grandes luis
50.739417608310795  luis roja
50.97596629207029   luis nunca
50.97596629207029   luis mas
51.215854517258855  aragones hoy
51.215854517258855  aragonés historia
51.45916160893309   fútbol hombre
51.95636312422304   aragonés hombre
52.21042998468123   siempre aragones
52.46826083659376   luis mejor
52.46826083659376   dep sabio
56.28098057747021   descanse paz
81.59614077426396   fútbol grande
102.31617863438613  entrenador gran
107.56036254304512  bernabéu santiago
107.5697695101089   noticia triste
107.87375147152346  hoy madrid
111.68234684538564  hizo hombre
111.93538868096397  dep triste
115.39967536884996  fútbol va
116.50155357908382  futbol hoy
119.45187311362997  día fútbol</screen>
</para>
</formalpara>
<simpara>Como se puede observar, términos vinculados al fallecimiento del seleccionador aportaban una gran información geográfica que hacía que todos los tuits que los contenían prácticamente fuesen vinculados a España (lo cual en este caso sería correcto). Pero, ¿qué sucede si ocurre sobre un evento que se pueda aplicar sobre cualquier otro lugar del mundo en cualquier momento?</simpara>
<simpara>Por ejemplo, es habitual que durante los meses de verano en el hemisferio sur, términos relativos a la estación estival sean más habituales en países como Argentina, Chile, etc. que en España. Lo cual sería justamente al revés en épocas de verano en el hemisferio norte. Por tanto, es importante tener en cuenta que los términos y el vocabulario tienen una importancia temporal, y el hecho de obtener grandes volúmenes de datos no tiene porque beneficiar el entrenamiento de un modelo de aprendizaje automático para inferir la localización de un tuit.</simpara>
<simpara>Por ello, se optó por trabajar siempre con conjuntos de datos recogidos en periodos de 24 horas, para evaluarlos contra colecciones de tuits que se recogerían en las 24 horas siguientes. De esta manera se puede aprovechar la repercusión de ciertos términos ante cualquier evento.</simpara>
</section>
<section xml:id="_trabajando_con_diferentes_niveles_de_granularidad">
<title>Trabajando con diferentes niveles de granularidad</title>
<section xml:id="_conceptos_previos">
<title>Conceptos previos</title>
<section xml:id="_precisión_y_exhaustividad">
<title>Precisión y exhaustividad</title>
<simpara>Para entender los siguientes resultados, será necesario explicar dos conceptos clave que se repetirán a lo largo de las diferentes tablas resumen. El concepto de <emphasis role="strong">precisión</emphasis> se puede entender como el número de predicciones que coinciden con los resultados reales sobre el total de las predicciones realizadas.</simpara>
<simpara>Por tanto, si tenemos un total de 20 predicciones y 18 de las mismas han coincidido con su valor real, se consideraría que el porcentaje de precisión sería:</simpara>
<screen>(18/20) * 100 = 90% de precisión</screen>
<simpara>El concepto <emphasis role="strong">exhaustividad</emphasis> se explica como el porcentaje de tuits locales que se han identificado sobre el conjunto de tuits locales reales. En este caso, partiendo de un ejemplo en el que se hubiesen etiquetado 25 tuits como locales de manera correcta y sabiendo que en el conjunto a evaluar existían 30 tuits locales, el porcentaje de exhaustividad de nuestro modelo sería de:</simpara>
<screen>(25/30) * 100 = 83.33%</screen>
<simpara>Entendiendo este porcentaje como que para cada 100 tuits que sabemos son locales, nuestro modelo es capaz de detectar ~ 83 satisfactoriamente.</simpara>
</section>
<section xml:id="_umbral_de_precisión">
<title>Umbral de precisión</title>
<simpara>Una característica que permite manejar el grado de exhaustividad de cada modelo es el valor que se determine como <emphasis role="strong">umbral de precisión</emphasis>.</simpara>
<simpara>Por defecto, Vowpal Wabbit devuelve un valor entre 0 y 1 con la probabilidad de que ese ejemplo pertenezca al conjunto de tuits locales o no (entendiendo 1 como que <emphasis role="strong">sí</emphasis> pertenece al conjunto local y 0 como que <emphasis role="strong">no</emphasis>).  Por tanto, es habitual que muchos valores que son realmente 1, no aparezca en los resultados de la predicción de manera tan absoluta, sino que se muestren con valores cada vez más cercanos a 1 en función del grado de confianza del clasificador en su predicción: <emphasis>0.5, 0.6, 0.7, etc.</emphasis></simpara>
<simpara>Ese valor que determinemos como umbral permitirá que más o menos valores sean etiquetados por el clasificador como locales y, por tanto, que más o menos tuits locales puedan ser potencialmente etiquetados. Sin embargo, cuanto más bajo pongamos el umbral de precisión, más baja será la precisión y por tanto se encontrará más ruido en los resultados finales.</simpara>
<simpara>Para los siguientes ejemplos se ha tomado como umbral el valor <emphasis role="strong">0.5</emphasis>, el cual demuestra tener, de manera empírica, un buen comportamiento en la relación precisión - exhaustividad.</simpara>
</section>
</section>
<section xml:id="_resultados_a_nivel_de_país">
<title>Resultados a nivel de país</title>
<simpara>De acuerdo con los objetivos establecidos al comienzo del proyecto, se realizaron dos experimentos a nivel de país para poder trabajar tanto con tuits escritos en español como con aquellos escritos en inglés.</simpara>
<simpara>Por un lado, se realizó un experimento que pudiese discriminar tuits procedentes de España en un conjunto de tuits globales escritos en español. Por el otro, se realizó un segundo experimento enfocado en detectar qué tuits escritos en inglés desde las islas británicas procedían de la República Irlandesa.</simpara>
<simpara>Los resultados para ambos experimentos se pueden observar en las tablas <xref linkend="experiment-spain-global"/> y <xref linkend="experiment-uk-ireland"/>.</simpara>
<table xml:id="experiment-spain-global" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes España</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>93.16%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>89.48%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>96.13%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>90.35%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-uk-ireland" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de la República Irlandesa</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>98.17%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>71.17%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>97.56%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>98.20%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_resultados_a_nivel_de_provincia">
<title>Resultados a nivel de provincia</title>
<simpara>Para realizar los experimentos a nivel de provincia se seleccionaron las siguientes tres regiones del estado español: <emphasis role="strong">Principado de Asturias</emphasis>, <emphasis role="strong">Madrid y</emphasis> <emphasis role="strong">Barcelona</emphasis>.</simpara>
<simpara>En este caso, se consideró como área local cada una de las tres regiones anteriores y como área global, el total del territorio español a excepción de la provincia evaluada. Así pues, para realizar el experimento sobre el Principado de Asturias se consideró:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Área local</emphasis>: Principado de Asturias</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Área global</emphasis>: Todo el territorio español a excepción del Principado de Asturias.</simpara>
</listitem>
</itemizedlist>
<simpara>Siguiendo el mismo mecanismo en el resto de provincias.</simpara>
<simpara>Las tablas <xref linkend="experiment-spain-asturias"/>, <xref linkend="experiment-spain-madrid"/> y <xref linkend="experiment-spain-barcelona"/> muestran los resultados de la ejecución para esta granularidad.</simpara>
<table xml:id="experiment-spain-asturias" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes del Pricipado de Asturias</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>98.89%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>53.53%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>89.83%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>99.00%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-spain-madrid" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Madrid</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>92.27%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>72.70%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>86.78%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>93.39%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-spain-barcelona" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Barcelona</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>98.25%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>65.73%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>97.09%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>98.29%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="_barcelona_y_el_impacto_del_catalán">
<title>Barcelona y el impacto del catalán</title>
<simpara>Uno de los aspectos más curiosos de los resultados y puntuaciones generadas por el algoritmo Log Likelihood-Ratio fue comprobar como entre los términos más discriminativos para la provincia de Barcelona se encontraban, en su mayoría, vocablos en catalán.</simpara>
<simpara>Esto certifica de manera empírica las intuiciones acerca de la validez del algoritmo para este tipo de problema, siendo capaz de detectar que los términos en catalán, con una frecuencia muy baja a nivel global y relativamente alta en Barcelona, son los más significativos para identificar al territorio.</simpara>
<simpara><?asciidoc-pagebreak?></simpara>
<formalpara>
<title>Bigramas más discriminativos para encontrar tuits procedentes de la provincia de Barcelona</title>
<para>
<screen>94.52274667047315   per tres
100.19650868429694  per dos
100.19650868429694  salón barcelona
100.19650868429694  ara per
100.19650868429694  acuerdo leo
100.19650868429694  gracias horas
100.19650868429694  debat els
101.79780787750204  buenos barcelona
111.11339666895931  catalunya és
111.11339666895931  avui amb</screen>
</para>
</formalpara>
</section>
</section>
<section xml:id="_resultados_a_nivel_de_área_metropolitana">
<title>Resultados a nivel de área metropolitana</title>
<simpara>Para la granularidad a nivel de área metropolitana, se utilizaron las tres metrópolis más importantes del Reino Unido: <emphasis role="strong">Londres</emphasis>, <emphasis role="strong">Manchester</emphasis> y <emphasis role="strong">Birminghan</emphasis>. El proceso para generar los conjuntos de datos utilizados para el análisis se basa en los mecanismos utilizados a nivel de provincia.</simpara>
<simpara>Las tablas <xref linkend="experiment-uk-london"/>, <xref linkend="experiment-uk-manchester"/> y <xref linkend="experiment-uk-birminghan"/> muestran los resultados para esta granularidad.</simpara>
<table xml:id="experiment-uk-london" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Londres</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>91.69%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>70.98%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>86.43%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>92.77%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-uk-manchester" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Manchester</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>96.68%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>51.10%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>88.53%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>96.97%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-uk-birminghan" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Birminghan</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>97.22%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>47.05%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>97.95%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>97.20%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_resultados_a_nivel_de_barrio">
<title>Resultados a nivel de barrio</title>
<simpara>Por último, se realizó un experimento a nivel de barrio, siendo la granularidad más baja analizada en el proyecto. Para ello, se utilizaron los 3 barrios más importantes de Londres como área local (<emphasis role="strong">Wandsworth</emphasis>, <emphasis role="strong">Lambeth</emphasis> y <emphasis role="strong">Southwark</emphasis>), así como la ciudad de Londres como área global. Los resultados de este experimento se pueden consultar en las tablas <xref linkend="experiment-london-wandsworth"/>, <xref linkend="experiment-london-lambeth"/> y <xref linkend="experiment-london-southwark"/>.</simpara>
<table xml:id="experiment-london-wandsworth" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Wandsworth</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>99.54%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>18.18%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>100.0%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>99.54%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-london-lambeth" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Lambeth</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>99.20%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>24.13%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>87.5%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>99.23%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="experiment-london-southwark" frame="all" rowsep="1" colsep="1">
<title>Resultados de la ejecución para detectar tuits procedentes de Southwark</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="75*"/>
<colspec colname="col_2" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Precisión</simpara></entry>
<entry align="left" valign="top"><simpara>98.98%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Exhaustividad</simpara></entry>
<entry align="left" valign="top"><simpara>25.0%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones locales</simpara></entry>
<entry align="left" valign="top"><simpara>90.0%</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Precisión en predicciones globales</simpara></entry>
<entry align="left" valign="top"><simpara>99.01%</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
</chapter>
<chapter xml:id="_conclusiones_y_trabajo_futuro_sobre_el_sistema_de_geolocalización">
<title>Conclusiones y trabajo futuro sobre el sistema de geolocalización</title>
<simpara>A raíz de las tablas resumen mostradas en <xref linkend="_experimentos_y_resultados"/> se puede concluir que la geolocalización de usuarios en base a sus contenidos es realmente posible y, además, con un alto grado de precisión (para todos los niveles de granularidad el porcentaje de precisión es superior al 90%).</simpara>
<simpara>Los niveles de exhaustividad se mantienen en un alto porcentaje para los casos de país (por encima del 70%) y empiezan a decaer a medida que se intentan estudiar granularidades más pequeñas y restrictivas. Lo cual, continúa suponiendo una notable mejoría sobre la situación actual, donde la falta de herramientas de este tipo para geolocalizar tuits que no contengan información geográfica adjunta hace que su grado de exhaustividad sea 0. Además, el sistema es lo suficientemente flexible como para poder aumentar el porcentaje de exhaustividad sacrificando los niveles de precisión colocando el umbral por debajo de 0.5.</simpara>
<simpara>Los diferentes prototipos que se han desarrollado utilizando los algoritmos anteriores, prueban como no es necesario recurrir a agentes externos para obtener información geográfica para geolocalizar a un usuario. En muchos de los artículos de investigación comentados en el <emphasis role="strong">Estado del arte</emphasis> era habitual que se hiciera referencia a <emphasis role="strong">gazetteers</emphasis> y sistemas de terceros como <emphasis>Foursquare</emphasis>, para obtener información geográfica en base al reconocimiento de entidades. Este proyecto se ha centrado en desarrollar un proceso automático que permita inferir qué términos son más o menos discriminativos para identificar la localidad de un usuario, partiendo de una premisa que ha sido corroborada en base a los resultados anteriores:</simpara>
<blockquote>
<simpara>Existen términos con una alta frecuencia para un determinado área geográfica y una muy baja dispersión que sirven para inferir la localización de casi cualquier tuit que los contenga.</simpara>
</blockquote>
<section xml:id="_trabajo_futuro">
<title>Trabajo futuro</title>
<simpara>De cara al futuro, sería muy interesante realizar un mayor número de experimentos para mejorar los resultados a nivel de barrio, los cuales, aunque tienen unos buenos niveles de precisión, se quedan en porcentajes de exhaustividad muy limitados. Una opción sería recoger datos únicamente a nivel de área metropolitana, para obtener una cantidad de tuits muy elevada en su granularidad inmediatamente superior, y realizar los experimentos a nivel de barrio utilizando dicho conjunto.</simpara>
<simpara>Otra vía de investigación interesante, podría ser utilizar algoritmos de aprendizaje automático multiclase. De esa manera, podríamos etiquetar cada tuit con distintos niveles de granularidad en función del número de decimales utilizados en sus coordenadas geográficas, pudiendo ofrecer predicciones con márgenes de error de 1, 10 ó 100 kilómetros. Aunque las últimas versiones de Vowpal Wabbit permiten la aplicación de este tipo de algoritmos, la manera en que se devolvían las predicciones no se ajustaba a las condiciones del experimento, puesto que no era capaz de retornar las probabilidades de que su predicción perteneciese a una clase u otra. Por tanto, este trabajo debería realizarse utilizando otro software de aprendizaje automático, como por ejemplo <emphasis role="strong">WEKA</emphasis>.</simpara>
<simpara>Por último, sería necesario comprobar como evolucionan las puntuaciones LLR de cada término a lo largo del tiempo. Aunque se ha demostrado como el contexto temporal en el que se produce un experimento influye fuertemente en la identificación de los términos más discriminativos (como sucedió en el caso del fallecimiento de Luis Aragonés), no se ha podido investigar si, a la larga, los términos identificados en los primeros meses de ejecución de un sistema podrían mermar la precisión del mismo a la hora de realizar nuevas ejecuciones.</simpara>
</section>
</chapter>
<chapter xml:id="_aplicación_web_de_geolocalización_como_servicio">
<title>Aplicación web de geolocalización como servicio</title>
<simpara>A continuación se muestran una serie de secciones que explicarán de manera aproximada la manera de implementar una aplicación web que ofrezca un servicio de geolocalización de tuits en base a su contenido utilizando la tecnología explorada en los prototipos y experimentos anteriores.</simpara>
<simpara>Es importante aclarar que, puesto que no se ha implementado la aplicación, los conceptos de diseño y análisis que se exponen en el presente proyecto podrían diferir de una aplicación real, en donde nunca se optaría por un enfoque <emphasis>en cascada</emphasis> sino que se utilizaría una metodología ágil basada en iteraciones, donde se irían añadiendo <emphasis role="strong">user stories</emphasis> con nuevas características que sirvan a su vez para refinar el diseño e implementación del sistema en cada iteración.</simpara>
<simpara><?asciidoc-pagebreak?></simpara>
<section xml:id="_análisis_del_sistema">
<title>Análisis del sistema</title>
<section xml:id="_definición_y_alcance_del_sistema">
<title>Definición y alcance del Sistema</title>
<simpara>A partir de las tecnologías y experimentos desarrollados en las secciones anteriores, las próximas páginas recogen el análisis y diseño de una posible aplicación web que, basándose en los trabajos anteriores, sirva para ofrecer un servicio de geolocalización de tuits en base a su contenido.</simpara>
<simpara>El sistema deberá ser capaz de permitir a los usuarios la creación de <emphasis role="strong">proyectos</emphasis> en los que identificar sobre qué idioma, localización y términos desean realizar la monitorización. Y ofrecer una interfaz web sencilla en la que puedan consultar los resultados y recuperar la información para poder trabajar posteriormente con ella (es decir, se debe brindar la capacidad de descargar los datos en algún tipo de fichero con información estructurada como CSV, JSON o XML).</simpara>
<simpara>El <emphasis>back-end</emphasis> de la aplicación deberá utilizar la experiencia de los experimentos anteriores para poder repetir los resultados obtenidos. Por tanto, deberá ser capaz de recuperar tuits de manera ininterrumpida a partir de algún streaming de datos con el objetivo de tener constantemente la información necesaria para poder ejecutar los proyectos de los usuarios. Además, debe ser capaz de comunicarse con software de aprendizaje automático y crear (o reutilizar) modelos de clasificación <emphasis>al vuelo</emphasis> para cada uno de los proyectos generados.</simpara>
</section>
<section xml:id="_requisitos_del_sistema">
<title>Requisitos del Sistema</title>
<section xml:id="_requisitos_funcionales">
<title>Requisitos Funcionales</title>
<table xml:id="user-requirements" frame="all" rowsep="1" colsep="1">
<title>Requisitos de Usuario - R1.x</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="16*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Código</entry>
<entry align="left" valign="top">Nombre del Requisito</entry>
<entry align="left" valign="top">Descripción del Requisito</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>R1.1</simpara></entry>
<entry align="left" valign="top"><simpara>Crear cuenta en el Sistema</simpara></entry>
<entry align="left" valign="top"><simpara>El usuario debe ser capaz de crear una cuenta en el sistema mediante la inserción de información básica como: <emphasis role="strong">e-mail</emphasis>, <emphasis role="strong">nombre</emphasis> y <emphasis role="strong">contraseña</emphasis> a través de un formulario de registro disponible en la aplicación web.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R1.2</simpara></entry>
<entry align="left" valign="top"><simpara>Identificarse</simpara></entry>
<entry align="left" valign="top"><simpara>Todo aquel usuario con una cuenta creada en el sistema, deberá ser capaz de identificarse a través de un formulario de <emphasis>login</emphasis> accesible desde la aplicación web.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R1.3</simpara></entry>
<entry align="left" valign="top"><simpara>Crear proyecto</simpara></entry>
<entry align="left" valign="top"><simpara>El usuario debe ser capaz de crear un proyecto sobre el que obtener datos geolocalizados aportando de manera sencilla toda la información necesaria para su evaluación: idioma, palabras a monitorizar, área local y global y fecha de evaluación.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R1.4</simpara></entry>
<entry align="left" valign="top"><simpara>Selección de <emphasis>bounding boxes</emphasis> de manera gráfica</simpara></entry>
<entry align="left" valign="top"><simpara>El usuario debe ser capaz de seleccionar <emphasis>bounding boxes</emphasis> de manera gráfica a través de un mapa.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R1.5</simpara></entry>
<entry align="left" valign="top"><simpara>Evaluar un proyecto</simpara></entry>
<entry align="left" valign="top"><simpara>El usuario debe ser capaz de evaluar un proyecto para las fechas seleccionadas en el momento de su creación.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R1.6</simpara></entry>
<entry align="left" valign="top"><simpara>Descargar resultados de ejecución</simpara></entry>
<entry align="left" valign="top"><simpara>El usuario debe disponer de un mecanismo para descargar los datos de cada ejecución en, al menos, formato XML y JSON.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R1.7</simpara></entry>
<entry align="left" valign="top"><simpara>Acceso a histórico de proyectos</simpara></entry>
<entry align="left" valign="top"><simpara>El usuario debe tener acceso a su historial de proyectos a través de la aplicación web. Deberá ser capaz de reevaluar un proyecto anterior o consultar su información.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R1.8</simpara></entry>
<entry align="left" valign="top"><simpara>Cerrar sesión</simpara></entry>
<entry align="left" valign="top"><simpara>El usuario deberá poder cerrar su sesión en la aplicación de manera sencilla a través de la interfaz web.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="project-requirements" frame="all" rowsep="1" colsep="1">
<title>Requisitos a nivel de Proyecto - R2.x</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="16*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Código</entry>
<entry align="left" valign="top">Nombre del Requisito</entry>
<entry align="left" valign="top">Descripción del Requisito</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>R2.1</simpara></entry>
<entry align="left" valign="top"><simpara>Fechas de ejecución</simpara></entry>
<entry align="left" valign="top"><simpara>Los proyectos deben tener asignada una fecha sobre la que se desea realizar la ejecución. Esta fecha siempre tiene que ser igual o posterior al día de creación del proyecto (en función de si se quiere trabajar con datos ya almacenados en el sistema o con datos futuros respectivamente).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R2.2</simpara></entry>
<entry align="left" valign="top"><simpara>Múltiples idiomas</simpara></entry>
<entry align="left" valign="top"><simpara>Los proyectos deben poder trabajar con múltiples idiomas. Cómo mínimo se debe ofrecer la posibilidad de trabajar con <emphasis role="strong">inglés</emphasis> y <emphasis role="strong">español</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R2.3</simpara></entry>
<entry align="left" valign="top"><simpara>Términos a monitorizar personalizables</simpara></entry>
<entry align="left" valign="top"><simpara>Los proyectos deben poder trabajar sobre una lista de términos propuesta por el usuario y encontrar y geolocalizar aquellos tuits que los contengan. Estos términos se deberán añadir en el momento de la creación del proyecto y deberán cumplir unas condiciones concretas: se debe incluir una fila por cada término que se desee monitorizar.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R2.4</simpara></entry>
<entry align="left" valign="top"><simpara>Localización local</simpara></entry>
<entry align="left" valign="top"><simpara>Los proyectos deben ser capaces de trabajar sobre un área local de la que obtener tuits.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R2.5</simpara></entry>
<entry align="left" valign="top"><simpara>Localización global</simpara></entry>
<entry align="left" valign="top"><simpara>De manera análoga al requisito de localización local, los proyectos debe ser capaces también de trabajar con otras áreas en los que se puedan encontrar tuits escritos en el mismo idioma seleccionado. De esta manera se aportará al sistema de los mecanismos adecuados para poder buscar términos discriminativos.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R2.6</simpara></entry>
<entry align="left" valign="top"><simpara>Reevaluar un proyecto</simpara></entry>
<entry align="left" valign="top"><simpara>El sistema debe ofrecer la posibilidad de reevaluar un proyecto a partir de una nueva fecha de ejecución que cumpla las mismas condiciones que las establecidas en el momento de su creación (se debe seleccionar una fecha igual o posterior a la del momento de la reevaluación).</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="execution-requirements" frame="all" rowsep="1" colsep="1">
<title>Requisitos a nivel de Ejecución - R3.x</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="16*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Código</entry>
<entry align="left" valign="top">Nombre del Requisito</entry>
<entry align="left" valign="top">Descripción del Requisito</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>R3.1</simpara></entry>
<entry align="left" valign="top"><simpara>Exhaustividad variable</simpara></entry>
<entry align="left" valign="top"><simpara>Las ejecuciones deben poder trabajar con diferentes niveles de exhaustividad con el objetivo de procesar más o menos tuits a distintos niveles de confianza.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>R3.2</simpara></entry>
<entry align="left" valign="top"><simpara>Exportar resultados</simpara></entry>
<entry align="left" valign="top"><simpara>Los resultados de una ejecución deben poder ser exportados a <emphasis role="strong">XML</emphasis> y <emphasis role="strong">JSON</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_requisitos_no_funcionales">
<title>Requisitos No Funcionales</title>
<table xml:id="information-retrival-requirements" frame="all" rowsep="1" colsep="1">
<title>Requisitos de acceso a la información - RNF1.x</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="16*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Código</entry>
<entry align="left" valign="top">Nombre del Requisito</entry>
<entry align="left" valign="top">Descripción del Requisito</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>RNF1.1</simpara></entry>
<entry align="left" valign="top"><simpara>Recolección de datos de manera ininterrumpida</simpara></entry>
<entry align="left" valign="top"><simpara>El sistema deberá de crear un flujo de comunicación constante que abastezca de información ininterrumpidamente.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RNF1.2</simpara></entry>
<entry align="left" valign="top"><simpara>Recolección de datos parametrizable</simpara></entry>
<entry align="left" valign="top"><simpara>Se deberá de trabajar con un sistema de <emphasis>streaming</emphasis> de datos que acepte parámetros para devolver información de manera selectiva.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RNF1.3</simpara></entry>
<entry align="left" valign="top"><simpara>Utilización de un sistema de <emphasis>streaming</emphasis> que aporte más de un 1% de los datos disponibles</simpara></entry>
<entry align="left" valign="top"><simpara>Con el objetivo de poder ofrecer el mejor servicio posible, será necesario trabajar con un sistema de <emphasis>streaming</emphasis> que sea capaz ofrecer el mayor número de datos posibles, es decir, que no limite como en el caso de la API Streaming de Twitter, su flujo a únicamente un 1% del total del <emphasis>streaming</emphasis> público.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="security-requirements" frame="all" rowsep="1" colsep="1">
<title>Requisitos de Seguridad - RNF2.x</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="16*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Código</entry>
<entry align="left" valign="top">Nombre del Requisito</entry>
<entry align="left" valign="top">Descripción del Requisito</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>RNF2.1</simpara></entry>
<entry align="left" valign="top"><simpara>SSL</simpara></entry>
<entry align="left" valign="top"><simpara>La aplicación deberá funcionar sobre <emphasis role="strong">HTTPS</emphasis> y <emphasis role="strong">SSL</emphasis> mediante un certificado válido que asegure que la información no pueda ser interceptada por terceras partes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RNF2.2</simpara></entry>
<entry align="left" valign="top"><simpara>Hashing para las contraseñas de usuario</simpara></entry>
<entry align="left" valign="top"><simpara>Las contraseñas de los usuarios deberán ser <emphasis>hasheadas</emphasis> mediante el algoritmo <emphasis role="strong">bcrypt</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="technologies-requirements" frame="all" rowsep="1" colsep="1">
<title>Requisitos Tecnológicos - RNF3.x</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="16*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Código</entry>
<entry align="left" valign="top">Nombre del Requisito</entry>
<entry align="left" valign="top">Descripción del Requisito</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>RNF3.1</simpara></entry>
<entry align="left" valign="top"><simpara>Scala</simpara></entry>
<entry align="left" valign="top"><simpara>Se utilizará el lenguaje de programación <emphasis role="strong">Scala</emphasis> con el objetivo de aprovechar la experiencia previa de la etapa de experimentación y prototipado. Su soporte para programación funcional dará al sistema la flexibilidad necesaria en materia de escalabilidad y concurrencia de cara al futuro.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RNF3.2</simpara></entry>
<entry align="left" valign="top"><simpara>Play! Framework</simpara></entry>
<entry align="left" valign="top"><simpara>Play! será el framework web utilizado para desarrollar la aplicación. Su soporte nativo para Scala, desarrollo de trabajos asíncronos e imposición de la arquitectura <emphasis role="strong">Modelo-Vista-Controlador</emphasis> encaja a la perfección con los requisitos del sistema.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RNF3.3</simpara></entry>
<entry align="left" valign="top"><simpara>Vowpal Wabbit</simpara></entry>
<entry align="left" valign="top"><simpara>Vowpal Wabbit y su ejecución como <emphasis>daemon</emphasis> del sistema, velocidad y buenos resultados en la etapa de prototipado, será el software de aprendizaje automático utilizado.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RNF3.4</simpara></entry>
<entry align="left" valign="top"><simpara>Mercurial</simpara></entry>
<entry align="left" valign="top"><simpara>Se utilizará Mercurial como sistema de control de versiones distribuido apoyado sobre <emphasis role="strong">Bitbucket</emphasis> como hosting del proyecto. Mercurial es un SCVD similar a Git, pero que simplifica varias de las acciones habituales en el flujo de trabajo de un desarrollador: <literal>pull</literal>, <literal>push</literal>, <literal>merge</literal>, etc. A pesar de no tener un sistema de ramas tan potente como Git, permite un funcionamiento <emphasis>similar</emphasis> mediante el uso de <emphasis role="strong">bookmarks</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RNF3.5</simpara></entry>
<entry align="left" valign="top"><simpara>Cola de mensajes con características TTL</simpara></entry>
<entry align="left" valign="top"><simpara>La implementación del estándar AMQP seleccionada, deberá ofrecer la capacidad de seleccionar el <emphasis role="strong">Time To Live</emphasis> tanto a nivel de cola como a nivel de mensaje, para evitar que se acumulen excesivos tuits en el sistema de colas. Para ello, lo ideal es que cada mensaje expire en el mismo momento de llegar a la cola a no ser que algún consumidor este disponible para procesarlo.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="storage-requirements" frame="all" rowsep="1" colsep="1">
<title>Requisitos de Almacenamiento - RNF4.x</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="16*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Código</entry>
<entry align="left" valign="top">Nombre del Requisito</entry>
<entry align="left" valign="top">Descripción del Requisito</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>RNF4.1</simpara></entry>
<entry align="left" valign="top"><simpara>48h de tuits.</simpara></entry>
<entry align="left" valign="top"><simpara>Debido a que los experimentos realizados han funcionado bien con datos recogidos en periodos de 24 horas, se almacenarán datos únicamente sobre periodos de 48h. eliminando todos aquellos tuits más antiguos con el objetivo de minimizar el tamaño de disco utilizado.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RNF4.2</simpara></entry>
<entry align="left" valign="top"><simpara>Máximo número de tuits por ejecución.</simpara></entry>
<entry align="left" valign="top"><simpara>Para prevenir un abuso de la capacidad de disco, que podría llegar a almacenar ficheros de cientos de megabytes en resultados, es necesario limitar el número máximo de tuits que se podrán guardar por ejecución para generar los ficheros XML y JSON. Este valor se deberá calcular de manera experimental, pero el fichero de resultados no debería ser superior a <emphasis role="strong">~200MB</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="_análisis_de_casos_de_uso_y_escenarios">
<title>Análisis de casos de uso y escenarios</title>
<figure>
<title>Casos de Uso para la Gestión de Usuarios</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/use-cases/user-managements.png" align="center"/>
</imageobject>
<textobject><phrase>user managements</phrase></textobject>
</mediaobject>
</figure>
<table xml:id="cu-user-system-1-1" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión Usuarios 1.1 Crear cuenta en el sistema</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se encuentra en la página principal de la aplicación y no dispone de una cuenta de usuario en el sistema.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá completado satisfactoriamente el proceso de creación de cuenta y podrá hacer <emphasis>login</emphasis> en el sistema de manera satisfactoria.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Situándose en la pantalla de inicio de la aplicación, el usuario accederá al formulario de creación de cuenta mediante un enlace o cualquier otro componente de la interfaz de usuario que alerte gráficamente de que se puede crear una nueva cuenta de usuario. Una vez en el formulario, se pedirán datos básicos como: <emphasis role="strong">e-mail</emphasis>, <emphasis role="strong">nombre</emphasis> y <emphasis role="strong">contraseña</emphasis> (con su correspondiente confirmación). Al rellenar el formulario, el sistema deberá procesar la petición y notificar al usuario de que su cuenta se ha creado con éxito.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Si el usuario introduce valores incorrectos en algunos de los campos, el sistema deberá aportar un mecanismo que alerte del error y vuelva a mostrar el formulario de registro.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.1</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-system-1-2" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión Usuarios 1.2 Identificarse en el sistema</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario debe tener una cuenta ya creada en el sistema, no debe estar identificado y debe estar situado en la página de inicio de la aplicación.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá podido identificarse satisfactoriamente y acceder a su <emphasis role="strong">dashboard</emphasis> de proyectos y evaluaciones.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Desde la pantalla principal de la aplicación, el usuario podrá acceder al formulario de identificación mediante un enlace o elemento identificativo a nivel gráfico en la interfaz web. La página de identificación constará de un pequeño formulario donde el usuario pueda insertar su <emphasis role="strong">e-mail</emphasis> y <emphasis role="strong">contraseña</emphasis>. Una vez identificado correctamente, el sistema redireccionará al usuario a su <emphasis role="strong">dashboard</emphasis> de proyectos y ejecuciones.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>En caso de que las credenciales introducidas por el usuario sean incorrectas, el sistema mostrará sobre el mismo formulario de identificación que existe un problema con las credenciales indicando que no son válidas.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.2</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-system-1-3" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión Usuarios 1.3 Cerrar sesión en el sistema</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se encuentra identificado en el sistema.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá conseguido cerrar su sesión actual en el sistema y estará situado en la página de inicio.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Estando el usuario identificado en el sistema, y siendo la acción accesible desde cualquier punto de la aplicación a través de un menú superior o elemento gráfico permanente, se podrá seleccionar <emphasis>Cerrar sesión</emphasis> sobre la sesión actual del usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>En caso de no poder completar la acción, se deberá notificar al usuario de que ha existido un error a la hora de cerrar su sesión actual.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.8</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<figure>
<title>Casos de Uso para la Gestión de Proyectos</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/use-cases/projects-management.png" align="center"/>
</imageobject>
<textobject><phrase>projects management</phrase></textobject>
</mediaobject>
</figure>
<table xml:id="cu-user-project-2-1" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Proyectos 2.1 Crear Proyecto</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se encuentra identificado y situado en su <emphasis role="strong">dashboard</emphasis> de proyectos y ejecuciones.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario tendrá un nuevo proyecto creado en su cuenta.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Desde la pantalla de dashboard, el usuario podrá acceder a la creación de un nuevo proyecto mediante un elemento gráfico visible en la interfaz web que lo conducirá a un formulario de creación. En este formulario se deberá completar información básica como: <emphasis role="strong">nombre</emphasis>, <emphasis role="strong">fecha</emphasis>, <emphasis role="strong">área local y global</emphasis>, <emphasis role="strong">términos a monitorizar</emphasis> e <emphasis role="strong">idioma</emphasis> para el nuevo proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>En caso de introducir valores incorrectos en el formulario de creación, el sistema deberá notificar sobre el mismo formulario qué errores existen y que no se ha podido completar el proceso de creación.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.3</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-project-2-1-1" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Proyectos 2.1.1 Seleccionar idioma</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se encuentra en el formulario de creación de proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá seleccionado el idioma del proyecto satisfactoriamente.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>En el momento de la creación del proyecto, se deberá mostrar un componente visual que permita al usuario seleccionar el idioma del proyecto a partir de una lista de opciones predefinidas. Como mínimo, se deberán mostrar las opciones de <emphasis role="strong">inglés</emphasis> y <emphasis role="strong">español</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R2.2</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-project-2-1-2" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Proyectos 2.1.2 Añadir términos a monitorizar</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se encuentra en el formulario de creación de proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá seleccionado una lista de términos a monitorizar.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario dispone de un campo en el formulario de creación de proyectos en el que puede seleccionar un fichero de su disco duro para introducir las palabras o términos que desea monitorizar.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Preferiblemente, se debería realizar un análisis del fichero en el momento en que el usuario realiza la subida del mismo para asegurar que cumpla con el formato establecido. En caso de no hacerlo, habría que realizar la notificación en el punto de ejecución del proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R2.3</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-project-2-1-3" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Proyectos 2.1.3 Seleccionar área local</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se encuentra en el formulario de creación de proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá seleccionado la localización local sobre la que desea realizar la ejecución.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>En el formulario de creación de un nuevo proyecto, el usuario podrá seleccionar de manera gráfica a través de un mapa, las localizaciones sobre las que desea realizar la monitorización.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.4; R2.4</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-project-2-1-4" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Proyectos 2.1.4 Seleccionar área global</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se encuentra en el formulario de creación de proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá seleccionado las localizaciones globales que ayudarán a identificar términos del área local.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>En el formulario de creación de un nuevo proyecto, el usuario podrá seleccionar de manera gráfica a través de un mapa, las localizaciones globales que ayudarán a reforzar el conocimiento sobre las áreas locales.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Aunque no exista ninguna restricción a la hora de señalar las localizaciones globales, se debería indicar al usuario que las localizaciones globales deben ser territorios donde se hable el mismo idioma que en los territorios seleccionados para el área local.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.4; R2.5</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-project-2-1-5" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Proyectos 2.1.5 Establecer fecha de ejecución</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se encuentra en el formulario de creación de proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá establecido una fecha para la ejecución del proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>En el formulario de creación de un nuevo proyecto el usuario deberá seleccionar, a través de un componente diseñado para tal fin (un calendario o similar), la fecha para realizar la ejecución del proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Se deberá utilizar algún mecanismo que indique al usuario que no debe seleccionar fechas anteriores al momento actual en el que realiza la creación (por ejemplo, deshabilitando los días anteriores, en caso de tratarse de un componente de tipo Calendario).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.5; R2.1; R.6</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-project-2-2" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Proyectos 2.2 Acceder al histórico de proyectos</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario está identificado correctamente en el sistema.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario es capaz de visualizar el histórico de proyectos.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Desde la página del <emphasis role="strong">dashboard</emphasis> principal del usuario se debe poder acceder a una sección de <emphasis role="strong">Proyectos</emphasis> donde se recojan todos los proyectos realizados por el usuario así como una breve descripción de sus características.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.7</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-project-2-2-1" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Proyectos 2.2.1 Consultar proyecto anterior</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario está visualizando el histórico de Proyectos.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario está visualizando la vista en detalle de un proyecto anterior.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario debe tener un enlace en la página del histórico de proyectos que le permita consultar los detalles del Proyecto así como realizar alguna modificación o reevaluación.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.7; R2.6</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<figure>
<title>Casos de Uso para la Gestión de Ejecuciones</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/use-cases/execution-management.png" align="center"/>
</imageobject>
<textobject><phrase>execution management</phrase></textobject>
</mediaobject>
</figure>
<table xml:id="cu-user-project-3-1" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Ejecuciones 3.1 Evaluar proyecto</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se encuentra en la vista detallada de un proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá realizado una ejecución con éxito.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario deberá poder ejecutar un proyecto para las fechas indicadas en el momento de su creación. Para ello, se deberá de indicar de manera gráfica cuando una ejecución es posible y los dos mecanismos para empezar y detener dicha ejecución.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Será posible también iniciar la ejecución de un proyecto directamente desde el <emphasis role="strong">dashboard</emphasis> de proyectos mediante un enlace en la zona del resumen de características.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Preferiblemente, la ejecución de un proyecto se deberá realizar en una vista independiente de la vista detallada del proyecto donde se origine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.5</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-project-3-2" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Ejecuciones 3.2 Consultar resultados de ejecución</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario ha realizado una ejecución de manera satisfactoria y se encuentra en el <emphasis role="strong">dashboard</emphasis> de ejecuciones.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario ha podido descargar satisfactoriamente los resultados de la ejecución.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Desde el conjunto de ejecuciones realizadas, se deberá poder acceder a sus resultados obtenidos mediante un enlace para descargar el fichero de resultados en formato <emphasis role="strong">XML</emphasis> o <emphasis role="strong">JSON</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R1.6; R3.2</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="cu-user-project-3-3" frame="all" rowsep="1" colsep="1">
<title>CU Usuario - Gestión de Ejecuciones 3.3 Reevaluar proyecto</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Precondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario se debe de encontrar en la vista del histórico de proyectos.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Postcondiciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>El usuario habrá sido capaz de reevaluar un proyecto.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Un usuario.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descripción</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Para reevaluar un proyecto que ya ha sido evaluado anteriormente, será necesario modificar su fecha de evaluación para la nueva fecha sobre la que se desea realizar. En ese momento el sistema volverá a recopilar los datos necesarios para poder realizar una reevaluación. En cuanto los datos estén disponibles, se deberá seguir el mismo escenario presentado para <xref linkend="cu-user-project-3-1"/></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Variaciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Excepciones</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Notas</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requisitos contemplados</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>R2.6</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_diseño_e_identificación_preliminar_de_sistemas_y_clases">
<title>Diseño e Identificación preliminar de Sistemas y Clases</title>
<section xml:id="_identificación_de_sistemas">
<title>Identificación de Sistemas</title>
<simpara>A continuación se realiza una separación del proyecto en varios sistemas y subsistemas. A pesar de que el proyecto será construido en su gran mayoría sobre una única aplicación web, se puede dividir conceptualmente en grupos de clases que trabajarán sobre un objetivo común, como por ejemplo el grupo de clases encargadas de tareas de administración, evaluación, etc.</simpara>
<section xml:id="_sistema_de_administración">
<title>Sistema de Administración</title>
<figure>
<title>Sistema de Administración</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/systems/administration.png" align="center"/>
</imageobject>
<textobject><phrase>administration</phrase></textobject>
</mediaobject>
</figure>
<simpara>El <emphasis role="strong">Sistema de Administración</emphasis> se encarga de implementar las tareas básicas para la gestión de <emphasis role="strong">usuarios</emphasis>, <emphasis role="strong">proyectos</emphasis> y <emphasis role="strong">ejecuciones</emphasis>. Entre estas tareas, se encuentran las de creación, actualización y borrado; así como el control de las relaciones que se establecen entre los modelos.</simpara>
</section>
<section xml:id="_sistema_de_evaluación">
<title>Sistema de Evaluación</title>
<figure>
<title>Sistema de Evaluación</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/systems/evaluation.png" align="center"/>
</imageobject>
<textobject><phrase>evaluation</phrase></textobject>
</mediaobject>
</figure>
<simpara>En el <emphasis role="strong">Sistema de Evaluación</emphasis> nos encontramos antes una situación diferente al apartado anterior. Este sistema deberá trabajar conjuntamente con clases propias de la aplicación así como también con un sistema externo que actuará como <emphasis>wrapper</emphasis> sobre el software de aprendizaje automático <emphasis role="strong">Vowpal Wabbit</emphasis>. La comunicación entre ambos sistemas se realizará a través de un servicio web <emphasis role="strong">RESTful</emphasis> implementado por el <emphasis>wrapper</emphasis> de Vowpal Wabbit.</simpara>
<simpara>Por otro lado, el Sistema de Evaluación trabajará en base a <emphasis role="strong">Proyectos</emphasis> que recibirá a través de su interfaz pública y contendrá a su vez los subsistemas encargados de procesar los datos del proyecto y generar los ficheros de puntuación para entrenar al modelo de aprendizaje automático.</simpara>
<simpara>Por último, también agrupa el sistema encargado de realizar las ejecuciones sobre los proyectos para obtener el conjunto de resultados geolocalizados en el área de interés. Este sistema se comunicará directamente con el subsistema de aprendizaje automático (<emphasis role="strong">Learning system</emphasis>) y con el Sistema de Streaming, el cual le proporcionará acceso a un flujo de datos en tiempo real.</simpara>
</section>
<section xml:id="_sistema_de_streaming">
<title>Sistema de Streaming</title>
<figure>
<title>Sistema de Streaming</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/systems/streaming.png" contentdepth="250px" align="center"/>
</imageobject>
<textobject><phrase>streaming</phrase></textobject>
</mediaobject>
</figure>
<simpara>El <emphasis role="strong">Sistema de Streaming</emphasis> será el encargado de implementar la lógica para comunicarse con el Streaming de Twitter y ofrecer tuits al resto de la aplicación. Sus dos principales misiones serán:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Guardar cada tuit recibido en la base de datos.</simpara>
</listitem>
<listitem>
<simpara>Hacer a su vez de streaming para otros componentes del sistema que necesiten acceso a un streaming de tuits (por ejemplo el sistema de ejecuciones de proyectos necesitará un flujo de datos real para inferir la localización de cada tuit).</simpara>
</listitem>
</orderedlist>
<simpara>Dentro de este sistema, se encuentra un último componente denominado <emphasis role="strong">Cleaner</emphasis> y cuya principal misión será ejecutarse de manera periódica para eliminar todos aquellos tuits de la base de datos recogidos en un plazo mayor de 48h.</simpara>
<simpara>La interfaz común entre ambos subsistemas será la base de datos en la que se almacenen los tuits obtenidos.</simpara>
</section>
</section>
<section xml:id="_diseño_de_clases_de_análisis">
<title>Diseño de Clases de Análisis</title>
<section xml:id="_sistema_de_administración_2">
<title>Sistema de Administración</title>
<figure>
<title>Clases del Sistema de Administración</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/systems/administration-classes.png" align="center"/>
</imageobject>
<textobject><phrase>administration classes</phrase></textobject>
</mediaobject>
</figure>
<simpara>El diagrama de clases para el Sistema de Administración muestra únicamente tres entidades que intentan reflejar lo señalado anteriormente respecto a este mismo sistema. Su única misión será realizar tareas de administración, como por ejemplo la creación de nuevos usuarios y proyectos, agregar a un usuario un proyecto o agregar una ejecución a un proyecto ya existente.</simpara>
<simpara>Lo más significativo de este diagrama es la representación de las asociaciones entre modelos y su cardinalidad.</simpara>
</section>
<section xml:id="_sistema_de_evaluación_2">
<title>Sistema de Evaluación</title>
<figure>
<title>Clases del Sistema de Evaluación</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/systems/evaluation-classes.png" align="center"/>
</imageobject>
<textobject><phrase>evaluation classes</phrase></textobject>
</mediaobject>
</figure>
<simpara>En el Sistema de Evaluación nos encontramos con el siguiente conjunto de clases:</simpara>
<variablelist>
<varlistentry>
<term><literal>ProjectDataProcessor</literal></term>
<listitem>
<simpara>Esta clase se ejecutará de manera asíncrona y recogerá la lógica para obtener los datos necesarios para la evaluación de un Proyecto. Para ello, utilizará las clases <literal>TermFrequencyExtractor</literal> para extraer la frecuencia por término de un conjunto de tuits. <literal>LogLikelihoodRatioGenerator</literal>, que trabajará con dos conjuntos de Término-Frecuencia para tuits pertenecientes al área global y tuits perteneciente al área local; y <literal>TweetScoreCalculator</literal>, que a partir de las puntuaciones LLR generadas, calculará la puntuación total de un nuevo conjunto de tuits mezcla de área local y global.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>MachineLearningManager</literal></term>
<listitem>
<simpara>Esta clase se utilizará como interfaz común para trabajar con sistemas software de aprendizaje automático. En ella, se recogen los métodos básicos que toda implementación debe definir para poder trabajar correctamente con el sistema:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>create_training_data</literal>: transforma un conjunto de tuits con su puntuación LLR asociada a un input válido para el software de aprendizaje automático que se quiera utilizar. Devolverá una instancia de la clase <literal>MachineLearningData</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>create_model</literal>: a partir de un objeto de tipo <literal>MachineLearningData</literal>, creará un modelo de evaluación en el clasificador de tipo <literal>MachineLearningModel</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>evaluate</literal>: utilizando un objeto de tipo <literal>MachineLearningModel</literal> y una cadena que represente el tuit a evaluar, devolverá un objeto de tipo <literal>MachineLearningPrediction</literal> con la predicción devuelta por el clasificador.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>VowpalWabbitManager</literal></term>
<listitem>
<simpara>Implementación de la interfaz anterior para trabajar con el software de aprendizaje automático Vowpal Wabbit. Puesto que Vowpal Wabbit se encontrará en un servicio web externo, será necesario utilizar una biblioteca que permite conectarse a un servicio RESTful para realizar las operaciones necesarias.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>ProjectExecutionService</literal></term>
<listitem>
<simpara>A partir de una instancia de la clase <literal>Project</literal>, este objeto realizará una ejecución del mismo utilizando los datos almacenados en el modelo del software de aprendizaje automático seleccionado. Esta clase tendrá una asociación con el Sistema de Streaming que le permita obtener un flujo de datos en tiempo real sobre el que realizar la ejecución.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="_sistema_de_streaming_2">
<title>Sistema de Streaming</title>
<figure>
<title>Clases del Sistema de Streaming</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/systems/streaming-classes.png" align="center"/>
</imageobject>
<textobject><phrase>streaming classes</phrase></textobject>
</mediaobject>
</figure>
<simpara>El Sistema de Streaming tiene como objetivo comunicarse con un flujo de tuits ofrecido por una API externa y tener la capacidad de procesarlo satisfactoriamente para ofrecer esas mismas capacidades de streaming al resto de la aplicación. La solución que se propone en este caso es utilizar el patrón de diseño <emphasis role="strong">Observer</emphasis>, en donde la clase <literal>TwitterStreamingService</literal> realizaría la conexión con el servicio externo, y lanzaría un evento cada vez que recibiera un nuevo tuit. De esta manera, las clases que requieran de un flujo de tuits en tiempo real únicamente tendría que suscribirse como <emphasis>listener</emphasis> de la clase <literal>TwitterStreamingService</literal>, implementar la interfaz <literal>TwitterStreamingListener</literal> y redefinir su método <literal>on_tweet_received</literal> para trabajar con el nuevo tuit recibido.</simpara>
<simpara>La clase <literal>RecollectionService</literal>, por ejemplo, implementaría la interfaz anterior para guardar todos los tuits recibidos en la base de datos, actuando así como el recolector de tuits del sistema. Por otro lado, la clase <literal>PublicationService</literal> realizará una implementación para publicar cada tuit recibido al resto del sistema.</simpara>
<simpara>Con el objetivo de poder realizar un sistema escalable, el sistema de streaming se ejecutará como un componente externo de la aplicación, puesto que podría presentar problemas a la hora de ejecutar dos instancias de la aplicación con el sistema de streaming integrado (produciendo resultados duplicados en la base de datos por acceder de manera concurrente al mismo flujo de datos).</simpara>
<simpara>Para comunicar ambos sistemas se utilizará una cola de mensajes (como por ejemplo <emphasis role="strong">RabbitMQ<footnote><simpara><link xlink:href="http://www.rabbitmq.com/">http://www.rabbitmq.com/</link></simpara></footnote></emphasis>) que transportará los tuits en un formato intermedio (JSON). En la aplicación web, cada instancia que necesite un flujo de datos necesitará suscribirse a una de las colas para recuperar el mensaje y poder trabajar con él (un caso de uso para esta funcionalidad sería en el momento de ejecutar un proyecto, donde la aplicación web necesitará un flujo en tiempo real de tuits para evaluarlos contra el modelo e identificar aquellos que pertenecen al área local).</simpara>
<simpara><literal>Cleaner</literal> es una clase que se ejecutará como <emphasis role="strong">cron</emphasis> de la aplicación de manera periódica en plazos de 24 horas. Su misión será eliminar tuits más antiguos de 48 horas.</simpara>
</section>
</section>
</section>
<section xml:id="_diseño_de_emphasis_mockups_emphasis_para_interfaces_de_usuario">
<title>Diseño de <emphasis>mockups</emphasis> para interfaces de usuario</title>
<simpara>Con el objetivo de crear una interfaz completamente funcional y sencilla de utilizar para el usuario, se han generado una serie de <emphasis>mockups</emphasis> que podrían ayudar en el diseño final de la aplicación.</simpara>
<simpara>La idea principal es mantener una interfaz de usuario con poco ruido, que muestre únicamente aquellas opciones básicas que el usuario pueda necesitar para realizar cada acción y teniendo en mete que en el futuro se podrían ofrecer operaciones más avanzadas para usuarios especializados utilizando para ello mecanismos más sofisticados.</simpara>
<simpara>A continuación se muestran un conjunto de <emphasis>mockups</emphasis> para las páginas con una mayor importancia para los usuarios.</simpara>
<section xml:id="_pantalla_de_inicio_y_login_de_la_aplicación">
<title>Pantalla de inicio y login de la aplicación</title>
<simpara>La pantalla de inicio de la aplicación consta de un formulario central donde el usuario deberá introducir sus credenciales si desea poder acceder a su panel de administración.</simpara>
<simpara>En caso de no tener una cuenta de usuario, se le informa del mecanismo que debe seguir si desea crear una (en este caso, hacer <emphasis>click</emphasis> sobre el enlace informativo).</simpara>
<simpara>Esta pantalla se mostrará tanto en la página raíz del sitio web para aquellos usuarios sin identificación en el sistema, como cada vez que se intente acceder a una página interior sin haberse identificado previamente.</simpara>
<figure>
<title>Pantalla de login de la aplicación.</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/mockup/home.png" align="center"/>
</imageobject>
<textobject><phrase>home</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="_pantalla_del_dashboard_de_proyectos">
<title>Pantalla del dashboard de Proyectos</title>
<simpara>La pantalla de administración, cuyo objetivo es gestionar el conjunto de proyectos de un usuario, se ha diseñado de tal manera que permita navegar de manera sencilla sobre el histórico de Proyectos, así como disponer de un acceso rápido para crear uno nuevo.</simpara>
<figure>
<title>Pantalla que muestra el dashboard del usuario en la sección de proyectos</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/mockup/dashboard-projects.png" align="center"/>
</imageobject>
<textobject><phrase>dashboard projects</phrase></textobject>
</mediaobject>
</figure>
<simpara>La sección que muestra la información sobre cada proyecto, aglutina también un pequeño resumen con sus características principales:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Nombre</simpara>
</listitem>
<listitem>
<simpara>Fecha de ejecución</simpara>
</listitem>
<listitem>
<simpara>Idioma</simpara>
</listitem>
<listitem>
<simpara>Localización global</simpara>
</listitem>
<listitem>
<simpara>Localización local</simpara>
</listitem>
<listitem>
<simpara>Términos a monitorizar</simpara>
</listitem>
</orderedlist>
<simpara>Además, el color de fondo que acompaña cada sección informa de si el proyecto se puede ejecutar en ese momento (fondo verde y botón para <emphasis role="strong">Comenzar ejecución en tiempo real</emphasis>) o si su fecha de ejecución ya ha pasado (color de fondo rojo).</simpara>
<simpara>En la parte superior derecha de cada Proyecto, se presenta un acceso rápido para realizar una edición de sus características.</simpara>
</section>
<section xml:id="_pantalla_del_dashboard_de_ejecuciones">
<title>Pantalla del dashboard de Ejecuciones</title>
<simpara>En el caso del panel de administración de las Ejecuciones de usuario, se muestra una tabla paginada a modo de histórico desde la que es posible consultar los detalles generales de la ejecución así como obtener un enlace a su fichero de resultados.</simpara>
<figure>
<title>Pantalla que muestra el dashboard del usuario en la sección de ejecuciones</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/mockup/dashboard-executions.png" align="center"/>
</imageobject>
<textobject><phrase>dashboard executions</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="_pantallas_para_la_creación_de_un_nuevo_proyecto">
<title>Pantallas para la creación de un nuevo Proyecto</title>
<simpara>Las siguientes imágenes muestran un conjunto de <emphasis>lightboxes</emphasis> que contienen el <emphasis>wizard</emphasis> propuesto para la creación de un nuevo proyecto. La primera imagen contiene el paso número uno, en donde se insertan las características principales del proyecto como su <emphasis role="strong">nombre</emphasis>, <emphasis role="strong">idioma</emphasis> o <emphasis role="strong">términos a monitorizar</emphasis>.</simpara>
<simpara>Los pasos dos y tres, muestran un mapa en el que el usuario podrá dibujar de manera guiada rectángulos que actuarán como <emphasis>bounding boxes</emphasis> para indicar las áreas local y global sobre las que realizar la ejecución. Un ejemplo real con la interfaz propuesta se puede observar en la aplicación web <emphasis role="strong">B2pick</emphasis> (ver <xref linkend="_b2pick_aplicación_web_para_seleccionar_bounding_boxes"/>).</simpara>
<figure>
<title>Lightboxes de creación de un nuevo proyecto</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/mockup/new-project.png" contentdepth="650px" align="center"/>
</imageobject>
<textobject><phrase>new project</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="_pantalla_de_ejecución">
<title>Pantalla de Ejecución</title>
<simpara>Por último, la pantalla de ejecución será el lugar en el que el usuario podrá ejecutar en tiempo real un proyecto y observar qué resultados consigue obtener. Su objetivo principal es ser lo más dinámica y flexible posible para permitir al usuario obtener información inmediata de los resultados de la ejecución. Por tanto, se establecen dos columnas a los laterales de la pantalla (<emphasis role="strong">Dentro</emphasis> y <emphasis role="strong">Fuera</emphasis>) donde se irán colocando los tuits que se reciban en función de la localización inferida por el clasificador (se colocarán en la columna de <emphasis role="strong">Dentro</emphasis> aquellos tuits que el clasificador considere que pertenecen al <emphasis role="strong">área local</emphasis> y viceversa).</simpara>
<simpara>Los controles centrales permitirán iniciar y detener la ejecución, y el <emphasis>spinner</emphasis> inferior permitirá al usuario establecer el nivel de confianza que requiere por parte del sistema para etiquetar un tuit como local. Si se indica que el sistema deba tener un grado de confianza muy alto, los resultados para la columna de <emphasis role="strong">Dentro</emphasis> serán muchos menos, pero su precisión será mucho más elevada.</simpara>
<figure>
<title>Pantalla de ejecución de un proyecto en tiempo real</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/analysis/mockup/execution.png" align="center"/>
</imageobject>
<textobject><phrase>execution</phrase></textobject>
</mediaobject>
</figure>
<simpara><?asciidoc-pagebreak?></simpara>
</section>
</section>
</section>
<section xml:id="_diseño_del_sistema">
<title>Diseño del sistema</title>
<simpara>Las siguientes páginas muestran el diseño propuesto para realizar la aplicación web de geolocalización como servicio. Debido a que el objetivo de este capítulo es indicar como se podría integrar en una aplicación web <emphasis>convencional</emphasis> la tecnología estudiada en los capítulos anteriores, se han omitido los detalles acerca de subsistemas y conjuntos de clases que no están relacionados directamente con el problema en cuestión. Específicamente, se han dejado de lado las capas de acceso a datos y las capas de presentación.</simpara>
<simpara>También es importante señalar que el diseño está pensado para ser implementado utilizando el framework de desarrollo web <emphasis role="strong">Play!</emphasis> y el lenguaje de programación <emphasis role="strong">Scala</emphasis>. Las razones de dicha elección, que ya figura como requisito del sistema en la sección de análisis, es que Scala ha sido el lenguaje utilizado para desarrollar los prototipos y experimentos para obtener un algoritmo capaz de inferir la localización de una publicación en medios sociales y su eficacia para tratar con un problema de este tipo, por tanto, ya ha sido probada con anterioridad. Play! es el framework de referencia para realizar aplicaciones web sobre dicho lenguaje, por lo que prácticamente es un requisito implícito al hecho de utilizar Scala como lenguaje principal.</simpara>
<simpara>A pesar de ello, Play! tiene una serie de características propias que será necesario remarcar para conseguir una mejor comprensión del diseño posterior:</simpara>
<variablelist>
<varlistentry>
<term>MVC</term>
<listitem>
<simpara>Play! adopta de manera muy estricta el patrón de diseño <emphasis role="strong">Modelo-Vista-Controlador</emphasis>. Por tanto la aplicación se estructura, de manera general, en un conjunto de acciones recogidas en un controlador que se encargan de vincular vistas con modelos de la aplicación. En los siguientes diagramas, los ficheros y clases que representarían las vistas y controladores se han omitido, puesto que el objetivo de esta sección es mostrar como podría ser aplicable e integrable en una aplicación web la tecnología propuesta en los prototipos y experimentos anteriores.</simpara>
<simpara>La filosofía de Play! sigue la estela de otros frameworks de desarrollo web muy populares como <emphasis>Ruby on Rails</emphasis> o <emphasis>Django</emphasis>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Tareas asíncronas</term>
<listitem>
<simpara>A lo largo de los diferentes diagramas se verá la notación <literal>«async»</literal> acompañando a algunas de las clases. Esto es debido a que Play! tiene soporte nativo para el desarrollo de tareas y trabajos asíncronos como parte de la aplicación web y, por tanto, debido a que en algunos casos las tareas realizadas por el sistema podrían ocupar varias horas, se ha decidido aprovechar esta característica para representar aquellas tareas que se deberían de ejecutar de manera asíncrona para evitar un bloqueo en la interacción del usuario con la aplicación web.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>trait</literal>, <literal>object</literal> y <literal>Unit</literal></term>
<listitem>
<simpara>Puesto que la aplicación se desarrollaría utilizando el lenguaje de programación Scala, se ha intentado mantener su propia notación para referirse a aspectos clave del sistema. Con el objetivo de facilitar el seguimiento de los diagramas y evitando que esta notación pueda provocar una falta de entendimiento de los mismos, se recogen de manera breve algunas definiciones de los términos más frecuentes:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>trait</literal>: De manera simplificada, se podría entender como un concepto equivalente a las <emphasis role="strong">interfaces</emphasis> en Java.</simpara>
</listitem>
<listitem>
<simpara><literal>object</literal>: Cualquier clase acompañada de esta notación, se ejecutará como un <emphasis role="strong">Singleton</emphasis> por parte del sistema.</simpara>
</listitem>
<listitem>
<simpara><literal>Unit</literal>: Equivalente al concepto de <literal>void</literal> en Java.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<section xml:id="_diagrama_de_paquetes">
<title>Diagrama de paquetes</title>
<simpara>El siguiente diagrama de paquetes muestra de manera general la relación entre los diferentes grupos de clases de la aplicación web ordenadas de manera conceptual en función de sus responsabilidades.</simpara>
<simpara>El paquete de <literal>models</literal> recoge las clases que representan los modelos de la aplicación. En él podremos encontrar las clases <literal>User</literal>, <literal>Project</literal>, <literal>Execution</literal>, <literal>Tweet</literal> o <literal>BoundingBox</literal>.</simpara>
<simpara>En <literal>evaluation</literal> se agrupan las clases encargadas de realizar la lógica de evaluación de un proyecto: obtención de datos de entrenamiento, cálculo e implementación de los algoritmos de geolocalización o ejecución de un proyecto.</simpara>
<simpara>El paquete <literal>learning</literal> contiene las clases encargadas de realizar la comunicación con el software de aprendizaje automático.</simpara>
<simpara>En <literal>streaming</literal> se recogen las clases encargadas de suscribirse a la cola de mensajes para recuperar los tuits recibidos por el sistema externo de Streaming y hacerlos accesibles para el resto de la aplicación.</simpara>
<figure>
<title>Diagrama de paquetes</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/design/packages.png" align="center"/>
</imageobject>
<textobject><phrase>packages</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="_diagramas_de_clases">
<title>Diagramas de clases</title>
<simpara>A continuación, se muestran una serie de diagramas que reflejan la organización de las clases para poder implementar los aspectos más interesantes de la aplicación web.</simpara>
<simpara>En primer lugar, la figura <xref linkend="streaming-system-diagram"/> muestra el sistema externo (implementado en Scala) encargado de comunicarse con el <emphasis>firehose</emphasis> de Twitter. El paquete <literal>com.bonobo.streaming.retriever</literal> contiene las clases encargadas de comunicarse con el servicio de streaming y hacer accesibles al resto de la aplicación los tuits obtenidos. Como se comentó en la sección de Análisis, se hace uso del patrón de diseño <emphasis role="strong">Observer</emphasis> para permitir que múltiples instancias puedan hacer uso del mismo flujo de datos.</simpara>
<simpara>El resto de paquetes representan la estructura convencional de cualquier sistema informático:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>com.bonobo.streaming.main</literal>: Contiene la clase que inicia la ejecución del sistema.</simpara>
</listitem>
<listitem>
<simpara><literal>com.bonobo.streaming.configuration</literal>: Contiene la clase de configuración que permite acceder a las credenciales de Twitter y la configuración de la base de datos.</simpara>
</listitem>
<listitem>
<simpara><literal>com.bonobo.streaming.db</literal>: Contiene la clase encargada de serializar los modelos a la base de datos.</simpara>
</listitem>
<listitem>
<simpara><literal>com.bonobo.streaming.model</literal>: Contiene el modelo de la aplicación: <literal>Tweet</literal>.</simpara>
</listitem>
</itemizedlist>
<figure xml:id="streaming-system-diagram">
<title>Diagrama de clases del servicio externo de Streaming</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/design/streaming-system.png" align="center"/>
</imageobject>
<textobject><phrase>streaming system</phrase></textobject>
</mediaobject>
</figure>
<simpara>En cuanto al diseño de la aplicación web, el primer paso será comentar el paquete encargado de gestionar los modelos que contienen la lógica de negocio de la aplicación. En <xref linkend="models-package-diagram"/> se muestran las relaciones entre todos los modelos, pudiendo observar como la clase <literal>Project</literal> ocupa una posición central en el diseño, manteniendo una relación <emphasis role="strong">1..</emphasis>* con la clase <literal>User</literal>, lo que indica que todos los proyectos deben pertenecer a un único usuario, pero un usuario puede disponer de múltiples proyectos.</simpara>
<simpara>Además, todos los proyectos tendrán una relación <emphasis role="strong">1..</emphasis>* con la clase <literal>BoundingBox</literal>, puesto que los proyectos vienen definidos por un conjunto de bounding boxes que definen tanto sus localizaciones globales o locales. La clase <literal>BoundingBox</literal> contiene la información relativa a las coordenadas del área que representan, así como una operación que será de gran utilidad (en base a la experiencia en el desarrollo de prototipos) para determinar cuando un par de coordenadas se encuentran contenidas en el bounding box actual.</simpara>
<simpara>La clase <literal>Execution</literal> también mantiene una relación con la clase <literal>Project</literal>, en la que se establece que todas las ejecuciones deben pertenecer a un único proyecto, pero este puede contener múltiples ejecuciones (debido al requisito para poder reevaluar un proyecto). <literal>Execution</literal>, tiene como cometido reflejar los resultados de la ejecución sobre un proyecto, manteniendo una referencia a sus resultados generados y una información general acerca de la ejecución.</simpara>
<simpara>La clase <literal>Tweet</literal> se utilizará como entidad para el intercambio de la información recogida a partir del servicio de Streaming. Contiene los atributos necesarios para trabajar con un tuit y realizar las operaciones necesarias.</simpara>
<simpara>Por último, <literal>Term</literal> es una representación de los términos que se extraerán más adelante en las tareas de evaluación. En un principio, es un modelo muy sencillo que únicamente sirve como abstracción sobre la cadena en plano que contiene el término extraído. Se decidió su diseño debido a que es una entidad propensa a gozar de un mayor número de responsabilidades en el futuro en caso de que aparezcan nuevos tipos de términos, o se deban realizar operaciones más complejas sobre ellos que una mera comparación de cadenas.</simpara>
<figure xml:id="models-package-diagram">
<title>Diagrama de clases del paquete de modelos</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/design/models-package.png" align="center"/>
</imageobject>
<textobject><phrase>models package</phrase></textobject>
</mediaobject>
</figure>
<simpara>El paquete <literal>evaluation</literal> recoge las clases implicadas en el proceso de generación de los algoritmos de inferencia así como la clase encargada de realizar las ejecuciones de los proyectos.</simpara>
<simpara><literal>ProjectDataProcessor</literal> se sitúa como la pieza central en el proceso de evaluación, se ejecutará de manera asíncrona y será la encargada de invocar al resto de clases encargadas de construir los modelos de clasificación. <literal>TermFrequencyExtractor</literal> será la clase encargada de extraer la frecuencia de apariciones de cada término para un conjunto de tuits definidos por un objeto <literal>QueryBuilder</literal>. Este tipo de objetos serán capaces de construir una sentencia SQL para recuperar tuits de la base de datos. El tipo de <emphasis>query</emphasis> que deberán de construir se basa en el tipo de proyecto a evaluar y las localizaciones sobre las que se quieren recuperar (<emphasis>bounding boxes</emphasis> locales o globales) tuits. La razón para diseñar esta clase es la aplicación del <emphasis role="strong">principio de responsabilidad única<footnote><simpara><link xlink:href="http://www.butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod">http://www.butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod</link></simpara></footnote></emphasis>, donde se establece que, idealmente, cada clase debe ser diseñada para ocuparse de una única responsabilidad.</simpara>
<simpara>En <literal>TermFrequencyExtractor</literal>, con el objetivo de no recuperar una cantidad de objetos en memoria demasiado elevada, se realizará un procesamiento por lotes, analizando de cada vez una cantidad <literal>n</literal> de tuits. Esta clase devolverá como resultado de la extracción un Hash que relacione una instancia de <literal>Term</literal> (que actúa como clave) con una frecuencia (representada por un número entero).</simpara>
<figure>
<title>Diagrama de clases del paquete de evaluación</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/design/evaluation-package.png" align="center"/>
</imageobject>
<textobject><phrase>evaluation package</phrase></textobject>
</mediaobject>
</figure>
<simpara>A partir de los mapas de término-frecuencia obtenidos por <literal>TermFrequencyExtractor</literal>, se hará uso de la clase <literal>LikelihoodRatioGenerator</literal> para calcular la puntuación de cada término utilizando el método estadístico Log Likelihood Ratio. El resultado de su ejecución, será un mapa que asocie cada término con su puntuación LLR obtenida (representada por un objeto <literal>Double</literal>).</simpara>
<simpara>En <literal>TweetsScoreGenerator</literal>, se hará uso de las puntuaciones obtenidas por el proceso anterior, y se utilizará un nuevo objeto <literal>QueryBuilder</literal> que permita recuperar tuits para las localizaciones globales y locales del proyecto con el objetivo de calcular la puntuación de cada tuit en función de los términos que contiene. Para extraer los términos de cada tuit, se utilizará un objeto <literal>ExtractionFilter</literal>, el cual se implementará utilizando un patrón de diseño <emphasis role="strong">Decorator</emphasis> que permita reflejar el mismo diseño explicado en <xref linkend="_sistema_de_filtros"/>. La puntuación de cada tuit, se almacenará como un nuevo objeto en la base de datos (<literal>TweetScoreContainer</literal>) que mantenga una referencia al identificador del tuit analizado, así como una puntuación representada por un objeto <literal>Double</literal> y una referencia al proyecto sobre el que se está realizando la evaluación.</simpara>
<simpara>Las últimas clases pertenecientes al paquete de evaluación son: <literal>ProjectExecutionService</literal>, <literal>ProjectExecutionResultContainer</literal> y la jerarquía de clases encargadas de tareas de exportación. La primera, se trata de una clase que se ejecutará de manera asíncrona e implementará la lógica necesaria para realizar la ejecución de un proyecto en base a las evaluaciones realizadas en los pasos anteriores. Así pues, a partir de un objeto <literal>Project</literal> y utilizando el paquete de <literal>streaming</literal>, realizará la evaluación de cada tuit recibido sobre el modelo de clasificación vinculado al proyecto e implementado como parte del paquete de <literal>learning</literal>.</simpara>
<simpara>Cada tuit positivamente evaluado se almacenará, junto con su <literal>Execution</literal> asociada, en el objeto <literal>ProjectExecutionResultContainer</literal>. Los objetos <literal>Execution</literal> serán instanciados una vez comenzado el proceso de ejecución en <literal>ProjectExecutionService</literal> y se irán actualizando con los datos de la ejecución en curso.</simpara>
<simpara>La jerarquía de clases encargadas de la exportación, haría uso de los objetos <literal>ProjectExecutionResultContainer</literal> relacionados con el objeto <literal>Execution</literal> recibido como parámetro a la hora de crear una nueva instancia del exportador. En función del tipo de exportación que se quiera realizar se deberá utilizar una u otra de las subclases que implementan el <literal>trait</literal> de <literal>ExecutionResultExporter</literal>. El resultado de invocar el método <literal>export</literal> será la generación de un fichero JSON o XML que contenga los objetos <literal>Tweet</literal> relacionados a cada <literal>ProjectExecutionResultContainer</literal> serializados de acuerdo al formato seleccionado. El método retornará el <emphasis>path</emphasis> hacia el fichero generado para facilitar su descarga.</simpara>
<simpara>En el paquete de <literal>learning</literal> se agrupan las clases que tienen como misión comunicarse con el software de aprendizaje automático y recoger toda la lógica referente a la creación y evaluación de nuevos modelos. La clase <literal>MachineLearningManager</literal> sirve como interfaz pública y como la fachada a utilizar por el resto de la aplicación, relegando la implementación de cada método sobre la propia clase <literal>...Manager</literal> de cada software de aprendizaje automático que se pueda utilizar como parte del sistema. Los métodos recogidos por esta clase son:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>createModel</literal>: creará un modelo de evaluación para el proyecto pasado como parámetro. La clase <literal>MachineLearningModel</literal> representa la entidad de un modelo de aprendizaje automático y únicamente contiene un <literal>id</literal> que permita identificar al modelo y el proyecto al que pertenece. Su misión es crear una entidad común para representar los modelos de cualquier software de aprendizaje automático.</simpara>
</listitem>
<listitem>
<simpara><literal>addTrainingData</literal>: añadirá la un nuevo ejemplo al modelo de aprendizaje automático.</simpara>
</listitem>
<listitem>
<simpara><literal>evaluate</literal>: evaluará el tuit recibido sobre el modelo asociado al proyecto que se pasa como parámetro. Se devolverá una instancia de <literal>MachineLearningPrediction</literal> que utilizará la propiedad <literal>isPositive</literal> para indicar si el tuit pertenece o no al área de estudio.</simpara>
</listitem>
</itemizedlist>
<simpara>En el caso de <literal>VowpalWabbitManager</literal> se deberá de realizar una comunicación con el servicio web RESTful que albergará el ejecutable de Vowpal Wabbit.</simpara>
<figure>
<title>Diagrama de clases del paquete de aprendizaje automático</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/design/learning-package.png" align="center"/>
</imageobject>
<textobject><phrase>learning package</phrase></textobject>
</mediaobject>
</figure>
<simpara>El paquete de <literal>streaming</literal> contiene las clases que esperan comunicarse con el sistema de cola de mensajes utilizado para transferir los tuits del sistema externo a la aplicación web. Además, su segunda responsabilidad es proveer un mecanismo que permita hacer accesibles esos mismos tuits al resto de clases de la aplicación. Para ello, el sistema propuesto se basa en una implementación del patrón de diseño <emphasis role="strong">Observer</emphasis> muy similar a lo propuesto también en el servicio externo de recolección de tuits.</simpara>
<simpara>En este caso, la clase <literal>TwitterStreamingSubscriber</literal> se ejecutará de manera asíncrona y estará suscrita a la cola de mensajes esperando recibir nuevos mensajes. Cada vez que recibe un nuevo mensaje, recorrerá la lista de listeners que tenga suscritos e invocará al método <literal>onTweetReceived</literal> con el nuevo tuit recibido. Todos los listeners de la clase <literal>TwitterStreamingSubscriber</literal> deberán implementar el <literal>trait</literal>: <literal>TwitterStreamingListener</literal>. En este caso, un ejemplo podría ser la clase <literal>ProjectExecutionListener</literal>, que se podría utilizar a la hora de ejecutar un nuevo proyecto, y en donde su método <literal>onTweetReceived</literal> realizaría la comunicación con el software de aprendizaje automático que evaluaría el nuevo tuit recibido en directo.</simpara>
<figure>
<title>Diagrama de clases del paquete de streaming</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/design/streaming-package.png" align="center"/>
</imageobject>
<textobject><phrase>streaming package</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="_diagrama_de_despliegue">
<title>Diagrama de despliegue</title>
<simpara>El diagrama de despliegue en <xref linkend="deployment-diagram"/> muestra la propuesta para desplegar el sistema presentado en los diagramas previos en un conjunto de servidores alojados, principalmente, en un proveedor de servicios Cloud. Puesto que lo habitual en este tipo de servicios es ofrecer unidades de procesamiento individuales<footnote><simpara>Denominados <emphasis>Dynos</emphasis> en Heroku (<link xlink:href="https://www.heroku.com/features">https://www.heroku.com/features</link>) o <emphasis>Gears</emphasis> en OpenShift (<link xlink:href="https://www.openshift.com/products/pricing">https://www.openshift.com/products/pricing</link>)</simpara></footnote> (de diversos tamaños y características) se ha intentado representar cada pieza independiente del sistema en un servidor que se instalaría en cada una de las unidades de procesamiento adquiridas.</simpara>
<figure xml:id="deployment-diagram">
<title>Diagrama de despliegue</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/design/deployment.png" align="center"/>
</imageobject>
<textobject><phrase>deployment</phrase></textobject>
</mediaobject>
</figure>
<simpara>El servidor web <emphasis role="strong">Apache</emphasis> serviría a su vez como balanceador de carga sobre el conjunto de aplicaciones web que podrían estar instaladas en varios servidores independientes. El proceso para configurar Apache como servidor <emphasis>front end</emphasis> de la aplicación podría seguir los pasos propuestos en la propia documentación del framework Play!: <link xlink:href="http://www.playframework.com/documentation/2.4.x/HTTPServer">http://www.playframework.com/documentation/2.4.x/HTTPServer</link>.</simpara>
</section>
<section xml:id="_desarrollo_dirigido_por_pruebas">
<title>Desarrollo dirigido por pruebas</title>
<simpara>Aunque el objetivo de este capítulo es mostrar un diseño capaz de ofrecer la posibilidad de aplicar la tecnología estudiada en los capítulos anteriores dentro de la estructura de una aplicación web <emphasis>convencional</emphasis>, a continuación se muestra una reflexión acerca del proceso para desarrollar un sistema basado en el diseño anterior de manera que presente el mayor grado de fiabilidad.</simpara>
<simpara>Existe mucha literatura acerca de los beneficios de realizar desarrollo dirigido por pruebas (conocido como <emphasis role="strong">Test Driven Development<footnote><simpara><link xlink:href="http://martinfowler.com/bliki/TestDrivenDevelopment.html">http://martinfowler.com/bliki/TestDrivenDevelopment.html</link></simpara></footnote></emphasis> en inglés). Principalmente, es un proceso que permite una mejor compresión del sistema que se va a implementar, puesto que es necesario conocer los requisitos en profundidad para poder establecer las pruebas antes de comenzar la implementación <emphasis>per se</emphasis>. Además, permite añadir un mayor número de capas de seguridad que facilitarán el desarrollo de un código más robusto (y, por supuesto, aporta el único mecanismo posible para realizar labores de <emphasis role="strong">Refactoring</emphasis> sin riesgo de introducir nuevos <emphasis>bugs</emphasis> en el sistema).</simpara>
<simpara>A la hora de desarrollar la aplicación se recomendaría encarecidamente el uso de esta metodología, acompañada del framework <emphasis role="strong">ScalaTest<footnote><simpara><link xlink:href="http://www.scalatest.org/">http://www.scalatest.org/</link></simpara></footnote></emphasis>, para ejecutar pruebas unitarias de todos los modelos y clases anteriormente descritas de manera automática.</simpara>
<simpara><?asciidoc-pagebreak?></simpara>
</section>
</section>
<section xml:id="_planificación_y_presupuesto">
<title>Planificación y presupuesto</title>
<simpara>Con el objetivo de aportar un estimación acerca del tiempo necesario para realizar un sistema basado en el diseño anterior, a continuación se ofrece una planificación y presupuesto basada en los siguientes supuestos:</simpara>
<itemizedlist>
<listitem>
<simpara>Debido a que el desarrollo de una aplicación web de estas características siempre estaría sujeta a una mejora continua del producto (para poder captar nuevos usuarios así como mantener a los más antiguos mediante la adición de nuevas funcionalidades), la siguiente planificación muestra los tiempos estimados para desarrollar únicamente los diseños mostrados en la sección anterior (lo que resultaría en el desarrollo del núcleo de la aplicación).</simpara>
</listitem>
<listitem>
<simpara>Para asegurar un escenario más real que el ofrecido en un Trabajo Fin de Máster, se supondrá que el desarrollo de la aplicación web será realizado por el equipo de I+D+i de una empresa del sector informático. El grupo estará formado por 3 Ingenieros Informáticos, en donde 2 de ellos serán los encargados de llevar el peso del desarrollo, mientras que el tercero hará las funciones de líder del proyecto y tareas de supervisión y diseño generales.</simpara>
<simpara>Los costes por hora para la empresa de los 3 ingenieros están recogidos en <xref linkend="workers-costs"/>.</simpara>
</listitem>
<listitem>
<simpara>Todos los participantes en el proyecto, habrán sido los encargados de realizar los experimentos y prototipos previos explicados en los puntos anteriores de este proyecto, por lo que se supone que todos ellos tienen un conocimiento en profundidad del problema y sus posibles soluciones.</simpara>
</listitem>
</itemizedlist>
<table xml:id="workers-costs" frame="all" rowsep="1" colsep="1">
<title>Costes para la empresa de cada trabajador</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="top">Trabajador</entry>
<entry align="left" valign="top">Coste</entry>
<entry align="left" valign="top">Años de experiencia</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Ingeniero Informático 1</simpara></entry>
<entry align="left" valign="top"><simpara>12 euros / hora</simpara></entry>
<entry align="left" valign="top"><simpara>5</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Ingeniero Informático 2</simpara></entry>
<entry align="left" valign="top"><simpara>12 euros / hora</simpara></entry>
<entry align="left" valign="top"><simpara>5</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Ingeniero Informático 3</simpara></entry>
<entry align="left" valign="top"><simpara>20 euros / hora</simpara></entry>
<entry align="left" valign="top"><simpara>10</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="_planificación">
<title>Planificación</title>
<simpara>En <xref linkend="tasks-table"/> se recogen el conjunto de tareas de desarrollo planificadas para realizar la implementación del sistema propuesto. Además, en <xref linkend="gantt"/> se muestra un diagrama de Gantt donde se puede observar la distribución temporal de las tareas, así como una expresión gráfica de su longitud.</simpara>
<simpara><?asciidoc-pagebreak?></simpara>
<table xml:id="tasks-table" frame="all" rowsep="1" colsep="1">
<title>Tareas propuestas para el desarrollo del sistema propuesto</title>
<tgroup cols="5">
<colspec colname="col_1" colwidth="7*"/>
<colspec colname="col_2" colwidth="30*"/>
<colspec colname="col_3" colwidth="15*"/>
<colspec colname="col_4" colwidth="15*"/>
<colspec colname="col_5" colwidth="30*"/>
<thead>
<row>
<entry align="left" valign="top">#</entry>
<entry align="left" valign="top">Tarea</entry>
<entry align="left" valign="top">Duración</entry>
<entry align="left" valign="top">Predecesora</entry>
<entry align="left" valign="top">Recursos</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>1</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Iteración 1</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">10,13 días</emphasis></simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>Preparación del entorno de desarrollo</simpara></entry>
<entry align="left" valign="top"><simpara>5 horas</simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>3</simpara></entry>
<entry align="left" valign="top"><simpara>Configuración y diseño de base de datos</simpara></entry>
<entry align="left" valign="top"><simpara>16 horas</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 3</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>4</simpara></entry>
<entry align="left" valign="top"><simpara>Comunicación con el servicio de Firehose</simpara></entry>
<entry align="left" valign="top"><simpara>5 horas</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>5</simpara></entry>
<entry align="left" valign="top"><simpara>Implementación del sistema de recolección</simpara></entry>
<entry align="left" valign="top"><simpara>16 horas</simpara></entry>
<entry align="left" valign="top"><simpara>4</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>6</simpara></entry>
<entry align="left" valign="top"><simpara>Integración con RabbitMQ</simpara></entry>
<entry align="left" valign="top"><simpara>5 horas</simpara></entry>
<entry align="left" valign="top"><simpara>5</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>7</simpara></entry>
<entry align="left" valign="top"><simpara>Creación del modelo de Proyecto</simpara></entry>
<entry align="left" valign="top"><simpara>2 horas</simpara></entry>
<entry align="left" valign="top"><simpara>6</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>8</simpara></entry>
<entry align="left" valign="top"><simpara>Implementación de TermFrequencyExtractor</simpara></entry>
<entry align="left" valign="top"><simpara>20 horas</simpara></entry>
<entry align="left" valign="top"><simpara>7</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>9</simpara></entry>
<entry align="left" valign="top"><simpara>Implementación de LikelihoodRatioGenerator</simpara></entry>
<entry align="left" valign="top"><simpara>20 horas</simpara></entry>
<entry align="left" valign="top"><simpara>7</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>10</simpara></entry>
<entry align="left" valign="top"><simpara>Implementación de TweetScoreCalculator</simpara></entry>
<entry align="left" valign="top"><simpara>20 horas</simpara></entry>
<entry align="left" valign="top"><simpara>8</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>11</simpara></entry>
<entry align="left" valign="top"><simpara>Configuración inicial de controladores, vistas y modelos</simpara></entry>
<entry align="left" valign="top"><simpara>5 horas</simpara></entry>
<entry align="left" valign="top"><simpara>9</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>12</simpara></entry>
<entry align="left" valign="top"><simpara>Versión inicial de pantalla de home</simpara></entry>
<entry align="left" valign="top"><simpara>8 horas</simpara></entry>
<entry align="left" valign="top"><simpara>11</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>13</simpara></entry>
<entry align="left" valign="top"><simpara>Retrospectiva y planificación de la siguiente iteración</simpara></entry>
<entry align="left" valign="top"><simpara>8 horas</simpara></entry>
<entry align="left" valign="top"><simpara>12;10</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2;Ing. Informático 3</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>14</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Iteración 2</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">10,5 días</emphasis></simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>15</simpara></entry>
<entry align="left" valign="top"><simpara>Creación del formulario de registro de usuarios</simpara></entry>
<entry align="left" valign="top"><simpara>8 horas</simpara></entry>
<entry align="left" valign="top"><simpara>13</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>16</simpara></entry>
<entry align="left" valign="top"><simpara>Creación del formulario de login</simpara></entry>
<entry align="left" valign="top"><simpara>8 horas</simpara></entry>
<entry align="left" valign="top"><simpara>15</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>17</simpara></entry>
<entry align="left" valign="top"><simpara>Lógica front-end para el dashboard de usuarios</simpara></entry>
<entry align="left" valign="top"><simpara>24 horas</simpara></entry>
<entry align="left" valign="top"><simpara>16</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>18</simpara></entry>
<entry align="left" valign="top"><simpara>Formularios para la creación de nuevo Proyecto</simpara></entry>
<entry align="left" valign="top"><simpara>16 horas</simpara></entry>
<entry align="left" valign="top"><simpara>17</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>19</simpara></entry>
<entry align="left" valign="top"><simpara>Instalación y configuración de Vowpal Wabbit</simpara></entry>
<entry align="left" valign="top"><simpara>4 horas</simpara></entry>
<entry align="left" valign="top"><simpara>17</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>20</simpara></entry>
<entry align="left" valign="top"><simpara>Desarrollo del wrapper RESTful sobre Vowpal Wabbit</simpara></entry>
<entry align="left" valign="top"><simpara>16 horas</simpara></entry>
<entry align="left" valign="top"><simpara>19</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>21</simpara></entry>
<entry align="left" valign="top"><simpara>Creación de la clase VowpalWabbitManager y comunicación con el sistema RESTful</simpara></entry>
<entry align="left" valign="top"><simpara>16 horas</simpara></entry>
<entry align="left" valign="top"><simpara>20</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>22</simpara></entry>
<entry align="left" valign="top"><simpara>Comunicación con RabbitMQ para obtener tuits</simpara></entry>
<entry align="left" valign="top"><simpara>16 horas</simpara></entry>
<entry align="left" valign="top"><simpara>18</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>23</simpara></entry>
<entry align="left" valign="top"><simpara>Implementación de TwitterStreamingService</simpara></entry>
<entry align="left" valign="top"><simpara>4 horas</simpara></entry>
<entry align="left" valign="top"><simpara>22</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>24</simpara></entry>
<entry align="left" valign="top"><simpara>Retrospectiva y planificación de la siguiente iteración</simpara></entry>
<entry align="left" valign="top"><simpara>8 horas</simpara></entry>
<entry align="left" valign="top"><simpara>23;20</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2;Ing. Informático 3</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>25</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Iteración 3</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">7 días</emphasis></simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>26</simpara></entry>
<entry align="left" valign="top"><simpara>Implementación de ProjectExecutionService y ProjectExecutionListener</simpara></entry>
<entry align="left" valign="top"><simpara>32 horas</simpara></entry>
<entry align="left" valign="top"><simpara>24</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2[50%]</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>27</simpara></entry>
<entry align="left" valign="top"><simpara>Implementar interfaz para la presentación de resultados de la ejecución</simpara></entry>
<entry align="left" valign="top"><simpara>24 horas</simpara></entry>
<entry align="left" valign="top"><simpara>24</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 2[50%]</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>28</simpara></entry>
<entry align="left" valign="top"><simpara>Lógica para presentar los resultados de la ejecución en formato JSON</simpara></entry>
<entry align="left" valign="top"><simpara>8 horas</simpara></entry>
<entry align="left" valign="top"><simpara>26;27</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>29</simpara></entry>
<entry align="left" valign="top"><simpara>Lógica para presentar los resultados de la ejecución en formato XML</simpara></entry>
<entry align="left" valign="top"><simpara>8 horas</simpara></entry>
<entry align="left" valign="top"><simpara>26;27</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>30</simpara></entry>
<entry align="left" valign="top"><simpara>Pruebas de aceptación</simpara></entry>
<entry align="left" valign="top"><simpara>16 horas</simpara></entry>
<entry align="left" valign="top"><simpara>29</simpara></entry>
<entry align="left" valign="top"><simpara>Ing. Informático 1;Ing. Informático 2;Ing. Informático 3</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>Para diseñar la planificación, se ha optado por una metodología basada en iteraciones donde se vayan completando historias de usuario independientes que puedan originar que en el fin de cada iteración el sistema se encuentre en un estado estable con la adición de nuevas características.</simpara>
<simpara>La planificación resultante, en base a los supuestos explicados al comienzo de esta sección, muestra un conjunto de tareas que abarca 27 días de desarrollo útiles, lo que acabaría resultando en, aproximadamente, mes y medio del calendario natural. Alcanzado ese periodo, el sistema desarrollado debería de cubrir la funcionalidad básica recogida a través de los diferentes diagramas del capítulo de Diseño.</simpara>
<simpara>En la planificación, se ha procurado paralelizar al máximo el conjunto de tareas para potenciar que los dos ingenieros encargados del desarrollo pudieran realizar el máximo número de acciones de manera concurrente. En el caso del <emphasis role="strong">Ingeniero Informático 3</emphasis>, sus actuaciones se han reducido a las tareas más afines a aspectos de gestión y dirección de proyectos (retrospectiva de cada iteración, planificación de la iteración siguiente, pruebas de aceptación) así como a una de las tareas de desarrollo que podría necesitar de sus 10 años de experiencia en el desarrollo de software (<emphasis>Configuración y diseño de base de datos</emphasis>).</simpara>
<figure xml:id="gantt">
<title>Diagrama de Gantt para la planificación del diseño propuesto</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/application/planning/gantt.png" contentdepth="650px" align="center"/>
</imageobject>
<textobject><phrase>gantt</phrase></textobject>
</mediaobject>
</figure>
<simpara>En <xref linkend="planning-overview"/> se recoge una visión general de la planificación, indicando también el coste total por parte de los trabajadores.</simpara>
<table xml:id="planning-overview" frame="all" rowsep="1" colsep="1">
<title>Estadísticas de la planificación del proyecto propuesta</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Fecha de inicio</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>lunes 30/06/2014</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Fecha de fin</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>miércoles 06/08/2014</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Duración</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>479 horas</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Coste total de los trabajadores</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>6.347,50 euros</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_presupuesto">
<title>Presupuesto</title>
<simpara>Además del costo por horas de los trabajadores, será necesario tener en cuenta las siguientes variables a la hora de preparar un presupuesto para realizar el proyecto:</simpara>
<variablelist>
<varlistentry>
<term>Firehose de Twitter</term>
<listitem>
<simpara>Uno de los requisitos principales del proyecto es la obtención de un flujo de datos constante sobre el streaming público de Twitter. Para ello, las opciones pasan por contratar a un distribuidor oficial de Twitter con permiso para realizar tareas de <emphasis>firehosing</emphasis> comerciales a partir de su streaming (Twitter únicamente aporta un 1% de su flujo de datos público a través de su API de Streaming y el acceso a su servicio de Firehose está restringido a un número muy reducido de clientes<footnote><simpara><link xlink:href="https://dev.twitter.com/discussions/2752">https://dev.twitter.com/discussions/2752</link></simpara></footnote>).</simpara>
<simpara>Los precios ofrecidos por este tipo de distribuidores son, aparentemente, bastante flexibles y ajustables a la cantidad de datos necesarios. Por ello, no ha sido posible obtener de manera oficial ninguna información acerca de los precios ofrecidos por los distribuidores más conocidos: <emphasis role="strong">Gnip<footnote><simpara><link xlink:href="http://gnip.com/">http://gnip.com/</link></simpara></footnote></emphasis> (adquirido por Twitter en Abril de 2014) y <emphasis role="strong">DataSift<footnote><simpara><link xlink:href="http://datasift.com/">http://datasift.com/</link></simpara></footnote></emphasis>. En el caso de Gnip, se ha podido obtener cierta información investigando acerca de su modelo de negocio por Internet:</simpara>
<blockquote>
<attribution>
Gnip Becomes Twitter's First Authorized Data Reseller - November 2010
<citetitle>http://allthingsd.com/20101117/gnip-becomes-twitters-first-authorized-data-reseller/</citetitle>
</attribution>
<simpara>Gnip will offer the Halfhose (50 percent of Tweets at a cost of <emphasis role="strong">$30,000</emphasis> per month), the Decahose (10 percent of Tweets for <emphasis role="strong">$5,000</emphasis> per month) and the Mentionhose (all mentions of a user including @replies and re-Tweets for <emphasis role="strong">$20,000</emphasis> per month). All feeds are available in original JSON and Activity Streams JSON formats.</simpara>
</blockquote>
<simpara>Para el presente proyecto, es posible que la opción del <emphasis role="strong">Decahose</emphasis> sea suficiente, por lo que se debería incluir un coste mensual de 5.000 $ (<emphasis role="strong">~3.668 €</emphasis>).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Tecnologías Open Source</term>
<listitem>
<simpara>Para el desarrollo del proyecto se ha propuesto un <emphasis>stack</emphasis> tecnológico basado en tecnologías Open Source las cuales no conllevan ningún coste adicional implícito.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Proveedor de infraestructura Cloud</term>
<listitem>
<simpara>Los precios para este tipo de servicios varían mucho en función del tipo de requisitos necesarios para desplegar la aplicación. En el caso de escoger el servicio ofrecido por <emphasis role="strong">Amazon Web Services</emphasis>, y viendo que, a priori y en función del Diagrama de Despliegue diseñado, harían falta un mínimo de 6 máquinas de una potencia moderada pero con una alta capacidad de almacenamiento (especialmente para el caso de la base de datos y el requisito de almacenar 48 horas de tuits), se podría estimar un coste mensual de:</simpara>
</listitem>
</varlistentry>
</variablelist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="28*"/>
<colspec colname="col_2" colwidth="14*"/>
<colspec colname="col_3" colwidth="57*"/>
<thead>
<row>
<entry align="left" valign="top">Nodo</entry>
<entry align="left" valign="top">Coste</entry>
<entry align="left" valign="top">Coste por hora de la instancia escogida</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Servidor de base de datos</simpara></entry>
<entry align="left" valign="top"><simpara>~450 €</simpara></entry>
<entry align="left" valign="top"><simpara>0.625 €</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Servidor de servicio de streaming</simpara></entry>
<entry align="left" valign="top"><simpara>~73 €</simpara></entry>
<entry align="left" valign="top"><simpara>0.102 €</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Servidor para la cola de mensajes</simpara></entry>
<entry align="left" valign="top"><simpara>~221 €</simpara></entry>
<entry align="left" valign="top"><simpara>0.308 €</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Servidores para Apache y dos instancias de la aplicación web</simpara></entry>
<entry align="left" valign="top"><simpara>~665 €</simpara></entry>
<entry align="left" valign="top"><simpara>0.308 €</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Total</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">1.409 €</emphasis></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>En <xref linkend="budget-costs"/> se recoge el total presupuestado por cada mes que la aplicación se encuentre desplegada, así como los costes mensuales de mantener a los trabajadores <emphasis role="strong">Ingeniero Informático 1</emphasis> e <emphasis role="strong">Ingeniero Informático 2</emphasis> realizando labores de desarrollo para añadir nuevas funcionalidades a la aplicación web.</simpara>
<table xml:id="budget-costs" frame="all" rowsep="1" colsep="1">
<title>Costes mensuales de mantenimiento y desarrollo de la aplicación web</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Firehose</simpara></entry>
<entry align="left" valign="top"><simpara>3.668 €</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Infraestructura Cloud</simpara></entry>
<entry align="left" valign="top"><simpara>1.409 €</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Costes por mano de obra</simpara></entry>
<entry align="left" valign="top"><simpara>3.840 €</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Total</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">8.917 €</emphasis></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
</chapter>
<chapter xml:id="_herramientas_utilizadas_durante_el_desarrollo_del_proyecto">
<title>Herramientas utilizadas durante el desarrollo del Proyecto</title>
<simpara>Además de las tecnologías utilizadas para el desarrollo de los prototipos y experimentos presentados en capítulos anteriores, el presente proyecto ha hecho uso de una serie de herramientas que facilitaron el desarrollo del mismo desde las fases iniciales hasta el desarrollo de la documentación final.</simpara>
<section xml:id="_trello">
<title>Trello</title>
<simpara><emphasis role="strong">Trello</emphasis> (<link xlink:href="https://trello.com/">https://trello.com/</link>) es una aplicación web desarrollada por <emphasis role="strong">Fog Creek</emphasis> con el objetivo de ofrecer una herramienta eficaz para la gestión de tareas (y en cierta medida, proyectos) de manera sencilla y muy colaborativa que potencie los procesos de desarrollos ágiles.</simpara>
<simpara>Trello basa su funcionamiento en establecer una serie de tablones en donde los diferentes usuarios de una organización o equipo puedan ir colocando tareas (en forma de tarjeta) en un conjunto de columnas totalmente personalizables. Su funcionamiento se basa en metodologías ágiles como <emphasis role="strong">Scrum<footnote><simpara><link xlink:href="https://www.scrum.org/resources/what-is-scrum/">https://www.scrum.org/resources/what-is-scrum/</link></simpara></footnote></emphasis>, donde el <emphasis>Scrum Board</emphasis> se utiliza para recoger todas las tareas <emphasis role="strong">Pendientes</emphasis>, <emphasis role="strong">En progreso</emphasis> o <emphasis role="strong">Terminadas</emphasis> para el <emphasis>sprint</emphasis> en curso.</simpara>
<simpara>En este caso se ha utilizado como herramienta para organizar y controlar el progreso de las tareas en las estaciones de experimentación y prototipado.</simpara>
</section>
<section xml:id="_omnigraffle_y_balsamiq_mockup">
<title>OmniGraffle y Balsamiq Mockup</title>
<simpara>Ambas herramientas se han utilizado a la hora de realizar la documentación del proyecto con la misión de realizar gráficos que permitan reflejar, en el caso de <emphasis role="strong">OmniGraffle</emphasis> los diagramas UML necesarios para explicar el análisis y diseño de la aplicación web propuesta y, en el caso de <emphasis role="strong">Balsamiq Mockup</emphasis> ejemplos de interfaces de usuario que se podrían implementar para facilitar la interacción del usuario con la aplicación web.</simpara>
</section>
<section xml:id="_desarrollo_de_la_documentación_utilizando_asciidoc">
<title>Desarrollo de la documentación utilizando AsciiDoc</title>
<simpara><emphasis role="strong">AsciiDoc</emphasis> (<link xlink:href="http://www.methods.co.nz/asciidoc/">http://www.methods.co.nz/asciidoc/</link>) ha sido el lenguaje de marcado seleccionado para realizar la documentación del proyecto, apoyado sobre el procesador <emphasis role="strong">Asciidoctor</emphasis> (<link xlink:href="http://asciidoctor.org/">http://asciidoctor.org/</link>).</simpara>
<simpara>La herramienta seleccionada para realizar la documentación debía de cumplir con las siguientes seis condiciones:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Trabajar en un formato de texto simple, preferiblemente texto plano, que lo hiciera independiente de la plataforma sobre la que realizar la documentación.</simpara>
</listitem>
<listitem>
<simpara>Trabajar con un lenguaje de marcado sencillo (similar a <emphasis role="strong">Markdown<footnote><simpara><link xlink:href="http://daringfireball.net/projects/markdown/">http://daringfireball.net/projects/markdown/</link></simpara></footnote></emphasis>) que simplificara el proceso de escritura ante alternativas como <emphasis role="strong">LaTeX</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Capacidad de dividir la documentación en varios ficheros que se pudiesen estructurar en base a directorios internos del proyecto de documentación.</simpara>
</listitem>
<listitem>
<simpara>Permitir el uso de un repositorio de código (como Bitbucket o Github) como sistema de control de versiones (utilizando para ello Mercurial o Git).</simpara>
</listitem>
<listitem>
<simpara>Ser Open Source, libre y gratuito.</simpara>
</listitem>
<listitem>
<simpara>Ser capaz de generar un documento en formato PDF con un resultado similar al de sistemas como LaTeX.</simpara>
</listitem>
</orderedlist>
<simpara>AsciiDoc, mediante la utilización de Asciidoctor y el procesador <emphasis role="strong">asciidoctor-fopub</emphasis> (<link xlink:href="https://github.com/asciidoctor/asciidoctor-fopub">https://github.com/asciidoctor/asciidoctor-fopub</link>) permitió generar de manera sencilla un pequeño proyecto basado en tecnologías Ruby que cumpliera con todos los requisitos anteriores.</simpara>
<simpara>A través de una serie de ficheros en texto plano escritos con AsciiDoc y respaldados por un repositorio en Bitbucket, se realizaba un compilado de los ficheros fuente utilizando Asciidoctor y <emphasis role="strong">Guard<footnote><simpara><link xlink:href="http://guardgem.org/">http://guardgem.org/</link> - gema de Ruby para detectar cambios en ficheros de un directorio</simpara></footnote></emphasis> para producir un documento <emphasis role="strong">DocBook<footnote><simpara><link xlink:href="http://www.docbook.org/whatis">http://www.docbook.org/whatis</link></simpara></footnote></emphasis> válido en XML que se pudiera transformar automáticamente en el PDF resultado utilizando asciidoctor-fopub.</simpara>
</section>
</chapter>
<chapter xml:id="_conclusiones_del_proyecto">
<title>Conclusiones del Proyecto</title>
<simpara>A título personal, considero que el resultado final del proyecto ha constituido una experiencia muy satisfactoria que me ha permitido trabajar en áreas en las que no había tenido la oportunidad de profundizar anteriormente.</simpara>
<simpara>La experiencia de trabajar en base a estudios y artículos de investigación, me ha permitido completar mi formación profesional mediante la adición de aspectos más cercanos a los campos de I+D+i los cuales también podrían suponer una oportunidad laboral en el futuro.</simpara>
<simpara>Además, el estudio de Twitter y la capacidad para procesar grandes volúmenes de datos mediante la aplicación de algoritmos más o menos complejos, ha resultado en una experiencia muy enriquecedora dentro del ámbito actual de la Ingeniería Informática e Ingeniería Web.</simpara>
<simpara>Todo lo anterior acompaña a los buenos resultados cosechados durante las etapas de experimentación y prototipado, donde se ha conseguido alcanzar el objetivo del proyecto al ser capaces de ofrecer un algoritmo capaz de inferir la localización de un usuario en base, únicamente, al contenido de sus publicaciones en Twitter. Además, se ha aprovechado la tecnología utilizada para ofrecer una posible solución a la hora de querer integrarla en una aplicación web capaz de utilizarla para vender un nuevo servicio de geolocalización de publicaciones en medios sociales.</simpara>
<simpara>Este último punto, también ha supuesto un reto debido, especialmente, a la complejidad de diseñar parte de una aplicación web sin haber tenido nunca la experiencia de trabajar en un entorno similar (tanto por las tecnologías propuestas como por lo complejo del problema a solucionar), lo que ha conllevado un estudio paralelo acerca de los diferentes <emphasis>frameworks</emphasis> existentes y conceptos como las colas de mensajes o programación asíncrona.</simpara>
<simpara>Por último, este proyecto me ha dado la oportunidad de trabajar con Scala y, por tanto, acercarme al paradigma de programación funcional que, sin duda, ha cambiado mi manera de desarrollar. Antes de esta experiencia, nunca me había planteado, por ejemplo, los conceptos funcionales que van tan intrínsecamente unidos a lenguajes populares como JavaScript o Ruby y que me ha aportado nuevas soluciones para resolver problemas comunes.</simpara>
<section xml:id="_ampliaciones_y_trabajo_futuro">
<title>Ampliaciones y trabajo futuro</title>
<simpara>Además de las ampliaciones propuestas en <xref linkend="_trabajo_futuro"/>, la ampliación más natural del proyecto sería realizar un prototipo de la aplicación web propuesta sobre la tecnología estudiada anteriormente y comprobar su viabilidad.</simpara>
<simpara>Dentro de la construcción de esta nueva aplicación web, habría que comenzar realizando el análisis y diseño de los aspectos que se han omitido en los capítulos anteriores de este proyecto (por entenderse que no se ajustaban al propósito del mismo), entre ellos podríamos encontrar:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Análisis y diseño de la base de datos</simpara>
</listitem>
<listitem>
<simpara>Diseño de interfaces de usuario</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Accesibilidad</simpara>
</listitem>
<listitem>
<simpara>Interacción del usuario</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Diseño de plan de negocio</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>División de usuarios por grupos</simpara>
</listitem>
<listitem>
<simpara>Creación de diferentes planes de precios con características de uso propias</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Diseño del plan de pruebas</simpara>
</listitem>
</orderedlist>
</section>
</chapter>
<appendix xml:id="_scala">
<title>Scala</title>
<simpara>El lenguaje de programación <emphasis role="strong">Scala</emphasis> (en su versión 2.11.0) fue la tecnología utilizada para desarrollar todos los prototipos y scripts que se realizaron como parte del ejercicio para poder inferir la localización de un usuario.</simpara>
<simpara>Scala (<link xlink:href="http://scala-lang.org/">http://scala-lang.org/</link>) es un lenguaje de programación orientado a objetos, fuertemente tipado y con un gran soporte para realizar programación funcional. Está construido sobre la Java Virtual Machine y por tanto compila al mismo <emphasis>bytecode</emphasis> que compilan el resto de lenguajes de la misma plataforma (Java, Groovy, etc.) permitiendo que la interoperabilidad entre bibliotecas escritas en estos lenguajes sea total.</simpara>
<simpara>Scala ha sido desarrollado con el objetivo de mejorar la experiencia de los programadores habituados a desarrollar en la JVM sobre Java, aportando características que permiten un desarrollo más potente mediante la inferencia de tipos dinámica, <emphasis>closures</emphasis>, <emphasis>pattern matching</emphasis>, sobrecarga de operadores y, por lo general, un código menos verboso.</simpara>
<simpara>Las principales razones de escoger Scala como el lenguaje de programación principal en el desarrollo de prototipos fueron:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Introducirme en los conceptos de la programación funcional con un lenguaje que también tiene soporte para un tipo de programación estructurada más tradicional.</simpara>
</listitem>
<listitem>
<simpara>Soporte nativo para trabajar con XML, tecnología sobre la que se basa gran parte del proyecto.</simpara>
</listitem>
<listitem>
<simpara>Compatibilidad total con bibliotecas desarrolladas en Java que hace que sea un complemento perfecto para mi preparación académica anterior.</simpara>
</listitem>
<listitem>
<simpara>Posibilidad de utilizarlo como un lenguaje de <emphasis>scripting</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="_sbt">
<title>sbt</title>
<simpara><emphasis role="strong">sbt</emphasis> (o <emphasis role="strong">Simple Build Tool</emphasis>) es una herramienta de gestión de dependencias y sistema de <emphasis>building</emphasis> (similar a <emphasis role="strong">Maven</emphasis> o <emphasis role="strong">Ant</emphasis>) muy popular en la comunidad de desarrolladores de Scala (aunque también se puede utilizar sobre Java).</simpara>
<simpara>sbt (<link xlink:href="http://www.scala-sbt.org/">http://www.scala-sbt.org/</link>) fue desarrollado por <emphasis role="strong">Typesafe Inc.</emphasis> con el objetivo de dotar a Scala de una herramienta sencilla para realizar una compilación incremental de los proyectos y ofrecer un <emphasis>shell</emphasis> interactivo sobre el que realizar ciertas acciones como: <literal>clean</literal>, <literal>compile</literal>, <literal>assembly</literal>, etc. Su uso está muy vinculado también a todos aquellos proyectos desarrollados sobre el framework de desarrollo web <emphasis role="strong">Play!</emphasis>.</simpara>
<simpara>Para varios de los sistemas desarrollados en este proyecto, se ha utilizado sbt para realizar tareas de gestión de dependencias y construcción de los ficheros <literal>jar</literal> que se utilizarían para su posterior ejecución (es decir, tareas de compilación y ensamblado).</simpara>
<formalpara>
<title>Ejemplo del fichero build.sbt para el sistema de análisis <emphasis role="strong">Puma</emphasis></title>
<para>
<screen>name := "Puma" <co xml:id="CO1-1"/>

version := "1.0" <co xml:id="CO1-2"/>

scalaVersion := "2.11.0" <co xml:id="CO1-3"/>

libraryDependencies ++= List(
  "org.scala-lang.modules" %% "scala-xml" % "1.0.1",
  "com.typesafe.scala-logging" %% "scala-logging-slf4j" % "2.1.2",
  "org.apache.logging.log4j" % "log4j-slf4j-impl" % "2.0-rc1",
  "com.twitter" % "twitter-text" % "1.6.1",
  "org.apache.logging.log4j" % "log4j-api" % "2.0-rc1",
  "org.apache.logging.log4j" % "log4j-core" % "2.0-rc1",
  "com.github.scopt" %% "scopt" % "3.2.0"
) <co xml:id="CO1-4"/>

scalacOptions ++= Seq("-unchecked", "-deprecation") <co xml:id="CO1-5"/></screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO1-1">
<para>Nombre del proyecto que se está desarrollando</para>
</callout>
<callout arearefs="CO1-2">
<para>Versión actual del proyecto</para>
</callout>
<callout arearefs="CO1-3">
<para>Versión de Scala que se desea utilizar para compilar el proyecto</para>
</callout>
<callout arearefs="CO1-4">
<para>Lista de dependencias que se desea que sean gestionadas por sbt</para>
</callout>
<callout arearefs="CO1-5">
<para>Opciones de compilación para el compilador de Scala</para>
</callout>
</calloutlist>
</section>
</appendix>
<appendix xml:id="_git">
<title>Git</title>
<simpara><emphasis role="strong">Git</emphasis> es un <emphasis role="strong">sistema de control de versiones distribuido</emphasis> desarrollado originalmente por Linus Torvalds para desarrollar el kernel de Linux en 2005. La diferencia con sistemas de control de versiones más tradicionales como SVN, es que en un proyecto con Git, cada desarrollador tiene una copia del proyecto entero en su máquina local, mientras que en SVN todo se basa en un repositorio central donde cada programador puede ir añadiendo o modificando ficheros.</simpara>
<simpara>Entre las características que hacen de Git un SCVD muy popular son: su velocidad, su capacidad para trabajar con diferentes ramas de manera sencilla, su eficiencia para trabajar con proyectos muy grandes (como queda demostrado en su uso por parte de Linux) y un buen soporte para realizar <emphasis>merges</emphasis> de manera sencilla (habitualmente, debido a la gran información que almacena cada nodo, Git es capaz de realizar los merges de manera automática).</simpara>
<simpara>En este proyecto, el uso de Git se une a los beneficios de poder trabajar con servicios de hosting como <emphasis role="strong">Github</emphasis> (<link xlink:href="http://www.github.com">http://www.github.com</link>), el cual permite tener un respaldo del código 100% asegurado. De la misma manera que aporta la flexibilidad necesaria para poder seguir trabajando en el proyecto desde diferentes máquinas y ubicaciones.</simpara>
<simpara>Los enlaces para consultar los diferentes sistemas y repositorios Git utilizados en este proyecto, hospedados en Github, son:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Falcon</emphasis> (<link xlink:href="https://github.com/sergio-alvarez/falcon">https://github.com/sergio-alvarez/falcon</link>), es el sistema encargado de recopilar tuits de la API de Streaming de Twitter.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Puma</emphasis> (<link xlink:href="https://github.com/sergio-alvarez/puma">https://github.com/sergio-alvarez/puma</link>), realiza los análisis de las colecciones de tuits recopilados por Falcon y calcula las puntuaciones LLR.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">vw-input-translator</emphasis> (<link xlink:href="https://github.com/sergio-alvarez/vw-input-translator">https://github.com/sergio-alvarez/vw-input-translator</link>), realiza las traducciones del fichero de salida de Puma, al fichero de entrada esperado por Vowpal Wabbit.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">tfm-utilities</emphasis> (<link xlink:href="https://github.com/sergio-alvarez/tfm-utilities">https://github.com/sergio-alvarez/tfm-utilities</link>), contiene varios <emphasis>scripts</emphasis> que han servido para realizar pequeñas tareas de refinamiento sobre los datos obtenidos.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">B2pick</emphasis> (<link xlink:href="https://github.com/sergio-alvarez/b2pick">https://github.com/sergio-alvarez/b2pick</link>), es una aplicación web desarrollada para poder seleccionar <emphasis>bounding boxes</emphasis> directamente sobre mapas de Google Maps.</simpara>
</listitem>
</itemizedlist>
</appendix>
<appendix xml:id="_software_de_aprendizaje_automático_y_vowpal_wabbit">
<title>Software de aprendizaje automático y Vowpal Wabbit</title>
<section xml:id="_software_de_aprendizaje_automático">
<title>Software de aprendizaje automático</title>
<simpara>Se denomina software de aprendizaje automático a aquellos sistemas que son capaces de <emphasis>aprender</emphasis> a identificar ciertos modelos a partir de un conjunto de características y métodos de entrenamiento.</simpara>
<simpara>Su uso es bastante habitual en los campos de investigación más actuales y en los ámbitos de la Inteligencia Artificial. Son muy útiles en una amplia colección de problemas de diferente índole. Por ejemplo, algunas de sus aplicaciones más importantes son:</simpara>
<itemizedlist>
<listitem>
<simpara>Análisis del mercado de valores.</simpara>
</listitem>
<listitem>
<simpara>Inteligencia Artificial en el mundo de los videojuegos.</simpara>
</listitem>
<listitem>
<simpara>Análisis de sentimientos.</simpara>
</listitem>
<listitem>
<simpara>Fraudes de tarjetas de crédito.</simpara>
</listitem>
<listitem>
<simpara>Diagnóstico médico.</simpara>
</listitem>
<listitem>
<simpara>Motores de búsqueda.</simpara>
</listitem>
</itemizedlist>
<simpara>El software de aprendizaje automático está siempre supeditado al algoritmo de aprendizaje que utilice por debajo. Es habitual que este tipo de sistemas puedan funcionar con diferentes tipos de algoritmos y permitan a los desarrolladores elegir cual se debe ejecutar para solucionar cada problema. El siguiente enlace contiene una explicación muy interesante acerca de los algoritmos más habituales que se pueden encontrar a la hora de trabajar con software de aprendizaje automático: <link xlink:href="http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/</link></simpara>
<simpara>En el siguiente apartado se explicará el software de aprendizaje automático utilizado en el presente proyecto: Vowpal Wabbit. A diferencia de la mayoría del software de aprendizaje automático disponible, Vowpal Wabbit utiliza un nuevo algoritmo de aprendizaje automático implementado por ellos mismos.</simpara>
</section>
<section xml:id="_vowpal_wabbit">
<title>Vowpal Wabbit</title>
<simpara><emphasis role="strong">Vowpal Wabbit<footnote><simpara><link xlink:href="http://hunch.net/~vw/">http://hunch.net/~vw/</link></simpara></footnote></emphasis> es un proyecto creado originalmente por <emphasis>Yahoo! Research</emphasis> que fue posteriormente continuado por <emphasis>Microsoft Research</emphasis>, consistente en la creación de un nuevo algoritmo de aprendizaje automático rápido y escalable que permita trabajar con grandes cantidades de datos. A partir de una serie de información específicamente preparada para ser consumida por Vowpal Wabbit, este es capaz de crear un propio modelo de datos que sirva como base de entrenamiento para después, mediante la aplicación de diversos algoritmos, predecir un resultado en base al conocimiento adquirido en ejecuciones anteriores.</simpara>
<simpara>Además de la velocidad y precisión de los resultados que puede ofrecer Vowpal Wabbit, una de sus características más importantes es la capacidad de funcionar como un demonio del sistema e ir aprendiendo <emphasis>en caliente</emphasis> a través de nuevos modelos de datos. Esto, a diferencia de otros software de aprendizaje automático, permite que el sistema pueda adquirir un conocimiento incremental y ofrecer mejores resultados a medida que pasa el tiempo.</simpara>
<section xml:id="_normalización_de_datos">
<title>Normalización de datos</title>
<simpara>Para aprovechar toda la potencia y velocidad que ofrece Vowpal Wabbit, es necesario generar ficheros de entrada que estén estructurados de acuerdo a un formato optimizado para el clasificador. En este caso, Vowpal Wabbit espera datos de entrada estructurados de la siguiente manera:</simpara>
<screen>[Label] [Importance [Tag]]|Namespace Features |Namespace Features ... |Namespace Features <co xml:id="CO2-1"/></screen>
<calloutlist>
<callout arearefs="CO2-1">
<para><link xlink:href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format">https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format</link></para>
</callout>
</calloutlist>
<simpara>En el presente proyecto, esta estructura generaría datos de la siguiente manera:</simpara>
<screen>1 |Tweet @adrianzenb scl rainer wirth óscar amigos radio quillota
0 |Tweet @thomasuribe medellín colombia celebra día hombre
1 |Tweet @fvminajx @cursiperono mujer ruega punto</screen>
<simpara>Debido a que un primer momento se consideró la opción de trabajar con modelos multiclase, el sistema que realiza la traducción entre los ficheros de puntuación generados por el sistema Puma y los datos que espera recibir Vowpal Wabbit, debía de ser parametrizable para poder cubrir los siguientes dos casos:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Entrenar un modelo de clasificación binario donde sólo se indicara si un tuit pertenece o no a un conjunto de coordenadas (útil para modelos en los que se quiera conocer de manera binaria si una dato pertenece o no a una localización en concreto).</simpara>
</listitem>
<listitem>
<simpara>Entrenar un modelo de clasificación multiclase donde se agruparan las clases en torno a un número de decimales para la latitud y longitud. Esto haría que se considerasen de la misma clase todos los tuits cuyas coordenadas con 3 decimales sean las mismas, permitiendo obtener predicciones con un grado de precisión de 10 kilómetros (en caso de agrupar por 2 decimales, la precisión sería de 100 y con 1, de 1000 kilómetros).</simpara>
</listitem>
</orderedlist>
<simpara>Para ello, se creo un sistema que mediante una interfaz de línea de comandos pudiese ser configurable mediante los siguientes argumentos:</simpara>
<screen>vw-input-translator 1.0
Usage: vw-input-translator [options] &lt;file&gt;

  --inout
        Flag for creating an input file for binary classification
  -d &lt;value&gt; | --decimals &lt;value&gt;
        Creates an input file for multi-class classification. Each sample will have the selected number of decimals on latitude and longitude coordinates
  &lt;file&gt;
        Source file</screen>
</section>
<section xml:id="_división_de_datos_en_conjuntos_de_entrenamiento_y_test">
<title>División de datos en conjuntos de entrenamiento y test</title>
<simpara>Con el objetivo de entrenar al clasificador, se desarrolló un script capaz de, a partir de los datos de entrada que recibiría Vowpal Wabbit, dividir el conjunto en dos para dedicar una parte al proceso de entrenamiento y otra a probar el modelo de datos generados.</simpara>
<simpara>Con el objetivo de generar dos conjuntos consistentes, la división se realizó en base a los usuarios, haciendo que un mismo usuario no pudiese formar parte de ambos grupos. Un 80% de los usuarios sería destinado al grupo de entrenamiento, mientras que el 20% restante iría a parar al conjunto de test.</simpara>
</section>
</section>
</appendix>
<appendix xml:id="_falcon_sistema_para_coleccionar_datos_de_la_api_streaming_de_twitter">
<title>Falcon, sistema para coleccionar datos de la API Streaming de Twitter</title>
<simpara>Para crear los diferentes datasets que fueron necesarios para desarrollar los prototipos y modelos de datos, se diseñó un sistema capaz de conectarse a la <emphasis role="strong">API Streaming de Twitter</emphasis> (<link xlink:href="https://dev.twitter.com/docs/api/streaming">https://dev.twitter.com/docs/api/streaming</link>) para descargar y almacenar tuits en tiempo real.</simpara>
<simpara>Este sistema debía ser parametrizable, con el objetivo de poder configurar en cada ejecución el tipo de tuits que se querían obtener de acuerdo a los filtros disponibles a través de la API de Twitter:</simpara>
<itemizedlist>
<listitem>
<simpara>Filtro por idioma del tuit</simpara>
</listitem>
<listitem>
<simpara>Filtro por localización del tuit (mediante el uso de <emphasis>bounding boxes</emphasis>)</simpara>
</listitem>
</itemizedlist>
<simpara>Para realizar la conexión entre el sistem Falcon y la API de Twitter se utilizó la biblioteca <emphasis role="strong">Twitter4j</emphasis> (<link xlink:href="http://twitter4j.org/en/index.html">http://twitter4j.org/en/index.html</link>). La ventajas de utilizar una biblioteca construida sobre la API original es que algunos de los problemas más habituales se solucionan a través de nuevas capas de abstracción:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Autenticación OAuth2 simplificada mediante clases propias de la biblioteca</simpara>
</listitem>
<listitem>
<simpara>Simplificación del proceso para poder utilizar la API de Streaming, aislando al desarrollador de la complejidad para mantener activa la conexión con el servidor de Twitter.</simpara>
</listitem>
</orderedlist>
<figure>
<title>Comunicación entre un cliente y la API Streaming de Twitter<footnote><simpara><link xlink:href="https://dev.twitter.com/docs/api/streaming">https://dev.twitter.com/docs/api/streaming</link></simpara></footnote></title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/twitter-streaming-api.png" align="center"/>
</imageobject>
<textobject><phrase>Modelo de comunicación entre un cliente y la API Streaming de Twitter</phrase></textobject>
</mediaobject>
</figure>
<section xml:id="_almacenamiento_de_datos">
<title>Almacenamiento de datos</title>
<simpara>Uno de los puntos más importantes que planteó el sistema para recolectar tuits era en qué formato sería más adecuado serializar los datos obtenidos.</simpara>
<simpara>En un primer momento se barajó la posibilidad de utilizar el formato CSV, el cual permitiría acceder de manera rápida al número de tuits guardados y realizar operaciones sencillas en línea de comandos mediante operaciones <literal>grep</literal>. Esta decisión fue descartada al realizar los primeros experimentos y comprobar como el guardado de ciertos datos en formato CSV presenta muchas dificultades para poder solventar todos los casos esquina que se presentan con la aparición de contenido complejo que pueda incluir comas, comillas y otros signos de puntuación (aún en el caso de utilizar bibliotecas especializadas como OpenCSV - <link xlink:href="http://opencsv.sourceforge.net/">http://opencsv.sourceforge.net/</link> ) combinados con caracteres extraños como Emoji (<link xlink:href="http://www.unicode.org/faq/emoji_dingbats.html">http://www.unicode.org/faq/emoji_dingbats.html</link>).</simpara>
<simpara>Como consecuencia de los resultados anteriores, y apoyado en el soporte nativo ofrecido por Scala, se utilizó XML como el lenguaje de estructuración de datos que mejor podría serializar la información obtenida a través de Twitter4j. El siguiente fragmento de código permite ver lo sencillo que es serializar un objeto en Scala a XML:</simpara>
<programlisting language="java" linenumbering="unnumbered">class Tweet(id:String, username: String, name:String, location: String, timezone: String, createdAt:String, latitude: String, longitude: String, text: String) {
  def toXML =
    &lt;tweet&gt;
      &lt;id&gt;
        {id}
      &lt;/id&gt;
      &lt;username&gt;
        {username}
      &lt;/username&gt;
      &lt;name&gt;
        {name}
      &lt;/name&gt;
      &lt;location&gt;
        {location}
      &lt;/location&gt;
      &lt;timezone&gt;
        {timezone}
      &lt;/timezone&gt;
      &lt;createdAt&gt;
        {createdAt}
      &lt;/createdAt&gt;
      &lt;latitude&gt;
        {latitude}
      &lt;/latitude&gt;
      &lt;longitude&gt;
        {longitude}
      &lt;/longitude&gt;
      &lt;text&gt;
        {text}
      &lt;/text&gt;
    &lt;/tweet&gt;
}</programlisting>
</section>
<section xml:id="_parámetros_del_sistema">
<title>Parámetros del sistema</title>
<simpara>Debido a que Falcon es un sistema sin ánimo de ejecutarse a través de una GUI, la manera de parametrizar la ejecución ha sido a través de una interfaz de línea de comandos. Para ello, se ha utilizado la biblioteca <emphasis role="strong">scopt</emphasis> (<link xlink:href="https://github.com/scopt/scopt">https://github.com/scopt/scopt</link>).</simpara>
<simpara>scopt permite parsear de manera sencilla los argumentos que se le pasan al programa en el momento de su ejecución. Para ello, simplemente hay que definir un objeto <literal>ScoptParser</literal> que contenga las reglas necesarias para especificar qué parámetros se esperan, qué tipo deben tener (<literal>String</literal>, <literal>Integer</literal>, <literal>Boolean</literal>, etc.) y si son requeridos u opcionales.</simpara>
<simpara>A continuación se muestra el mensaje de ayuda de la aplicación:</simpara>
<screen>Falcon 1.0
Usage: Falcon [options]

  -l &lt;value&gt; | --language &lt;value&gt;
        Specifies the language, in ISO 639-1 format, for the tweets to collect.
  -k &lt;value&gt; | --keywords &lt;value&gt;
        Specifies the file for the keywords.
  -t &lt;value&gt; | --time-in &lt;value&gt;
        The time measure for collecting tweets (SECONDS, MINUTES, HOURS, DAYS).
  -n &lt;value&gt; | --timestamp &lt;value&gt;
        Units of time for collecting tweets.
  -o &lt;value&gt; | --output &lt;value&gt;
        The output filename where store the collection results.
  -c &lt;value&gt; | --credentials &lt;value&gt;
        Properties file with the Twitter credentials
  -b &lt;value&gt; | --bounding-boxes &lt;value&gt;
        Specifies the file which contains the bounding boxes.</screen>
<itemizedlist>
<listitem>
<simpara><literal>--time-in</literal> y <literal>--timestamp</literal>: permiten establecer al recolector un tiempo de ejecución representado en diferentes magnitudes.</simpara>
</listitem>
<listitem>
<simpara><literal>--output</literal>: nombre del fichero de salida.</simpara>
</listitem>
<listitem>
<simpara><literal>--language</literal>: idioma en el que se desean obtener los tuits.</simpara>
</listitem>
<listitem>
<simpara><literal>--stopwords</literal>: debido a restricciones de Twitter, es necesario proveer una lista de términos cuando se intenta realizar un filtrado por idioma. Con el objetivo de restringir lo mínimo posible el número de tuits a obtener, se provee una lista de <emphasis>stop words</emphasis> del idioma por el que se esté filtrando.</simpara>
</listitem>
<listitem>
<simpara><literal>--bounding-boxes</literal>: fichero <literal>XML</literal> que contiene los <emphasis>bounding boxes</emphasis> sobre los que se realizará el filtrado.</simpara>
</listitem>
<listitem>
<simpara><literal>--credentials</literal>: indica el fichero que contiene las credenciales que se utilizarán para conectarse con la API de Twitter y utilizar su servicio de Streaming.</simpara>
</listitem>
</itemizedlist>
<simpara>Un ejemplo de uso para obtener datos en español sobre los bounding boxes de España durante un día, podría ser algo como esto:</simpara>
<screen>falcon.jar -l es -s es_stop_words.txt -t DAYS -n 1 -o es_tweets_collection -c credentials.properties -b spain_bounding_boxes.xml</screen>
</section>
<section xml:id="_ejemplo_de_resultados">
<title>Ejemplo de resultados</title>
<simpara>Un ejemplo de los resultados obtenidos por el recolector sería el siguiente:</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;tweet&gt;
  &lt;id&gt;
    469499350327255040
  &lt;/id&gt;
  &lt;username&gt;
    jessfhickey
  &lt;/username&gt;
  &lt;name&gt;
    Jessica Hickey
  &lt;/name&gt;
  &lt;location&gt;
    Ireland
  &lt;/location&gt;
  &lt;timezone&gt;
    London
  &lt;/timezone&gt;
  &lt;createdAt&gt;
    2014-05-22 15:25
  &lt;/createdAt&gt;
  &lt;latitude&gt;
    53.3444086
  &lt;/latitude&gt;
  &lt;longitude&gt;
    -6.2649497
  &lt;/longitude&gt;
  &lt;text&gt;
    In the most beautiful cafe ever. #boulevardcafe
  &lt;/text&gt;
&lt;/tweet&gt;</programlisting>
</section>
</appendix>
<appendix xml:id="_b2pick_aplicación_web_para_seleccionar_bounding_boxes">
<title>B2pick, aplicación web para seleccionar Bounding Boxes</title>
<simpara>Uno de los procesos más habituales durante el desarrollo del proyecto fue la selección de <emphasis>bounding boxes</emphasis> para aplicar filtros sobre el Streaming de Twitter. Un <emphasis>bounding box</emphasis>, se puede definir como un rectángulo que abarca un área geográfica en concreto que queda definida por sus coordenadas suroeste y noreste.</simpara>
<simpara><emphasis role="strong">B2pick</emphasis> (<link xlink:href="https://github.com/sergio-alvarez/b2pick">https://github.com/sergio-alvarez/b2pick</link>), viene a cubrir la necesidad de poder automatizar el proceso mediante una sencilla aplicación web escrita puramente en JavaScript en la capa de cliente, que mediante la utilización de la API de Google Maps permita dibujar al usuario rectángulos sobre las zonas geográficas que desee.</simpara>
<simpara>Actualmente, no se conoce ninguna herramienta que ofrezca un servicio similar.</simpara>
<simpara>Una vez el usuario ha seleccionado los <emphasis>bounding boxes</emphasis> que desea, B2pick ofrece la posibilidad de descargarlos en formato XML (que es el formato que se ha utilizado a la hora de desarrollar los prototipos), siendo posible en el futuro adaptar fácilmente el formato de salida a otros también populares como JSON o CSV.</simpara>
<formalpara>
<title>Ejemplo de la serialización en XML de un conjunto de <emphasis>bounding boxes</emphasis></title>
<para>
<programlisting language="xml" linenumbering="unnumbered">&lt;boundingBoxes&gt;
  &lt;boundingBox&gt;
    &lt;sw&gt;
      &lt;latitude&gt;35.75&lt;/latitude&gt;
      &lt;longitude&gt;-12.04&lt;/longitude&gt;
    &lt;/sw&gt;
    &lt;ne&gt;
      &lt;latitude&gt;44.53&lt;/latitude&gt;
      &lt;longitude&gt;4.22&lt;/longitude&gt;
    &lt;/ne&gt;
  &lt;/boundingBox&gt;
  &lt;boundingBox&gt;
    &lt;sw&gt;
      &lt;latitude&gt;50.23&lt;/latitude&gt;
      &lt;longitude&gt;-13.89&lt;/longitude&gt;
    &lt;/sw&gt;
    &lt;ne&gt;
      &lt;latitude&gt;59.27&lt;/latitude&gt;
      &lt;longitude&gt;1.85&lt;/longitude&gt;
    &lt;/ne&gt;
  &lt;/boundingBox&gt;
  &lt;boundingBox&gt;
    &lt;sw&gt;
      &lt;latitude&gt;54.72&lt;/latitude&gt;
      &lt;longitude&gt;6.24&lt;/longitude&gt;
    &lt;/sw&gt;
    &lt;ne&gt;
      &lt;latitude&gt;57.61&lt;/latitude&gt;
      &lt;longitude&gt;12.83&lt;/longitude&gt;
    &lt;/ne&gt;
  &lt;/boundingBox&gt;
&lt;/boundingBoxes&gt;</programlisting>
</para>
</formalpara>
<figure>
<title>Captura de la pantalla principal de B2pick</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/b2pick-main-screen.png" align="center"/>
</imageobject>
<textobject><phrase>Imagen de la pantalla principal de b2pick</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Ejemplo del lightbox de resultados con los <emphasis>bounding boxes</emphasis> seleccionados</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/b2pick-lightbox.png" align="center"/>
</imageobject>
<textobject><phrase>Ejemplo del lightbox de salida con los bounding boxes seleccionados</phrase></textobject>
</mediaobject>
</figure>
</appendix>
<appendix xml:id="_tweetheat_mapas_de_calor_sobre_ficheros_tsv_de_puntuación">
<title>TweetHeat, mapas de calor sobre ficheros TSV de puntuación</title>
<simpara><emphasis role="strong">TweetHeat</emphasis> (<link xlink:href="http://sergioalvarezsuarez.com/utilities/tweetheat/">http://sergioalvarezsuarez.com/utilities/tweetheat/</link>) es una aplicación web desarrollada con el objetivo de poder visualizar de manera rápida y sencilla cuándo un conjunto de puntuaciones para una determinada localización en formato TSV están dentro de un área específica.</simpara>
<simpara>En el presente proyecto, esta aplicación fue realmente útil en el proceso de refinamiento de datos, en donde se quería entrenar al clasificador con datos 100% válidos, eliminando aquellos tuits con puntuación positiva que se encontrasen fuera del área local y viceversa.</simpara>
<simpara>Para su desarrollo se utilizaron únicamente tecnologías front-end como HTML5, CSS3 y JavaScript, con algunas de las nuevas características como la API de JavaScript para la gestión de ficheros y la API de <emphasis>drag&amp;drop</emphasis> para HTML5.</simpara>
<simpara>El funcionamiento de la aplicación es realmente sencillo, a partir de un fichero TSV con el siguiente formato:</simpara>
<screen>[puntuacion_llr]  latitud  longitud  contenido_del_tuit</screen>
<simpara>Se debe arrastrar sobre la zona de <emphasis>drag&amp;drop</emphasis> superior y automáticamente la aplicación mostrará, utilizando un mapa de calor y la API de Google Maps, las zonas cubiertas por los tuits contenidos en dicho fichero.</simpara>
<figure>
<title>Pantalla de inicio de la aplicación TweetHeat</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/tweetheat-home.png" align="center"/>
</imageobject>
<textobject><phrase>Pantalla de inicio de la aplicación TweetHeat</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Resultados tras cargar un fichero TSV con puntuaciones para tuits localizados en Asturias</title>
<mediaobject>
<imageobject>
<imagedata fileref="../assets/appendixes/tweetheat-asturias-results.png" align="center"/>
</imageobject>
<textobject><phrase>Resultados tras soltar un fichero TSV con puntuaciones para tuits localizados en Asturias</phrase></textobject>
</mediaobject>
</figure>
</appendix>
<appendix xml:id="_código_fuente_de_los_prototipos_y_emphasis_scripts_emphasis_desarrollados">
<title>Código fuente de los prototipos y <emphasis>scripts</emphasis> desarrollados</title>
<section xml:id="_falcon">
<title>Falcon</title>
<formalpara>
<title>Main.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.falcon.main

import org.falcon.streaming.Collector
import org.falcon.util.Util

/**
 * Project: falcon
 * Package: org.falcon.main
 *
 * Author: Sergio Álvarez
 * Date: 09/2013
 */
object Main {
  def main(args: Array[String]): Unit = {
    CLIParser.parse(args, Configuration()) map { config =&gt; run(config) }
  }

  def run(config: Configuration) = {
    println("========================================")
    println("                 falcon                 ")
    println("========================================")
    println()

    Util.loadConfiguration(config)
    val collector = new Collector()
    collector.collect
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Configuration.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.falcon.main

import java.io.File

case class Configuration(language:String = "es", keywords:File = null, boundingBoxes:File = null, timeMeasure:String = "SECONDS",
    timeToCollect:Int = 10, output:String = "falcon_collection.xml", coordinatesMandatory:Boolean = false, credentials:File = null) {}

object CLIParser {
  private[this] val parser = new scopt.OptionParser[Configuration]("Falcon") {
    head("Falcon", "1.0")
    opt[String]('l', "language")
      .required() action { (x, c) =&gt; c.copy(language = x) } text("Specifies the language, in ISO 639-1 format, for the tweets to collect.")
    opt[File]('k', "keywords")
      .required() action { (x, c) =&gt; c.copy(keywords = x) } text("Specifies the file for the keywords.")
    opt[String]('t', "time-in")
      .required() action { (x, c) =&gt; c.copy(timeMeasure = x) } text("The time measure for collecting tweets (SECONDS, MINUTES, HOURS, DAYS).")
    opt[Int]('n', "timestamp")
      .required() action { (x, c) =&gt; c.copy(timeToCollect = x) } text("Units of time for collecting tweets.")
    opt[String]('o', "output")
      .required() action { (x, c) =&gt; c.copy(output = x) } text("The output filename where store the collection results.")
    opt[File]('c', "credentials")
      .required() action { (x, c) =&gt; c.copy(credentials = x) } text("Properties file with the Twitter credentials")
    opt[File]('b', "bounding-boxes")
      .required() action { (x, c) =&gt; c.copy(boundingBoxes = x) } text ("Specifies the file which contains the bounding boxes.")
  }

  def parse(args:Array[String], conf:Configuration):Option[Configuration] = this.parser.parse(args, conf)
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Tweet.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.falcon.model

/**
 * Project: falcon
 * Package: org.falcon.model
 *
 * Author: Sergio Álvarez
 * Date: 09/2013
 */
  class Tweet(id:String, username: String, name:String, location: String, timezone: String, createdAt:String, latitude: String,
              longitude: String, text: String) {
    def toXML =
      &lt;tweet&gt;
        &lt;id&gt;
          {id}
        &lt;/id&gt;
        &lt;username&gt;
          {username}
        &lt;/username&gt;
        &lt;name&gt;
          {name}
        &lt;/name&gt;
        &lt;location&gt;
          {location}
        &lt;/location&gt;
        &lt;timezone&gt;
          {timezone}
        &lt;/timezone&gt;
        &lt;createdAt&gt;
          {createdAt}
        &lt;/createdAt&gt;
        &lt;latitude&gt;
          {latitude}
        &lt;/latitude&gt;
        &lt;longitude&gt;
          {longitude}
        &lt;/longitude&gt;
        &lt;text&gt;
          {text}
        &lt;/text&gt;
      &lt;/tweet&gt;
  }</programlisting>
</para>
</formalpara>
<formalpara>
<title>FilterFactory.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.falcon.streaming.filter

import twitter4j.FilterQuery
import org.falcon.util.Util

/**
 * Project: falcon
 * Package: org.falcon.streaming.filter
 *
 * Author: Sergio Álvarez
 * Date: 02/2014
*/
object FilterFactory {
  def createFilterQuery: FilterQuery = new FilterQuery().language(Util.language).track(Util.keywords)
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Collector.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.falcon.streaming

import org.falcon.writer.Writer
import org.falcon.util.Util
import org.falcon.streaming.filter.FilterFactory

/**
 * Project: falcon
 * Package: org.falcon.streaming
 *
 * Author: Sergio Álvarez
 * Date: 02/2014
 */
class Collector {
  def collect() = {
    val twitterStreaming = new TwitterStreaming
    twitterStreaming.filter(FilterFactory.createFilterQuery)
    try{
      Writer.open(Util.fileName)
      Writer.write("&lt;tweets&gt;\n")
      twitterStreaming.run()

      val top = System.currentTimeMillis() + Util.timeToCollect
      while(System.currentTimeMillis() &lt;= top) {}

      twitterStreaming.close()
      Writer.write("&lt;/tweets&gt;")
      Writer.close()
    } finally {
      Writer.close()
      twitterStreaming.close()
    }

  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>TwitterStreaming.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.falcon.streaming

import twitter4j._
import org.falcon.model.Tweet
import org.falcon.writer.Writer
import org.falcon.util.Util
import java.util.{Date, TimeZone}
import java.text.SimpleDateFormat

/**
 * Project: falcon
 * Package: org.falcon.streaming
 *
 * Author: Sergio Álvarez
 * Date: 09/2013
 */
 class TwitterStreaming() {
  private[this] var _filter: FilterQuery = _
  private[this] var _twitter: TwitterStream = _

  def filter(filter: FilterQuery): Unit = _filter = filter

  def run() = {
    _twitter =
      new TwitterStreamFactory(Util.twitterConfiguration).getInstance()
    _twitter.addListener(myTwitterStatusListener)
    _twitter.filter(_filter)
  }

  def close() = if (_twitter != null) _twitter.shutdown()

  private def myTwitterStatusListener = new StatusListener {
    def onStatus(status: Status) {
      if(status.getGeoLocation == null) return;

      val id:String        = status.getId.toString
      val username:String  = status.getUser.getScreenName
      val name:String      = status.getUser.getName
      val location:String  = status.getUser.getLocation
      val timezone:String  = status.getUser.getTimeZone
      val createdAt:String = toUTC(status.getCreatedAt)
      val text:String      = status.getText
      val latitude:String  = status.getGeoLocation.getLatitude.toString
      val longitude:String = status.getGeoLocation.getLongitude.toString

      if(Util.isInBoundingBoxes(latitude, longitude)){
        val tweet = new Tweet(id, username, name, location, timezone, createdAt, latitude, longitude, text)
        Writer.write(s"\t${tweet.toXML.toString()}\n")
      }
    }

    def onStallWarning(p1: StallWarning) {}

    def onException(p1: Exception) {}

    def onDeletionNotice(p1: StatusDeletionNotice) {}

    def onScrubGeo(p1: Long, p2: Long) {}

    def onTrackLimitationNotice(p1: Int) {}

    private[this] def toUTC(date: Date): String = {
      val f = new SimpleDateFormat("yyyy-MM-dd HH:mm")
      f.setTimeZone(TimeZone.getTimeZone("UTC"))
      f.format(date)
    }
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Util.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.falcon.util

import java.util.Properties
import scala.io.Source
import scala.Array
import scala.collection.JavaConverters._
import scala.concurrent.duration._
import java.util.concurrent.TimeUnit
import scala.xml.XML
import scala.collection.mutable.ListBuffer

import org.falcon.main.Configuration

/**
 * Project: falcon
 * Package: org.falcon.util
 *
 * Author: Sergio Álvarez
 * Date: 09/2013
 */
object Util {
  private[this] val AccessTokenSecret = "access_token_secret"
  private[this] val AccessToken = "access_token"
  private[this] val ConsumerSecret = "consumer_secret"
  private[this] val ConsumerKey = "consumer_key"

  private[this] val _locations: Array[Array[Double]] = null
  private[this] var configuration: Configuration = null

  def loadConfiguration(config: Configuration): Unit =
    this.configuration = config

  def twitterConfiguration: twitter4j.conf.Configuration = {
    val props = new Properties()
    props
      .load(Source.fromFile(configuration.credentials)
      .bufferedReader())

    new twitter4j.conf.ConfigurationBuilder()
      .setOAuthConsumerKey(props.getProperty(ConsumerKey))
      .setOAuthConsumerSecret(props.getProperty(ConsumerSecret))
      .setOAuthAccessToken(props.getProperty(AccessToken))
      .setOAuthAccessTokenSecret(props.getProperty(AccessTokenSecret))
      .build
  }

  def keywords: Array[String] =
    Source.fromFile(configuration.keywords).getLines().toArray

  def locations: Array[Array[Double]] =
    if (_locations != null) _locations else getBoundingBoxes

  def language: Array[String] = Array(configuration.language)

  def timeToCollect: Long = {
    val timeMeasure = configuration.timeMeasure
    val timeToCollect = configuration.timeToCollect
    Duration(timeToCollect, TimeUnit.valueOf(timeMeasure)).toMillis
  }

  def fileName: String = configuration.output

  def areCoordinatesMandatory: Boolean =
    configuration.coordinatesMandatory

  def isInBoundingBoxes(latitude: String, longitude: String): Boolean = {
    val lat = latitude.toDouble
    val lng = longitude.toDouble

    Util.locations.grouped(2).exists(group =&gt; {
      val sw = group(0)
      val ne = group(1)
      sw(0) &lt;= lat &amp;&amp; ne(0) &gt;= lat &amp;&amp; sw(1) &lt;= lng &amp;&amp; ne(1) &gt;= lng
    })
  }

  private[this] def getBoundingBoxes: Array[Array[Double]] = {
    val root = XML.loadFile(configuration.boundingBoxes)
    var boundingBoxes = new ListBuffer[Array[Double]]
    (root \\ "boundingBox").foreach(b =&gt; {
      val sw_long = (b \ "sw" \ "longitude").text
      val sw_lat = (b \ "sw" \ "latitude").text
      val ne_long = (b \ "ne" \ "longitude").text
      val ne_lat = (b \ "ne" \ "latitude").text

      boundingBoxes += Array(sw_lat.toDouble, sw_long.toDouble)
      boundingBoxes += Array(ne_lat.toDouble, ne_long.toDouble)
    })
    boundingBoxes.toArray
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Writer.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.falcon.writer

import java.io._
import java.nio.charset.Charset

/**
 * Project: falcon
 * Package: org.falcon.writer
 *
 * Author: Sergio Álvarez
 * Date: 09/2013
 */
object Writer {
  private var writer: BufferedWriter = null

  def open(file: String) ={
    if (writer == null){
      val charset = Charset.forName("UTF-8").newEncoder()
      writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file, true), charset))
    }
  }

  def close() = {
    if (writer != null) { writer.close(); writer = null }
  }

  def write(string: String) = {
    if (writer != null) { writer.flush(); writer.write(string) }
  }
}</programlisting>
</para>
</formalpara>
<simpara><?asciidoc-pagebreak?></simpara>
</section>
<section xml:id="_puma_sistema_de_análisis">
<title>Puma (sistema de análisis)</title>
<formalpara>
<title>Main.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.main

import org.puma.generator.GeneratorFactory
import org.puma.configuration.{Configuration, ConfigurationUtil, CLIParser}

/**
 * Project: puma
 * Package: org.puma.main
 *
 * User: Sergio Álvarez
 * Date: 09/2013
 */
object Main {
  def main(args: Array[String]): Unit = CLIParser.parse(args, Configuration()) map { config =&gt; run(config) }

  private[this] def run(config: Configuration) {
    println("==============================================")
    println("                    puma                      ")
    println("==============================================")
    println()

    ConfigurationUtil.load(config)
    val generator = GeneratorFactory.get
    generator.generate()
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>GeneratorFactory.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.generator

import org.puma.configuration.ConfigurationUtil

/**
 * Project: puma
 * Package: org.puma.generator
 *
 * Author: Sergio Álvarez
 * Date: 03/2014
 */
object GeneratorFactory {
  def get:Generator = {
    val mode = ConfigurationUtil.getMode
    mode match {
      case "scoreGenerator" =&gt; new ScoreGenerator
      case "llrGenerator" =&gt; new LLRGenerator
      case _ =&gt; throw new IllegalStateException("Invalid mode")
    }
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Generator.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.generator

/**
 * Project: puma
 * Package: org.puma.generator
 *
 * Author: Sergio Álvarez
 * Date: 03/2014
 */
trait Generator {
  def generate(): Unit
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>LLRGenerator.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.generator

import org.puma.configuration.ConfigurationUtil
import org.puma.analyzer.Analyzer
import org.puma.analyzer.filter.ExtractorFilter
import java.io._
import java.util.Calendar
import java.text.SimpleDateFormat
import scala.Console

/**
 * Project: puma
 * Package: org.puma.generator
 *
 * Author: Sergio Álvarez
 * Date: 03/2014
 */
class LLRGenerator extends Generator {
  def generate(): Unit = {
    try{
      val files  = ConfigurationUtil.getFilesToAnalyze
      val local  = files(0)
      val global = files(1)

      ConfigurationUtil.getFiltersToApply.foreach(f =&gt; {
        val analyzer = new Analyzer(local, global, f)
        val mostValuedTerms = analyzer.analyze
        saveToFile(mostValuedTerms, f)
      })
    }catch {
      case ex: FileNotFoundException =&gt; Console.err.println("ERROR: The file does not exist. " + ex.getMessage)
    }
  }

  private[this] def saveToFile(terms:List[(List[String], Double)], filter: ExtractorFilter): Unit = {
    val title =
      new SimpleDateFormat("yyyyMMddHHmmss").format(Calendar.getInstance.getTime)
    val filterName = filter.getClass.getSimpleName

    val file = new File(s"${title}_$filterName.tsv")
    if(!file.exists) file.createNewFile

    val writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file)))
    try{
      terms.foreach(t =&gt; writer.write(s"${t._2}\t${t._1.mkString(" ")}\n"))
    }finally{
      writer.flush()
      writer.close()
    }
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>ScoreGenerator.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.generator

import org.puma.configuration.ConfigurationUtil
import org.puma.analyzer.filter._
import scala.collection.mutable
import scala.io.Source
import org.puma.analyzer.NgramExtractor
import java.io.{FileOutputStream, OutputStreamWriter, BufferedWriter, File}
import scala.xml.pull.{EvElemEnd, EvText, EvElemStart, XMLEventReader}
import scala.collection.mutable.ListBuffer
import scala.xml.XML


/**
 * Project: puma
 * Package: org.puma.generator
 *
 * Author: Sergio Álvarez
 * Date: 03/2014
 */
class ScoreGenerator extends Generator{

  private[this] val resultFiles   = ConfigurationUtil.getLLRResultFiles
  private[this] val fileToAnalyze = ConfigurationUtil.getFileToAnalyze
  private[this] val scoreByTerm   = getScores.withDefaultValue(0.0)
  private[this] val boundingBoxes = getBoundingBoxes

  def generate(): Unit = {
    val location, text, latitude, longitude, username = new mutable.StringBuilder()
    var insideLocation, insideText, insideLatitude, insideLongitude, insideUsername = false

    val reader = new XMLEventReader(Source.fromFile(fileToAnalyze))
    reader.foreach({
        case EvElemStart(_, "username", _, _)  =&gt; insideUsername = true
        case EvElemEnd(_, "username")          =&gt; insideUsername = false
        case EvElemStart(_, "location", _, _)  =&gt; insideLocation = true
        case EvElemEnd(_, "location")          =&gt; insideLocation = false
        case EvElemStart(_, "latitude", _, _)  =&gt; insideLatitude = true
        case EvElemEnd(_, "latitude")          =&gt; insideLatitude = false
        case EvElemStart(_, "longitude", _, _) =&gt; insideLongitude = true
        case EvElemEnd(_, "longitude")         =&gt; insideLongitude = false
        case EvElemStart(_, "text", _, _)      =&gt; insideText = true
        case EvElemEnd(_, "text")              =&gt; insideText = false
        case EvText(nodeText) =&gt;
        {
          if(insideLocation)
            location ++= nodeText
          else if(insideLatitude)
            latitude ++= nodeText
          else if(insideLongitude)
            longitude ++= nodeText
          else if(insideText)
            text ++= nodeText
          else if(insideUsername)
            username ++= nodeText
        }
        case EvElemEnd(_, "tweet") =&gt;
        {
          processXmlData(username.toString(), location.toString(), latitude.toString(), longitude.toString(), text.toString())
          location.clear()
          latitude.clear()
          longitude.clear()
          text.clear()
          username.clear()
        }
        case _ =&gt; ;
    })
  }

  private[this] def processXmlData(username: String, location: String, lat: String, lng: String, text: String): Unit = {
    val locationKeywords = new LocationFilter().extract(location)
    val mentionsHashtags = new MentionFilter(new HashtagFilter()).extract(text)
    val bigrams  = new BigramsFilter().extract(text)
    val keywords = new KeywordFilter().extract(text).filter(keyword =&gt; {
      bigrams.find(bigram =&gt; bigram.contains(keyword)) == None
    })

    val terms = locationKeywords ++ mentionsHashtags ++ bigrams ++ keywords

    val score = terms.foldLeft(0.0)((acc, current) =&gt; acc + scoreByTerm(current.mkString(" ")))
    if(isValidScore(score, lat, lng)){
      saveScore(score, username, location, lat, lng, text)
    }
  }

  private[this] def isValidScore(score:Double, latitude:String, longitude:String): Boolean = {
    if(score == 0.0) return false

    val mustBeLocal = score &gt; 0.0
    mustBeLocal == isInBoundingBoxes(longitude, latitude)
  }

  private[this] def saveScore(score:Double, username:String, location:String, latitude:String, longitude:String, text:String) = {
    val file = new File(s"${fileToAnalyze}_scores.tsv")
    if(!file.exists) file.createNewFile

    val writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file, true)))
    val line = score + "\t" + latitude.trim + "\t" + longitude.trim + "\t" + normalize(username, location, text) + "\n"
    try{
      writer.write(line)
    }finally{
      writer.flush()
      writer.close()
    }
  }

  private[this] def normalize(username:String, location:String, text:String): String = {
    val normalized = NgramExtractor.extract(location.trim + " " + text.trim(), 1, allowMentionsAndHashtags = true).flatten.mkString(" ")
    "@" + username.toLowerCase.trim + " " + normalized
  }

  private[this] def getScores: Map[String, Double] = {
    val scores = mutable.Map.empty[String, Double]
    resultFiles.foreach(file =&gt; {
      val lines = Source.fromFile(file).getLines().toArray
      lines.foreach(line =&gt; {
       val fields = line.split("\t")
       scores(fields(1)) = fields(0).toDouble
      })
    })
    scores.toMap
  }

  private[this] def getBoundingBoxes: List[List[(Double, Double)]] = {
    val root = XML.loadFile(ConfigurationUtil.getBoundingBoxesFile)
    var boundingBoxes = new ListBuffer[List[(Double, Double)]]
    (root \\ "boundingBox").foreach(b =&gt; {
      val sw_long = (b \ "sw" \ "longitude").text
      val sw_lat = (b \ "sw" \ "latitude").text
      val ne_long = (b \ "ne" \ "longitude").text
      val ne_lat = (b \ "ne" \ "latitude").text

      boundingBoxes += List((sw_lat.toDouble, sw_long.toDouble), (ne_lat.toDouble, ne_long.toDouble))
    })
    boundingBoxes.toList
  }

  private[this] def isInBoundingBoxes(lng: String, lat: String): Boolean = {
    boundingBoxes.exists(boundingBox =&gt; {
      val sw = boundingBox(0)
      val ne = boundingBox(1)
      sw._1 &lt;= lat.toDouble &amp;&amp; ne._1 &gt;= lat.toDouble &amp;&amp; sw._2 &lt;= lng.toDouble &amp;&amp; ne._2 &gt;= lng.toDouble
    })
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Extractor.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer

import scala.io.Source
import scala.xml.pull._
import scala.xml.pull.EvElemStart
import scala.xml.pull.EvText
import scala.collection.mutable
import org.puma.analyzer.filter.ExtractorFilter
import com.typesafe.scalalogging.slf4j.LazyLogging
import org.puma.configuration.ConfigurationUtil

/**
 * Project: puma
 * Package: org.puma.analyzer
 *
 * Author: Sergio Álvarez
 * Date: 09/2013
 */
class Extractor extends LazyLogging{
  private[this] var _path: String = null
  private[this] var _filter: ExtractorFilter = null

  private[this] var results = mutable.Map.empty[List[String], Int]
  private[this] var minimumFreq = 1
  private[this] val MaximumExtractedTerms = ConfigurationUtil.getMaximumExtractedTerms
  private[this] val FactorToRemove = ConfigurationUtil.getFactorToRemove

  def path(value: String): Extractor = {
    _path = value
    this
  }

  def filter(value: ExtractorFilter): Extractor = {
    _filter = value
    this
  }

  def extract: Map[List[String], Int] = {
    if(_filter == null || _path == null){
      throw new IllegalArgumentException("You must provide a filter and valid path for making the extraction")
    }

    logger.debug("Extracting: " + _path + " with filter: " + _filter.getClass.getSimpleName)

    val reader = new XMLEventReader(Source.fromFile(_path))
    var in = false
    reader.foreach({
        case e: EvElemStart if e.label == _filter.field =&gt; in = true
        case EvText(text) if in =&gt; applyFilter(text)
        case e: EvElemEnd if e.label == _filter.field =&gt; in = false
        case _ =&gt; ;
    })
    results.toMap
  }

  private[this] def applyFilter(tweet: String) = {
    checkMemoryStatus()
    _filter.extract(tweet).foreach(term =&gt; {
      if(results.contains(term)){
        results(term) += 1
      }else{
        results(term) = minimumFreq
      }
    })
  }

  private[this] def checkMemoryStatus() = {
    if(results.keys.size &gt;= MaximumExtractedTerms) {
      logger.debug("Memory overload. Maximum limit for extracted terms have been reached. Reducing map...")
      reduceMapLoad()
    }
  }

  private[this] def reduceMapLoad() = {
    val itemsToRemove = (results.keys.size * FactorToRemove).toInt
    logger.debug("They are going to be removed " + itemsToRemove + " items")

    val orderedList = results.toList.sortBy({_._2})
    minimumFreq = orderedList(itemsToRemove - 1)._2
    logger.debug("New minimum frequency is " + minimumFreq)

    val reduced = orderedList.slice(itemsToRemove - 1, orderedList.size)
    results = collection.mutable.Map(reduced.toMap[List[String], Int].toSeq: _*) // converting to mutable map
    logger.debug("Reduced map contains " + results.keys.size + " terms")
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>NgramExtractor.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer

import org.puma.configuration.ConfigurationUtil
import scala.collection.mutable.ListBuffer

/**
 * Project: puma
 * Package: org.puma.analyzer.filter
 *
 * Author: Sergio Álvarez
 * Date: 03/2014
 */
object NgramExtractor {
  private[this] val SymbolsToClean = Array('\\', ',', '(', '\'', ')', '{', '}', '?', '¿', '¡', '!', '.', '&amp;', '%',
    '$', ';', ':', '+', '-', '*', '^', '/', '_', '\n', '\t', '=', '|')

  def extract(tweet:String, count:Int, allowMentionsAndHashtags:Boolean = false): List[List[String]] = {
    var result = new ListBuffer[List[String]]
    val ngrams = getNgrams(clear(tweet), count)
    ngrams.foreach(ngram =&gt; {
      if (isValidNgram(ngram, count, allowMentionsAndHashtags)) {
        val termToAdd = ngram.map(ngram =&gt; ngram.trim).toList
        if(!result.contains(termToAdd)) result += termToAdd
      }
    })
    result.toList
  }

  private[this] def isValidNgram(ngram:Array[String], count:Int, allowMentionsAndHashtags:Boolean): Boolean = {
    ngram.size == count &amp;&amp;
    ngram.find(term =&gt; term.trim.isEmpty) == None &amp;&amp;
    ngram.find(term =&gt; term.trim.length == 1) == None &amp;&amp;
    isValidCharSequence(ngram, allowMentionsAndHashtags) &amp;&amp;
    !ngram.forall(term =&gt; if(count &gt; 1) term.trim == ngram(0).trim else false) &amp;&amp;
    !hasStopWords(ngram, allowMentionsAndHashtags)
  }

  private[this] def isValidCharSequence(ngram:Array[String], allowMentionsAndHashtags:Boolean): Boolean = {
    if(allowMentionsAndHashtags) {
      ngram.find(term =&gt; !term.trim.matches("^(@|#)?[a-záéíóú]*")) == None
    }else {
      ngram.find(term =&gt; !term.trim.matches("[a-záéíóú]*")) == None
    }
  }

  private[this] def hasStopWords(ngram:Array[String], allowMentionsAndHashtags:Boolean): Boolean = {
    ngram.exists(term =&gt; {
      if(allowMentionsAndHashtags &amp;&amp; (term.startsWith("@") || term.startsWith("#"))) {
        false
      }else {
        ConfigurationUtil.stopWords.contains(term.trim)
      }
    })
  }

  private[this] def clear(raw:String): String = {
    raw.toLowerCase.trim.map[Char, String](char =&gt; if(SymbolsToClean.contains(char)) 0 else char)
  }

  private[this] def getNgrams(tweet:String, count: Int): List[Array[String]] = {
    val words = tweet.split(" ")
    words.combinations(count).toList
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>ExtractorFilter.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer.filter


/**
 * Project: puma
 * Package: org.puma.analyzer
 *
 * Author: Sergio Álvarez
 * Date: 01/2014
 */
trait ExtractorFilter {
  def extract(tweet: String): List[List[String]]
  def field: String
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>SimpleTermExtractorFilter.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer.filter

/**
 * Project: puma
 * Package: org.puma.analyzer
 *
 * Author: Sergio Álvarez
 * Date: 01/2014
 */
class SimpleTermExtractorFilter extends ExtractorFilter{
  def extract(tweet: String): List[List[String]] = {
    List.empty[List[String]]
  }

  def field:String = "text"
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>ExtractorFilterDecorator.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer.filter

/**
 * Project: puma
 * Package: org.puma.analyzer.filter
 *
 * Author: Sergio Álvarez
 * Date: 01/2014
 */
abstract class ExtractorFilterDecorator(filter: ExtractorFilter) extends ExtractorFilter{
  def extract(tweet: String): List[List[String]]
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>MentionFilter.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer.filter

import scala.collection.JavaConverters._
import scala.collection.mutable.ListBuffer

/**
 * Project: puma
 * Package: org.puma.analyzer
 *
 * Author: Sergio Álvarez
 * Date: 01/2014
 */
class MentionFilter(filter: ExtractorFilter) extends ExtractorFilterDecorator(filter) {
  def this() = this(new SimpleTermExtractorFilter())

  private[this] val extractor = new com.twitter.Extractor

  def extract(tweet: String): List[List[String]] = {
    val mentions = filter.extract(tweet).to[ListBuffer] // initializing with previous extraction
    extractor.extractMentionedScreennames(tweet).asScala.foreach(term =&gt; {
      mentions += List(term.toLowerCase)
    })
    mentions.toList
  }

  def field: String = "text"
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>HashtagFilter.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer.filter

import scala.collection.JavaConverters._
import scala.collection.mutable.ListBuffer

/**
 * Project: puma
 * Package: org.puma.analyzer
 *
 * Author: Sergio Álvarez
 * Date: 01/2014
 */
class HashtagFilter(filter: ExtractorFilter) extends ExtractorFilterDecorator(filter){
  def this() = this(new SimpleTermExtractorFilter())

  private[this] val extractor = new com.twitter.Extractor

  def extract(tweet: String): List[List[String]] = {
    val hashtags = filter.extract(tweet).to[ListBuffer] // initializing with previous extraction
    extractor.extractHashtags(tweet).asScala.foreach(term =&gt; {
      hashtags += List(term.toLowerCase)
    })
    hashtags.toList
  }

  def field: String = "text"
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>BigramsFilter.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer.filter

import scala.collection.mutable.ListBuffer
import org.puma.analyzer.NgramExtractor

/**
 * Project: puma
 * Package: org.puma.analyzer
 *
 * Author: Sergio Álvarez
 * Date: 01/2014
 */
class BigramsFilter(filter: ExtractorFilter) extends ExtractorFilterDecorator(filter) {
  def this() = this(new SimpleTermExtractorFilter())

  def extract(tweet: String): List[List[String]] = {
    val results = filter.extract(tweet).to[ListBuffer] // initializing with previous extraction
    NgramExtractor.extract(tweet, 2).foreach(ngram =&gt; results += ngram)
    results.toList
  }

  def field: String = "text"
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>KeywordFilter.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer.filter

import scala.collection.mutable.ListBuffer
import org.puma.analyzer.NgramExtractor

/**
 * Project: puma
 * Package: org.puma.analyzer.filter
 *
 * Author: Sergio Álvarez
 * Date: 03/2014
 */
class KeywordFilter (filter: ExtractorFilter) extends ExtractorFilterDecorator(filter) {
  def this() = this(new SimpleTermExtractorFilter())

  def extract(tweet: String): List[List[String]] = {
    val results = filter.extract(tweet).to[ListBuffer] // initializing with previous extraction
    NgramExtractor.extract(tweet, 1).foreach(ngram =&gt; results += ngram)
    results.toList
  }

  def field: String = "text"
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>LocationFilter.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer.filter

import scala.collection.mutable.ListBuffer
import org.puma.analyzer.NgramExtractor

/**
 * Project: puma
 * Package: org.puma.analyzer.filter
 *
 * Author: Sergio Álvarez
 * Date: 03/2014
 */
class LocationFilter(filter: ExtractorFilter) extends ExtractorFilterDecorator(filter) {
  def this() = this(new SimpleTermExtractorFilter())

  def extract(tweet: String): List[List[String]] = {
    val results = filter.extract(tweet).to[ListBuffer] // initializing with previous extraction
    NgramExtractor.extract(tweet, 1).foreach(ngram =&gt; results += ngram)
    results.toList
  }

  def field: String = "location"
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Analyzer.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.analyzer

import scala.collection.mutable
import org.puma.configuration.ConfigurationUtil
import org.puma.stat.Dunning
import org.puma.analyzer.filter.ExtractorFilter

/**
 * Project: puma
 * Package: org.puma.analyzer
 *
 * Author: Sergio Álvarez
 * Date: 01/2014
 */
class Analyzer(local: String, global: String, filter: ExtractorFilter) {
  private[this] val localTerms =
    new Extractor().filter(filter).path(local).extract

  private[this] val globalTerms =
    new Extractor().filter(filter).path(global).extract

  private[this] val totalLocalFrequencies = localTerms.foldLeft(0)(_+_._2)
  private[this] val totalGlobalFrequencies = globalTerms.foldLeft(0)(_+_._2)

  private[this] val results = mutable.Map.empty[List[String], Double]
  private[this] val commonFreq = mutable.Map.empty[Int, Int]
  private[this] val minFrequencyLLR = ConfigurationUtil.getMinFrequencyForLLR

  def analyze: List[(List[String], Double)] = {
    localTerms.keys.foreach(terms =&gt; {
      val localFreq = localTerms.get(terms).get.toLong
      if(localFreq &gt; minFrequencyLLR) {
        val globalOption = globalTerms.get(terms)
        val globalFreq:Long =
          if (globalOption.isDefined) globalOption.get else averageGlobalFrequency(terms)

        if(globalFreq &gt; 0) {
          val k11 = localFreq
          val k12 = globalFreq
          val k21 = totalLocalFrequencies
          val k22 = totalGlobalFrequencies
          val llr = Dunning.normalizedRootLogLikelihoodRatio(k11, k12, k21, k22)
          results.put(terms, llr)
        }
      }
    })
    results.toList.sortBy({ _._2 })
  }

  private[this] def averageGlobalFrequency(terms: List[String]): Int = {
    if(commonFreq.get(localTerms.get(terms).get).isDefined) {
      return commonFreq.get(localTerms.get(terms).get).get
    }

    val sameFreq = localTerms.filterKeys((localTerm) =&gt;
      globalTerms.get(localTerm).isDefined &amp;&amp; localTerms.get(localTerm).get == localTerms.get(terms).get
    )

    val avg = if(!sameFreq.isEmpty) sameFreq.foldLeft(0)(_+_._2) / sameFreq.size else 0
    commonFreq.put(localTerms.get(terms).get, avg)

    avg
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Dunning.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.stat

/**
 * Project: puma
 * Package: org.puma.stat
 *
 * Author: Daniel Gayo-Avello
 * Date: 02/2014
 */
object Dunning {

  def rootLogLikelihoodRatio(a: Long, b: Long, c: Long, d: Long): Double = {
    val E1 = c * ((a + b) / (c + d).toDouble)
    val E2 = d * ((a + b) / (c + d).toDouble)

    val E3 = if(a == 0) 1/E1 else a/E1
    val E4 = if(b == 0) 1/E2 else b/E2
    val result = Math.sqrt(2 * (a * Math.log(E3) + b * Math.log(E4)))

    if ((a / c.toDouble) &lt; (b / d.toDouble)) -result else result
  }

  def normalizedRootLogLikelihoodRatio (a: Long, b: Long, c: Long, d: Long): Double = {
    val min    = rootLogLikelihoodRatio(0,d,c,d)
    val max    = rootLogLikelihoodRatio(c,0,c,d)

    val a_norm = a / c.toDouble
    val b_norm = b / d.toDouble
    val llr    = rootLogLikelihoodRatio(a,b,c,d)

    if (llr &gt; 0) llr / (max * a_norm) else -llr / (min * b_norm)
  }

}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Configuration.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.configuration

import java.io.File

case class Configuration(config: File = null) {}

object CLIParser {
  private[this] val parser = new scopt.OptionParser[Configuration]("Puma") {
    head("Puma", "1.0")
    opt[File]('c', "config")
      .required() action { (x, c) =&gt; c.copy(config = x) } text("Configuration XML file for executing Puma.")
  }

  def parse(args:Array[String], conf:Configuration):Option[Configuration] = this.parser.parse(args, conf)
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>ConfigurationUtil.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">package org.puma.configuration

import scala.collection.mutable.ListBuffer
import scala.io.Source
import org.puma.analyzer.filter.ExtractorFilter
import scala.xml.{Elem, XML}
import java.io.{BufferedReader, File, InputStream}

/**
 * Project: puma
 * Package: org.puma.configuration
 *
 * Author: Sergio Álvarez
 * Date: 01/2014
 */
object ConfigurationUtil {

  /* ===========================================================
   *                     CONSTANTS
   * =========================================================== */
  private[this] val ModePropertyKey              = "mode"
  private[this] val LocalFilePropertyKey         = "local"
  private[this] val GlobalFilePropertyKey        = "global"
  private[this] val FiltersPropertyKey           = "filters"
  private[this] val FilterPropertyKey            = "filter"
  private[this] val MinFrequencyForLLRKey        = "minimumFrequency"
  private[this] val MaximumExtractedTermsKey     = "maximumExtractedTerms"
  private[this] val FactorToRemoveKey            = "factorToRemove"
  private[this] val FilePropertyKey              = "file"
  private[this] val FileToAnalyzePropertyKey     = "fileToAnalyze"
  private[this] val BoundingBoxesFilePropertyKey = "localBoundingBoxes"

  private[this] val LLRGeneratorKey              = "llrGenerator"
  private[this] val ScoreGeneratorKey            = "scoreGenerator"
  private[this] val LLRFilesPropertyKey          = "llrFiles"

  private[this] val StopWordsFileName            = "common-stop-words.txt"

  /* =========================================================== */

  private[this] var config: Configuration     = null
  private[this] var _XmlConfiguration: Elem   = null
  private[this] var _stopWords: Array[String] = null

  def load(config: Configuration): Unit = this.config = config

  def getMode: String = (XmlConfiguration \\ ModePropertyKey).text

  def getFilesToAnalyze: List[String] = {
    val local  = (XmlConfiguration \\ LLRGeneratorKey \\ LocalFilePropertyKey).text
    val global = (XmlConfiguration \\ LLRGeneratorKey \\ GlobalFilePropertyKey).text
    List(local, global)
  }

  def getFiltersToApply: Seq[ExtractorFilter] = {
    (XmlConfiguration \\ LLRGeneratorKey \\ FiltersPropertyKey \\ FilterPropertyKey)
      .map(n =&gt; Class.forName(n.text).newInstance.asInstanceOf[ExtractorFilter])
  }

  def getMinFrequencyForLLR: Int = (XmlConfiguration \\ LLRGeneratorKey \\ MinFrequencyForLLRKey).text.toInt

  def getMaximumExtractedTerms: Int = (XmlConfiguration \\ LLRGeneratorKey \\ MaximumExtractedTermsKey).text.toInt

  def getFactorToRemove: Float = (XmlConfiguration \\ LLRGeneratorKey \\ FactorToRemoveKey).text.toFloat

  def stopWords: Array[String] = {
    if(_stopWords == null) { _stopWords = loadStopWords }
    _stopWords
  }

  def getLLRResultFiles: Seq[String] = (XmlConfiguration \\ ScoreGeneratorKey \\ LLRFilesPropertyKey \\ FilePropertyKey).map(n =&gt; n.text)

  def getFileToAnalyze:String = (XmlConfiguration \\ ScoreGeneratorKey \\ FileToAnalyzePropertyKey).text

  def getBoundingBoxesFile: String = (XmlConfiguration \\ ScoreGeneratorKey \\ BoundingBoxesFilePropertyKey).text

  private[this] def XmlConfiguration: Elem = {
    if(_XmlConfiguration == null) { _XmlConfiguration = XML.loadFile(config.config) }
    _XmlConfiguration
  }

  private[this] def loadStopWords: Array[String] =
    Source.fromInputStream(ConfigurationUtil.getClass.getResourceAsStream(s"/$StopWordsFileName")).getLines().toArray
}</programlisting>
</para>
</formalpara>
<simpara><?asciidoc-pagebreak?></simpara>
</section>
<section xml:id="__emphasis_scripts_emphasis_de_utilidad">
<title><emphasis>Scripts</emphasis> de utilidad</title>
<formalpara>
<title>training_dataset_divisor.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">#!/bin/sh
exec scala "$0" "$@"
!#
import java.io._
import scala.io.Source
import scala.collection.mutable.ListBuffer

val TRAINING_PERCENT = 0.8

val dataset = args(0)
val lines = Source.fromFile(dataset).getLines.toArray

val (trainingUsers, testingUsers) = divideUsersForDatasets(lines)

writeLines(dataset + "_train", trainingUsers, lines)
writeLines(dataset + "_test", testingUsers, lines)

/* ==================================================
 *                   FUNCTIONS
 * ================================================== */

def divideUsersForDatasets(lines:Array[String]):(Array[String], Array[String]) = {
  val users = new ListBuffer[String]
  lines.foreach(line =&gt; {
      val username = getUserByLine(line)
      if(!users.contains(username)) users += username
  })

  val forTraining = (users.length * TRAINING_PERCENT).toInt
  val train = users.slice(0, forTraining)
  val test  = users.slice(forTraining, users.length)

  (train.toArray, test.toArray)
}

def writeLines(datasetName:String, users:Array[String], lines:Array[String]):Unit = {
  val out = new File(datasetName)
  if(!out.exists) out.createNewFile
  val writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(out, true)))
  try {
    lines.foreach(line =&gt; if(users.contains(getUserByLine(line))) writer.write(line + "\n"))
  } finally {
    writer.flush()
    writer.close()
  }
}

def getUserByLine(line:String):String = {
  val features = line.split("\\|");
  val fields = features(1).split(" ")
  fields(1)
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>tweets_by_location.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">#!/bin/sh
exec scala "$0" "$@"
!#
import scala.io.Source
import scala.collection.mutable._
import scala.xml.pull._
import scala.xml._

import java.io._

def getBoundingBoxes(root: Elem): List[List[(Double, Double)]] = {
  var boundingBoxes = new ListBuffer[List[(Double, Double)]]
  (root \\ "boundingBox").foreach(b =&gt; {
    val sw_long = (b \ "sw" \ "longitude").text
    val sw_lat = (b \ "sw" \ "latitude").text
    val ne_long = (b \ "ne" \ "longitude").text
    val ne_lat = (b \ "ne" \ "latitude").text

    boundingBoxes += List((sw_lat.toDouble, sw_long.toDouble), (ne_lat.toDouble, ne_long.toDouble))
  })
  boundingBoxes.toList
}

def isInBoundingBoxes(latitude: String, longitude: String): Boolean = {
  boundingBoxes.foreach(b =&gt; {
    val sw = b(0)
    val ne = b(1)
    if(sw._1 &lt;= latitude.toDouble &amp;&amp; ne._1 &gt;= latitude.toDouble &amp;&amp; sw._2 &lt;= longitude.toDouble &amp;&amp; ne._2 &gt;= longitude.toDouble) return true
  })
  false
}

def isIncludable(latitude: String, longitude: String): Boolean = {
  if(mustBeIn) isInBoundingBoxes(latitude, longitude) else !isInBoundingBoxes(latitude, longitude)
}

def buildTweet(values: List[String]): String = {
  val tweet =
  &lt;tweet&gt;
    &lt;id&gt;{values(0)}&lt;/id&gt;
    &lt;username&gt;{values(1)}&lt;/username&gt;
    &lt;name&gt;{values(2)}&lt;/name&gt;
    &lt;location&gt;{values(3)}&lt;/location&gt;
    &lt;timezone&gt;{values(4)}&lt;/timezone&gt;
    &lt;createdAt&gt;{values(5)}&lt;/createdAt&gt;
    &lt;latitude&gt;{values(6)}&lt;/latitude&gt;
    &lt;longitude&gt;{values(7)}&lt;/longitude&gt;
    &lt;text&gt;{values(8)}&lt;/text&gt;
  &lt;/tweet&gt;

  s"\n${tweet.toString}\n"
}

def serialize(tweet: String) = { writer.write(tweet) }

// id, username, name, location, timezone, createdAt, latitude, longitude, text
def save(values: List[String]) = {
  val latitude  = values(6)
  val longitude = values(7)
  if(isIncludable(latitude, longitude)){
    serialize(buildTweet(values))
  }
}

def appendToLastValue(text:String, values:ListBuffer[String]): Unit = {
  var last = values(values.size - 1)
  values(values.size - 1) = last + text;
}

def getWriter(): BufferedWriter = {
  val suffix = if(mustBeIn) "_in_bounding_boxes" else "_no_bounding_boxes"
  val out = new File(tweetsFile + "_tweets_by_location" + suffix + ".xml")
  if(!out.exists) out.createNewFile

  new BufferedWriter(new OutputStreamWriter(new FileOutputStream(out)))
}

val tweetsFile        = args(0).toString
val boundingBoxesFile = args(1).toString
val mustBeIn          = args(2).toBoolean

val boundingBoxes = getBoundingBoxes(XML.loadFile(boundingBoxesFile))

val tweets = new XMLEventReader(Source.fromFile(tweetsFile))
var values = new ListBuffer[String]

val writer: BufferedWriter = getWriter()

try {
  var current = ""
  var last = ""

  writer.write("&lt;tweets&gt;")
  tweets.foreach(event =&gt; {
    event match {
      case e: EvElemStart if e.label != "tweets" &amp;&amp; e.label != "tweet" =&gt;
      current = e.label
      case e: EvElemEnd if e.label != "tweets" &amp;&amp; e.label != "tweet" =&gt;
      current = ""
      case EvElemEnd(_, "tweet") =&gt;
      save(values.toList)
      values.clear
      case EvText(text) if
        Array("id",
              "username",
              "name",
              "location",
              "latitude",
              "longitude",
              "timezone",
              "createdAt",
              "text"
        ).contains(current) =&gt;
      if(current == last) appendToLastValue(text, values) else values += text
      last = current
      case EvEntityRef("amp") =&gt;
      if(current == last) appendToLastValue("&amp;", values) else values += "&amp;"
      last = current
      case EvEntityRef("lt")  =&gt;
      if(current == last) appendToLastValue("&lt;", values) else values += "&lt;"
      last = current
      case EvEntityRef("gt")  =&gt;
      if(current == last) appendToLastValue("&gt;", values) else values += "&gt;"
      last = current
      case _ =&gt; ;
    }
  })
  writer.write("&lt;/tweets&gt;")
} finally {
  writer.flush()
  writer.close()
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>tweets_by_time.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">#!/bin/sh
exec scala "$0" "$@"
!#
import scala.io.Source
import scala.xml.pull._
import scala.xml.pull.EvElemStart
import scala.xml.pull.EvText
import java.io._


val fileToAnalyze = args(0).toString
var freq = collection.mutable.Map[String, Int]().withDefaultValue(0)
var isCreatedAtElement = false

new XMLEventReader(Source.fromFile(fileToAnalyze)).foreach(event =&gt; {
  event match {
    case EvElemStart(_, "createdAt", _, _) =&gt; isCreatedAtElement = true
    case EvText(text) if isCreatedAtElement =&gt; {
      val hour = text.trim.slice(text.trim.lastIndexOf(" "), text.trim.length - 1)
      freq(hour) += 1
    }
    case EvElemEnd(_, "createdAt") =&gt; isCreatedAtElement = false
    case _ =&gt; ;
  }
})

val sorted = freq.toList.sortBy({ _._1})

val file = new File(s"${fileToAnalyze}_tweetsByTime.tsv")
if(!file.exists) file.createNewFile

val writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(file)))
try{
  sorted.foreach(t =&gt; writer.write(s"${t._2}\t${t._1}0\n"))
}finally{
  writer.flush()
  writer.close()
}</programlisting>
</para>
</formalpara>
<simpara><?asciidoc-pagebreak?></simpara>
</section>
<section xml:id="_vowpal_wabbit_input_translator">
<title>Vowpal Wabbit Input Translator</title>
<formalpara>
<title>Configuration.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">import java.io.File

case class Configuration(inout:Boolean = false, decimals:Int = -1, file:File = null) {}

object CLIParser {
  private[this] val parser = new scopt.OptionParser[Configuration]("vw-input-translator") {
    head("vw-input-translator", "1.0")
    opt[Unit]("inout") action { (x, c) =&gt; c.copy(inout = true) } text("Flag for creating an input file for binary classification")
    opt[Int]('d', "decimals") action { (x, c) =&gt; c.copy(decimals = x) } validate { x =&gt;
      if (x &gt; 0) success else failure("Option --decimals must be &gt; 0") } text("Creates an input file for multi-class classification." +
        " Each sample will have the selected number of decimals on latitude and longitude coordinates")
    arg[File]("&lt;file&gt;") unbounded() action { (x, c) =&gt; c.copy(file = x) } text("Source file")
    checkConfig { c =&gt; if (c.inout &amp;&amp; c.decimals &gt; -1) failure("You cannot use --inout and --decimals at the same time") else success }
    checkConfig { c =&gt; if (!c.inout &amp;&amp; c.decimals == -1) failure("You have to provide the --inout OR --decimals parameter") else success }
  }

  def parse(args:Array[String], conf:Configuration):Option[Configuration] = this.parser.parse(args, conf)
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Main.scala</title>
<para>
<programlisting language="java" linenumbering="unnumbered">import java.io._
import scala.io.Source
import scala.collection.mutable.ListBuffer
import java.security.MessageDigest

object Main {
  def main(args: Array[String]): Unit = {
    CLIParser.parse(args, Configuration()) map { config =&gt;
      val translator = new VWTranslator(config)
      Source.fromFile(config.file).getLines.foreach(line =&gt; translator.translate(line))
      translator.saveDictionary

      println("Number of classes extracted: " + translator.numberOfClasses)
    }
  }
}

object Writer {
  def write(line: String, file: String):Unit = {
    val out = new File(file)
    if(!out.exists) out.createNewFile

    val writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(out, true)))
    try {
      writer.write(line)
    } finally {
      writer.flush()
      writer.close()
    }
  }
}

class VWTranslator(config: Configuration) {
  private[this] val dictionary = new ListBuffer[(String, Int)]()

  def translate(raw: String): Unit = {
    val columns       = raw.split("\t")
    val classLabel    = getClass(columns)
    val formattedLine = s"$classLabel |Tweet ${columns(3)}\n"

    Writer.write(formattedLine, outputFileName)
  }

  def numberOfClasses: Int = dictionary.size

  def saveDictionary: Unit = {
    val fileName = s"${outputFileName}_dictionary"
    dictionary.foreach { case (k, c) =&gt; Writer.write(s"$c\t$k\n", fileName) }
  }

  private[this] def outputFileName: String = {
    var file = config.file
    if(this.config.inout) s"${file}_vw_inout" else s"${file}_vw_${config.decimals}_decimals"
  }

  private[this] def getClass(columns: Array[String]): Int =
    if(config.inout) getBinaryClass(columns(0).toDouble) else getMultiClassLabel(columns(1), columns(2))

  private[this] def getBinaryClass(score: Double): Int = if(score &lt; 0.0) 0 else 1

  private[this] def getMultiClassLabel(lat:String, lng:String): Int = {
    val trunkedLat = trunkCoordinate(lat)
    val trunkedLng = trunkCoordinate(lng)

    val key = trunkedLat + "," + trunkedLng

    if(dictionary.isEmpty) {
      dictionary += key -&gt; 1
      1
    } else{
      val storedClassLabel = getClassLabelInDictionary(key)
      if(storedClassLabel.isEmpty) {
        val last = dictionary.last._2
        dictionary += key -&gt; (last + 1)
        last + 1
      }else {
        storedClassLabel.get._2
      }
    }
  }

  private[this] def getClassLabelInDictionary(key: String): Option[(String, Int)] = dictionary.find(t =&gt; t._1 == key)

  private[this] def trunkCoordinate(coordinate: String): String = {
    val parts = coordinate.split("\\.")
    parts(0) + "." + parts(1).slice(0, config.decimals)
  }
}</programlisting>
</para>
</formalpara>
<simpara><?asciidoc-pagebreak?></simpara>
</section>
<section xml:id="_b2pick">
<title>B2pick</title>
<formalpara>
<title>map.html</title>
<para>
<programlisting language="html" linenumbering="unnumbered">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset="UTF-8"&gt;
  &lt;meta name="description" content="Bounding boxes picker"&gt;
  &lt;meta name="keywords" content="bounding, boxes, twitter, locations, georeference, coordinates, maps"&gt;
  &lt;meta name="author" content="Sergio Álvarez Suárez"&gt;

  &lt;link rel="icon" href="../favicon.png" type="image/png" /&gt;

  &lt;link rel="stylesheet" href="./fancybox/source/jquery.fancybox.css" type="text/css" media="screen" /&gt;

  &lt;link href='http://fonts.googleapis.com/css?family=Neucha' rel='stylesheet' type='text/css'&gt;
  &lt;link href='http://fonts.googleapis.com/css?family=Signika+Negative:300,400' rel='stylesheet' type='text/css'&gt;
  &lt;link href='http://fonts.googleapis.com/css?family=Ubuntu+Mono' rel='stylesheet' type='text/css'&gt;

  &lt;link rel="stylesheet" type="text/css" href="style.css"&gt;

  &lt;title&gt; b2pick: picking bounding-boxes&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;header&gt;
    &lt;h1&gt;b2pick&lt;/h1&gt;
    &lt;h2&gt;Bounding Boxes picker&lt;/h2&gt;
  &lt;/header&gt;
  &lt;section id="main_content"&gt;
    &lt;section class="column"&gt;
      &lt;section&gt;
        &lt;p&gt;b2pick allows to picking up bounding boxes directly from Google Maps.&lt;/p&gt;
        &lt;p&gt;You can draw rectangles around the desired locations. Coordinates from the bounding boxes will be shown down taking the following form:&lt;/p&gt;
        &lt;p&gt;&lt;span class="code"&gt;SW_long, SW_lat, NE_long, NE_lat&lt;/span&gt;&lt;/p&gt;
      &lt;/section&gt;
      &lt;button type="button"&gt;Print in XML&lt;/button&gt;
      &lt;section class="boxes"&gt;
      &lt;/section&gt;
    &lt;/section&gt;
    &lt;section class="column"&gt;
      &lt;div id="map_canvas"&gt;&lt;/div&gt;
    &lt;/section&gt;
    &lt;div class="clear_both"&gt;&lt;/div&gt;
  &lt;/section&gt;
  &lt;a id="inline" href="#xml" style="display:none"&gt;&lt;/a&gt;
  &lt;div style="display:none"&gt;&lt;div id="xml"&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;footer&gt;
    &lt;p&gt;Developed by &lt;a href="http://www.sergioalvarezsuarez.com" target="blank"&gt;Sergio Álvarez&lt;/a&gt;&lt;/p&gt;
  &lt;/footer&gt;

  &lt;!-- loading scripts --&gt;
  &lt;script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"&gt;&lt;/script&gt;
  &lt;script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jqueryui/1.10.3/jquery-ui.min.js"&gt;&lt;/script&gt;
  &lt;script type="text/javascript" src="./fancybox/source/jquery.fancybox.pack.js"&gt;&lt;/script&gt;
  &lt;script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?v=3.exp&amp;sensor=false&amp;libraries=drawing"&gt;&lt;/script&gt;
  &lt;script src="index.js"&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</programlisting>
</para>
</formalpara>
<formalpara>
<title>style.css</title>
<para>
<programlisting language="css" linenumbering="unnumbered">* {
  margin: 0;
  padding: 0;
  font-size: 10px;

  font-family: 'Signika Negative', sans-serif;
}

body{
  background-color: rgba(0, 0, 0, .7);
}

header{
  width: 100%;
  padding: 1em 0;

  background-color: rgb(0, 0, 0);
  color: rgb(255, 255, 255);
}

header h1 { font-size: 3em;  margin-left: 1em;  font-family: 'Neucha', cursive;}
header h2 { font-size: 1.5em;  margin-left: 30px;  font-family: 'Neucha', cursive;}

#main_content {
  overflow: hidden;
  width: 100%;
}

section.column{
  float: left;
  padding-bottom: 9999px;
  margin-bottom: -9999px;
}

#main_content &gt; section:first-child  { width: 40em; }
#main_content &gt; section:last-of-type { width: calc(100% - 40em); }

#main_content &gt; section:first-child {
  overflow-y: scroll;
  height: 600px;
  background-color: rgba(255, 255, 255, .7);
}

#main_content &gt; section:first-child &gt; section:first-child {
  margin: 1em;
  padding: 1em;
  background-color: rgba(255, 255, 255, .4);

  font-weight: lighter;
}

#main_content &gt; section:first-child &gt; section:first-child p {
  font-size: 1.4em;
}

span.code {
  font-size: 1em;
  font-family: 'Ubuntu Mono', sans-serif;
}

div.box {
  margin: 1em;
  padding: 1em;
  background-color: rgba(255, 255, 255, .4);

  font-weight: lighter;
}

div.box h3 {
  font-size: 1.6em;
}

div.box p {
  font-size: 1.4em;
}

#map_canvas {
  width: 100%;
  height: 600px;
}

.clear_both {
  clear: both;
}

button {
  background-image: -webkit-linear-gradient(#88af39, #690);
  border-radius: 4px;
  border: 1px solid rgb(45, 128, 19);
  color: #FFF;
  cursor: pointer;
  display: block;
  font-size: 1.3em;
  margin: 0 auto;
  padding: 10px;
  width: 100px;
}

#xml pre {
  font-size: 1.5em;
}

/* FOOTER
******************************************* */

footer {
  padding: 5em;

  color: rgb(255, 255, 255);
  text-align: center;
  font-weight: lighter;
}

footer p {
  font-size: 1.5em;
}

footer p a {
  font-size: 1em;
}

footer p a:link, footer p a:visited {
  color: rgb(255, 255, 255);
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>index.js</title>
<para>
<programlisting language="javascript" linenumbering="unnumbered">$(document).ready(function(){
  salvarezsuar.MapModule.install();

  $('button').click(function() {
    var xml = salvarezsuar.BoundingBoxesManager.toXML();
    $xml = $('#xml');
    $xml.empty();

    $output = $('&lt;pre&gt;&lt;/pre&gt;').text(xml);
    $xml.append($output);

    $('a#inline').click();
  });

  $('a#inline').fancybox({
    'width': 500,
    'autoDimensions': false,
    'autoSize': false
  });
});

var salvarezsuar = salvarezsuar || {};
salvarezsuar.BoundingBoxesManager = (function() {
  var boundingBoxes = [];

  var add = function(boundingBox) {
    boundingBoxes.push(boundingBox);
  };

  var clear = function() {
    boundingBoxes.length = 0;
  };

  var toXML = function() {
    var boundingBoxes2xml = boundingBoxes.map(function(boundingBox){
      var xml = "";
      xml += "\n\t&lt;boundingBox&gt;\n";
      xml += "\t\t&lt;sw&gt;\n";
      xml += "\t\t\t&lt;latitude&gt;" + boundingBox.sw.latitude + "&lt;/latitude&gt;\n";
      xml += "\t\t\t&lt;longitude&gt;" + boundingBox.sw.longitude + "&lt;/longitude&gt;\n";
      xml += "\t\t&lt;/sw&gt;\n";
      xml += "\t\t&lt;ne&gt;\n";
      xml += "\t\t\t&lt;latitude&gt;" + boundingBox.ne.latitude + "&lt;/latitude&gt;\n";
      xml += "\t\t\t&lt;longitude&gt;" + boundingBox.ne.longitude + "&lt;/longitude&gt;\n";
      xml += "\t\t&lt;/ne&gt;\n";
      xml += "\t&lt;/boundingBox&gt;";

      return xml;
    });

    var xml = "&lt;boundingBoxes&gt;";
    xml += boundingBoxes2xml.join("");
    xml += "\n&lt;/boundingBoxes&gt;";

    return xml;
  };

  return {
    add: add,
    toXML: toXML,
    clear: clear
  };
})();

salvarezsuar.MapModule = (function(){
  var rectangles = [];
  var currentRectangle = null;
  var isMovingRectangle = false;

  var createMap = function(){
    var centerPoint = new google.maps.LatLng(43.354810, -5.851805); // Oviedo, Asturias, Spain.
    var options = {
      zoom : 3,
      heading : 90,
      tilt : 45,
      center : centerPoint,
      mapTypeControl : false
    };

    // Initialize variables
    var map = new google.maps.Map($('#map_canvas')[0], options);
    var drawingManager = new google.maps.drawing.DrawingManager({
      drawingControl: true,
      drawingControlOptions: {
        position: google.maps.ControlPosition.TOP_CENTER,
        drawingModes: [google.maps.drawing.OverlayType.RECTANGLE]
      },
      rectangleOptions : {
        draggable: true,
        clickable: true,
        editable: true
      }
    });
    drawingManager.setMap(map);

    google.maps.event.addListener(
      drawingManager, 'rectanglecomplete', function(rectangle){
        appendBoundingBoxInfo(rectangle);
        rectangles.push(rectangle);
        google.maps.event.addListener(
          rectangle, 'bounds_changed', function(){
            $('section.boxes').empty();
            salvarezsuar.BoundingBoxesManager.clear();

            $(rectangles).each(function(index, rectangle){
              appendBoundingBoxInfo(rectangle);
            });
          }
        );
      }
    );
  };

  var appendBoundingBoxInfo = function (rectangle){
    //Obtainig bouding boxes
    var swLong = rectangle.getBounds().getSouthWest().lng().toFixed(2);
    var swLat  = rectangle.getBounds().getSouthWest().lat().toFixed(2);
    var neLong = rectangle.getBounds().getNorthEast().lng().toFixed(2);
    var neLat  = rectangle.getBounds().getNorthEast().lat().toFixed(2);

    salvarezsuar.BoundingBoxesManager.add({
      sw: {
        latitude: swLat,
        longitude: swLong
      },
      ne: {
        latitude: neLat,
        longitude: neLong
      }
    });

    //Creating element for the info panel
    var newBox = $(document.createElement('div'));
    $(newBox).addClass('box');
    $(newBox).append('&lt;h3&gt;Bounding Box&lt;/h3&gt;');
    $(newBox).append('&lt;p&gt;Coordinates:&lt;/p&gt;');
    $(newBox).append('&lt;p&gt;' + swLong + ', ' + swLat + ', ' + neLong + ', ' + neLat + '&lt;/p&gt;');
    $('section.boxes').append(newBox);
  };

  return {
    install: createMap
  };
})();</programlisting>
</para>
</formalpara>
<simpara><?asciidoc-pagebreak?></simpara>
</section>
<section xml:id="_tweetheat">
<title>Tweetheat</title>
<formalpara>
<title>index.html</title>
<para>
<programlisting language="html" linenumbering="unnumbered">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;link href="./stylesheet/main.css" rel="stylesheet" type="text/css"&gt;
  &lt;script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?v=3.exp&amp;sensor=false&amp;libraries=visualization"&gt;&lt;/script&gt;
  &lt;script type="text/javascript" src="./javascript/lib/jquery.js"&gt;&lt;/script&gt;
  &lt;script type="text/javascript" src="./javascript/map.js"&gt;&lt;/script&gt;
  &lt;script type="text/javascript" src="./javascript/index.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;section&gt;
    &lt;div class="drop_zone"&gt;
      &lt;p&gt;Drop here your TSV file!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/section&gt;
  &lt;div class="map"&gt;&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;</programlisting>
</para>
</formalpara>
<formalpara>
<title>main.css</title>
<para>
<programlisting language="css" linenumbering="unnumbered">* {
  margin: 0;
  padding: 0;
}

html, body { height: 100%; }

section:first-child{
  background-color: rgb(0, 154, 206);
  padding: 10px;
  height: 130px;
}

.drop_zone {
  height: calc(100% - 20px);
  border: 10px dashed rgb(159, 221, 237);
  text-align: center;
  line-height: 110px;
}

.drop_zone {
  color: rgb(159, 221, 237);
  font-size: 32px;
  font-weight: bold;
  font-family: sans-serif;
}

.hover {
  opacity: 0.6;
  transition: opacity 0.2s linear;
}

.map {
  height: calc(100% - 150px);
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>index.js</title>
<para>
<programlisting language="javascript" linenumbering="unnumbered">$(document).ready(function(){
  $('body').on('dragover', 'section:first-child', function(event){
    event.stopPropagation();
    event.preventDefault();
    $('section:first-child').addClass('hover');
  });

  $('body').on('dragleave', 'section:first-child', function(event){
    event.stopPropagation();
    event.preventDefault();
    $('section:first-child').removeClass('hover');
  });

  $('body').on('drop', 'section:first-child', function(event){
    if(event.originalEvent.dataTransfer &amp;&amp; event.originalEvent.dataTransfer.files.length &gt; 0) {
        event.stopPropagation();
        event.preventDefault();
        var files = event.originalEvent.dataTransfer.files;
        MapModule.loadFiles(files);
    }
    $('section:first-child').removeClass('hover');
  });

  google.maps.event.addDomListener(window, 'load', MapModule.install());
});</programlisting>
</para>
</formalpara>
<formalpara>
<title>map.js</title>
<para>
<programlisting language="javascript" linenumbering="unnumbered">var MapModule = (function(){
  var heatpoints = new google.maps.MVCArray([]);

  var addPointToHeatmap = function(lat, lng) {
    var point = new google.maps.LatLng(lat, lng);
    heatpoints.push(point);
  };

  var processTsvFile = function(event){
    var tsv = event.target.result;
    tsv.split("\n").forEach(function(row){
      var columns = row.split('\t');
      var lat = parseFloat(columns[1]);
      var lng = parseFloat(columns[2]);
      addPointToHeatmap(lat, lng);
    });
  };

  var install = function(){
    var mapOptions = { zoom: 2, center: new google.maps.LatLng(43.354810, -5.851805)};
    var map = new google.maps.Map($('.map')[0], mapOptions);
    var heatmap = new google.maps.visualization.HeatmapLayer({
      data: heatpoints
    });
    heatmap.setMap(map);
  };

  var loadFiles = function(files) {
    for(var i = 0; i &lt; files.length; i++) {
      var currentFile = files[i];
      var reader = new FileReader();
      reader.onload = processTsvFile;
      reader.readAsText(currentFile);
    }
  };

  return {
    install: install,
    loadFiles: loadFiles
  };
})();</programlisting>
</para>
</formalpara>
</section>
</appendix>
</book>