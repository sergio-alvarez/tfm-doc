<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:lang="es">
<info>
<title>Geolocalización de usuarios en medios sociales mediante análisis de contenidos</title>
<date>2014-05-24</date>
<authorgroup>
<author>
<personname>
<firstname>Sergio</firstname>
<othername>Álvarez</othername>
<surname>Suárez</surname>
</personname>

</author>

<author>
<personname>
<firstname>Daniel</firstname>
<othername>Gayo</othername>
<surname>Avello</surname>
</personname>

</author>

</authorgroup>



</info>
<chapter xml:id="_introducci_n">
<title>Introducción</title>
<section xml:id="_motivaci_n_del_proyecto">
<title>Motivación del proyecto</title>
<simpara>Aquí la motivación del proyecto</simpara>
</section>
<section xml:id="_alcance">
<title>Alcance</title>
<simpara>Aquí el alcance del proyecto.</simpara>
</section>
<section xml:id="_estado_del_arte">
<title>Estado del arte</title>
<simpara>El crecimiento exponencial de las redes sociales durante los últimos años ha despertado un gran interés en los diferentes ámbitos de la informática, siendo un claro objetivo comercial para profesionales del sector, así como un nuevo campo de investigación para los investigadores universitarios.</simpara>
<simpara>Como consecuencia de todo ello, durante los últimos años han ido apareciendo diversas aplicaciones que, de una u otra manera, se centran en estudiar ciertos aspectos de las redes sociales para poder extraer información acerca de sus usuarios gracias a las diversas publicaciones que estos mismos realizan en sus perfiles.</simpara>
<simpara>El estudio de la geolocalización de un usuario a partir de su contenido, sin embargo, es una de las pocas áreas que <emphasis>tan sólo</emphasis> agrupa un pequeño número de estudios teóricos, pero en donde no han proliferado herramientas que comprueben de manera empírica los resultados teóricos emitidos por diversos investigadores.</simpara>
<simpara>Por ello, en este capítulo se recopilan algunos estudios teóricos y artículos de investigación que han servido como punto de arranque para este proyecto, así como algunas aplicaciones que comparten características similares.</simpara>
<section xml:id="_estudios_y_art_culos_de_investigaci_n">
<title>Estudios y artículos de investigación</title>
<section xml:id="_tweets_from_justin_bieber_s_heart_the_dynamics_of_the_location_field_in_user_profiles">
<title>Tweets from Justin Bieber’s Heart: The Dynamics of the "Location" Field in User Profiles</title>
<simpara><emphasis>Por Brent Hecht et al. Northwestern University y Palo Alto Research Center</emphasis></simpara>
<simpara>El estudio liderado por Brent Hecht demuestra de manera científica como el uso del campo de Localización disponible en los perfiles de los usuarios en Twitter no es un indicador válido para obtener su posición geográfica real.</simpara>
<simpara>Entre los datos obtenidos por Brent y su equipo se pueden destacar los siguientes:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Sólo un 66% de los usuarios utiliza el campo de Localización para aportar información geográfica válida (es decir, localizaciones reales pero que <emphasis role="strong">no tienen porqué indicar su situación real</emphasis>. Por ejemplo, si una persona de <emphasis role="strong">Oviedo</emphasis> escribe <emphasis role="strong">California</emphasis> en el campo de Localización de su perfil, este estudio lo incluye dentro del 66% anterior).</simpara>
</listitem>
<listitem>
<simpara>Algunos de los usos que los usuarios dentro del 34% restante aplican al campo de Localización se puede observar en <xref linkend="location-use-type"/>.</simpara>
</listitem>
<listitem>
<simpara>La manera de distinguir entre localización real y ficticia supuso de un proceso manual en el que dos miembros del equipo debieron revisar tuit a tuit el campo de Localización, debido a la habilidad de los usuarios para poder expresar sarcasmo o ironía, así como expresiones comunes que pueden tener asociado un componente geográfico inherente (un ejemplo puede ser referirse al Principado de Asturias como <emphasis>la tierrina</emphasis>).</simpara>
</listitem>
</orderedlist>

<table xml:id="location-use-type" frame="all"
    rowsep="1" colsep="1">
<title>Tipos de uso del campo de Localización por parte de los usuarios de Twitter</title>
  
  <tgroup cols="2">
    
    <colspec colname="col_1" colwidth="75*"/>
    
    <colspec colname="col_2" colwidth="25*"/>
    
    
    <tbody>
      
      <row>
        
        <entry align="left" valign="top"><simpara>Cultura popular</simpara></entry>
        
        <entry align="left" valign="top"><simpara>12.9%</simpara></entry>
        
      </row>
      
      <row>
        
        <entry align="left" valign="top"><simpara>Referencias a su propia privacidad</simpara></entry>
        
        <entry align="left" valign="top"><simpara>1.2%</simpara></entry>
        
      </row>
      
      <row>
        
        <entry align="left" valign="top"><simpara>Insultos o contenido violento</simpara></entry>
        
        <entry align="left" valign="top"><simpara>4.6%</simpara></entry>
        
      </row>
      
      <row>
        
        <entry align="left" valign="top"><simpara>Localizaciones no terráqueas</simpara></entry>
        
        <entry align="left" valign="top"><simpara>5.0%</simpara></entry>
        
      </row>
      
      <row>
        
        <entry align="left" valign="top"><simpara>Emociones negativas hacia su localización real</simpara></entry>
        
        <entry align="left" valign="top"><simpara>3.2%</simpara></entry>
        
      </row>
      
      <row>
        
        <entry align="left" valign="top"><simpara>Naturaleza sexual</simpara></entry>
        
        <entry align="left" valign="top"><simpara>3.2%</simpara></entry>
        
      </row>
      
    </tbody>
    
  </tgroup>
</table>

<simpara>Como alternativa a los resultados anteriores, y buscando una manera de automatizar el proceso y encontrar resultados más fiables y exhaustivos, se propusieron hacer un primer experimento para comprobar si el estudio de los contenidos publicados por un usuario pueden aportar la información necesaria para permitir inferir su ubicación geográfica.</simpara>
<simpara>Para ello, utilizaron un software de aprendizaje automático y un clasificador Bayesiano multinomial que en base a un conjunto de datos obtenidos a partir de aplicar el algoritmo <emphasis>CALGARI</emphasis> (de implementación propia), fuese capaz de predecir a qué área (País y Estado) pertenece un tuit en base a su contenido.</simpara>
<note>
<title>CALGARI</title>
<simpara>El algoritmo CALGARI tiene como objetivo normalizar la frecuencia con la que un término ha aparecido dentro de un dataset de tuits para priorizar aquellos que son más específicas de un área (ciudad o estado) en concreto, penalizando palabras comunes como <emphasis>ya, hola, adiós, etc.</emphasis></simpara>
</note>

<simpara>Entre los resultados ofrecidos por el estudio destacan un <emphasis role="strong">72.7% de acierto para inferir el país</emphasis> de un usuario pero tan <emphasis role="strong">sólo un 30% de acierto a nivel de estado</emphasis>.</simpara>
<simpara>Como apunte adicional, es interesante una afirmación que se enuncia en el artículo cuando se hace referencia al nuevo campo de localización que la API Streaming de Twitter ofrece para adjuntar información geolocalizada (siempre que el usuario lo permita):</simpara>
<blockquote>
  
<simpara>First, our focus is on the geographic information revealed in the “location” field of user profiles, a type of geographic information that is prevalent across the Web 2.0 world. <emphasis role="strong">Second, we found that only 0.77% of our 62 million tweets contained this embedded location information</emphasis>.</simpara>
</blockquote>

<simpara>De 62 millones de tuits, únicamente un 0.77% (~= 477.400) contenían información geográfica adjunta.</simpara>
</section>
<section xml:id="_where_is_this_tweet_from_inferring_home_locations_of_twitter_users">
<title>Where Is This Tweet From? Inferring Home Locations of Twitter Users</title>
<simpara><emphasis>Por Jalal Mahmud et al. IBM Research</emphasis></simpara>
<simpara>Con el objetivo de poder identificar un tuit a diferentes granularidades: ciudad o estado, el estudio plantea la posibilidad de analizar tres tipos de términos diferentes para localizar una publicación en Twitter:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Palabras</simpara>
</listitem>
<listitem>
<simpara>Hashtags</simpara>
</listitem>
<listitem>
<simpara>Nombres de lugares (utilizando un gazetteer<footnote><simpara>Conjunto de nombres geográficos que, junto con un mapa, constituye una importante referencia sobre lugares y sus nombres</simpara></footnote> geográfico). Puesto que estos términos podía estar compuestos por más de una palabra, se utilizaron bigramas y trigamas, así como un heurístico especializado en reconocer nombres de lugares expresados mediante vocabulario común (un ejemplo sería <emphasis>Red Sox</emphasis> para referirse a la ciudad de Boston).</simpara>
</listitem>
</orderedlist>

<simpara>Es interesante observar como empiezan a aparecer pequeñas diferencias entre términos, considerando que en función de su categoría, pueden ofrecer más o menos información geográfica. Esta misma estrategia será también utilizada en el presente proyecto, mediante la extracción de Hashtags, Menciones y N-gramas.</simpara>
<simpara>Con el objetivo de minimizar la aparición de ruido, normalizaron el contenido de cada tuit eliminando signos de puntuación (a excepción de aquellos que indican una entidad propia cuando se encuentran al principio de una palabra, como <literal>#</literal> para indicar <emphasis>hashtags</emphasis>) y palabras vacías.</simpara>
<simpara>También en este estudio se hace mención a la utilización de un <emphasis role="strong">software de aprendizaje automático</emphasis>, en este caso WEKA, y su conjunción con un modelo estadístico que realice los cálculos necesarios para el clasificador. El modelo que seleccionaron de manera empírica fue un clasificador Bayesiano multinomial.</simpara>
<simpara>La estrategia propuesta en este trabajo para inferir la localización de un usuario en Twitter fue:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>A lo largo de sus tuits, mencionará más veces su ciudad o estado de origen que el resto de ciudades o estados.</simpara>
</listitem>
<listitem>
<simpara>Visitará más lugares de su ciudad o estado de origen que del resto de ciudades o estados (para detectar este tipo de visitas, se guardan todas las URLs generadas a partir de <emphasis>check-ins</emphasis> compartidos a través de <emphasis role="strong">Foursquare</emphasis> para luego comprobar su información asociada a través de la propia API de Foursquare).</simpara>
</listitem>
</orderedlist>

<simpara>A partir de estas premisas y de las decisiones anteriores, se crearon 3 modelos diferentes para poder entrenar sobre cada uno de los términos que se quieren extraer: palabras, hashtags y nombres de lugares. Los resultados presentados a nivel de ciudad no fueron realmente positivos, y sólo presentan niveles de precisión superiores al 70% cuando se permiten márgenes de error superiores a 200 millas (~= 322 kilómetros).</simpara>
<simpara>Por último, no se especifica con exactitud cómo actúa realmente el algoritmo cuando se trabaja con usuarios que no tienen contenido generado por Foursquare o no hacen una referencia explícita a su ciudad, estado o país.</simpara>
</section>
<section xml:id="_tweolocator_a_non_intrusive_geographical_locator_system_for_twitter">
<title>TweoLocator: A Non-Intrusive Geographical Locator System for Twitter</title>
<simpara><emphasis>Por Yi-Shin Chen et al. National Tsing Hua University</emphasis></simpara>
<simpara>En este estudio, Yi-Shin Chen diseña un sistema que a través de diferentes etapas y aglutinando varios procesos es capaz de inferir la localización de un usuario en Twitter en función del contenido de sus tuits.</simpara>
<variablelist>
<varlistentry>
<term>Baseline Classification</term>
<listitem>
<simpara>A partir de un gran dataset de usuarios de Twitter, en esta fase se realiza un análisis para comprobar qué perfiles puede ser potencialmente válidos para realizar un análisis de contenidos, eliminando aquellos que puedan pertenecer a <emphasis>bots</emphasis> automáticos o sean perfiles de spam. Una vez se obtiene una masa de usuarios válidos se procede, dentro aún de esta etapa, a analizar todos sus tuits (a excepción de aquellos con información de geolocalización asociada) para volver a categorizarlos en 3 tipos:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Direct subject</emphasis>: Tuits que hacen referencia al usuario en primera persona.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Anonymous subject</emphasis>: Tuits que no hacen una referencia directa al usuario, pero utilizan otros pronombres personales o la primera secuencia de palabras es un verbo que no es una palabra vacía.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Others</emphasis>: Tuits descartados por no pertenecer a ninguna de las 2 categorías anteriores.</simpara>
</listitem>
</itemizedlist>

</listitem>
</varlistentry>
<varlistentry>
<term>Rule Generation</term>
<listitem>
<simpara>Una vez todos los tuits anteriores han sido analizados semánticamente se realiza una normalización de los mismos aplicando técnicas de análisis de texto (utilizando un tokenizador y un stemmer) para luego poder formar n-gramas como los mismos. Durante esta etapa, se intentan inferir reglas que permitan asociar términos comunes a localizaciones específicas como aeropuertos, parques, estaciones de tren, etc.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Location Discovery</term>
<listitem>
<simpara>A partir de los términos de cada tuit, se generan trigramas, bigramas y unigramas y se comparan sobre un gazetteer y las reglas generadas en el paso anterior, obteniendo localizaciones que se pueden agrupar en:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Explicit Specific</emphasis>: Nombres que hacen una referencia directa a una ciudad o lugar determinado, como por ejemplo «The White House» or «Los Angeles».</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Explicit</emphasis>: Nombres que hacen referencia a localizaciones generales como parques o gimnasios.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Implicit</emphasis>: Combinaciones de palabras que implícitamente sugieren una localización. Estos resultaos se obtienen a partir de las reglas generadas en el paso anterior.</simpara>
</listitem>
</itemizedlist>

</listitem>
</varlistentry>
<varlistentry>
<term>Toponym Removal</term>
<listitem>
<simpara>Mediante la utilización de un clúster, y partiendo de la premisa de que un usuario nombrará con mayor frecuencia lugares cercanos a su lugar de origen, en esta fase se analizan las menciones realizadas por el usuario sobre ciudades, lugares, países y se refinarán los datos para obtener su lugar de origen.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Timeline Sorting</term>
<listitem>
<simpara>Es el último paso en el refinamiento de los datos. En esta fase se intenta minimizar la aparición de ruido detectando aquellas ocasiones en las que el usuario hace referencia a una localización geográfica sin aportar una información real acerca de su posición. Por ejemplo, es habitual que alguien situado en Asturias pueda nombrar la ciudad de Nueva York para hablar de alguna noticia o para mostrar sus ganas por conocer la ciudad, sin que esa mención indique que se encuentre realmente allí. Para resolver este problema, y aceptando que en algunos casos sólo se podrían resolver dichas inconsistencias de manera manual mediante la intervención humana, se diseñó un sistema que a partir de dos tuits con contenido geolocalizado consecutivos (del mismo usuario) compruebe si su diferencia en el tiempo es acorde a la posibilidad de haberse movido entre ambos puntos a una velocidad normal de transporte.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Location Inferred</term>
<listitem>
<simpara>De acuerdo a los resultados obtenidos en todas las fases anteriores y de acuerdo al nivel sobre que el que se haya podido inferir su localización, los usuarios son clasificados en los siguientes grupos:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">No information</emphasis>: Si no se ha podido obtener información geográfica válida para inferir la localización del usuario.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Just country</emphasis>: Si sólo se ha podido inferir el país del usuario.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Timeline</emphasis>: Se han podido detectar ubicaciones actuales y previas del usuario, pero no su lugar de origen.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Hometown</emphasis>: Se han podido detectar ubincaciones actuales y previas del usuario y <emphasis role="strong">también</emphasis> su lugar de origen. Es el grupo con información más completa.</simpara>
</listitem>
</itemizedlist>

</listitem>
</varlistentry>
</variablelist>

<simpara>En las conclusiones que se exponen en el artículo se muestran unos resultados bastante aceptables, donde hay porcentajes de acierto cercanos al 80%. Al igual que en el caso anterior, TweoLocator tiene una gran dependencia de que los usuarios incluyan en el contenido de sus tuits información explícitamente geolocalizable.</simpara>
</section>
<section xml:id="_a_multi_indicator_approach_for_geolocalization_of_tweets">
<title>A Multi-Indicator Approach for Geolocalization of Tweets</title>
<simpara><emphasis>Por Axel Schulz et al. SAP Research</emphasis></simpara>
<simpara>Presenta un sistema muy interesante mediante la utilización de formas poligonales en 3D para decidir la localización de un tuit y usuario. Cada polígono tiene dos valores de calidad en base al indicador que los define. Los polígonos se superponen, y la intersección de mayor altura es el área con más probabilidades de contener el tuit analizado.</simpara>
<simpara>Para obtener la información de cada tuit, utiliza varios sistemas como la <emphasis role="strong">DBPedia</emphasis> (<link xlink:href="http://dbpedia.org/">http://dbpedia.org/</link>) o <emphasis role="strong">Foursquare</emphasis> (<link xlink:href="https://es.foursquare.com/">https://es.foursquare.com/</link>) para reconocer entidades y topónimos. Su dependencia de sistemas externos impide que sea capaz de deducir una localización a través del contexto si esta no contiene ninguna referencia a una entidad localizable.</simpara>
</section>
<section xml:id="_inferring_the_origin_locations_of_tweets_with_quantitative_confidence">
<title>Inferring the Origin Locations of Tweets with Quantitative Confidence</title>
<simpara><emphasis>Por Reid Priedhorsky et al. Los Alamos National Laboratory y Northeastern Illinois University</emphasis></simpara>
<simpara>Este nuevo estudio presenta otra estrategia para encontrar términos fuertemente localizados en base al uso de <emphasis>n-gramas</emphasis>. Para ello, los investigadores se centraron en extraer bigramas de los siguientes campos:</simpara>
<itemizedlist>
<listitem>
<simpara>Campo de Localización</simpara>
</listitem>
<listitem>
<simpara>Contenido del tuit</simpara>
</listitem>
<listitem>
<simpara>Zona horaria</simpara>
</listitem>
<listitem>
<simpara>Idioma seleccionado en el perfil del usuario</simpara>
</listitem>
</itemizedlist>

<simpara>A su vez, desarrollaron un modelo estadístico propio, basado en un clasificador Gaussiano, el cual aplicaban sobre aquellos bigramas que superaban un número mínimo de apariciones.</simpara>
</section>
<section xml:id="_otros_emphasis_papers_emphasis_de_inter_s">
<title>Otros <emphasis>papers</emphasis> de interés</title>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">You Are Where You Tweet: A Content-Based Approach to Geo-locating Twitter Users</emphasis> <emphasis>por Zhiyuan Cheng et al. Texas A&amp;M University</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Location Type Classification Using Tweet Content</emphasis> <emphasis>por Haibin Liu et al. The Pennsylvania State University</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">TweetLocalize: Inferring Author Location in Social Media</emphasis> <emphasis>por Evan Sparks et al. University of California-Berkeley</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Inferring the Location of Twitter Messages Based on User Relationships</emphasis> <emphasis>por Clodoveu A. Davis Jr. et al. Universidade Federal de Minas Gerais</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Geolocation Prediction in Social Media Data by Finding Location Indicative Words</emphasis> <emphasis>por HAN Bo et al. University of Melbourne</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Home Location Identification of Twitter Users</emphasis> <emphasis>por Jalal Mahmud et al. IBM Research</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Geotagging One Hundred Million Twitter Accounts with Total Variation Minimization</emphasis> <emphasis>por Ryan Compton et al. HRL Laboratories (Malibu)</emphasis></simpara>
</listitem>
</itemizedlist>

</section>
</section>
<section xml:id="_aplicaciones_web">
<title>Aplicaciones web</title>
<simpara>A continuación, se recopilan algunas aplicaciones con características similares a las del presente proyecto, y otras que, sin centrarse en el estudio de la geolocalización del contenido de manera específica, utilizan <emphasis role="strong">Twitter</emphasis> como fuente de información y realizan un estudio a gran escala sobre las publicaciones realizadas.</simpara>
<section xml:id="_trendsmap">
<title>Trendsmap</title>
<simpara><emphasis role="strong">Trendsmap</emphasis> (<link xlink:href="http://trendsmap.com/">http://trendsmap.com/</link>) es una aplicación web que muestra en tiempo real las tendencias en Twitter para cada localización a escala mundial. Según datos ofrecidos por la propia web en su página de <emphasis>FAQ</emphasis>, gestionan un volumen de tuits del orden de <emphasis role="strong">80 millones al día</emphasis>.</simpara>
<simpara>Su objetivo principal consiste, por tanto, en localizar las tendencias específicas de cada localización, sin ofrecer ninguna información acerca de las posibilidades de que uno u otro tuit que contengan dicha tendencia puedan pertenecer a una localización en concreto.</simpara>
<simpara>No se ha podido encontrar ninguna información acerca del tipo de algoritmo que utiliza Trendsmap para determinar la localización de un tuit (en pos de poder determinar la tendencia para un alto número de los mismos).</simpara>
</section>
<section xml:id="_what_the_trend">
<title>What the Trend</title>
<simpara><emphasis role="strong">What the Trend</emphasis> (<link xlink:href="http://whatthetrend.com/faq">http://whatthetrend.com/faq</link>) se centra en ofrecer al usuario una explicación acerca de los propios <emphasis>Trending Topics</emphasis> identificados por Twitter para cada localidad.</simpara>
<simpara>En este caso, la aplicación no incluye ningún tipo de algoritmo para adivinar la localidad de un volumen de tuits, si no que únicamente recoge las tendencias previamente analizadas y localizadas por Twitter.</simpara>
</section>
<section xml:id="_klout">
<title>Klout</title>
<simpara><emphasis role="strong">Klout</emphasis> (<link xlink:href="http://klout.com/home">http://klout.com/home</link>) se describe como un servicio capaz de obtener la influencia de un usuario en la red a través de sus publicaciones y relaciones en redes sociales. Durante sus primeros años fue objetivo de varias inversiones millonarias que sacaron a la luz la gran importancia que tiene a nivel empresaria el análisis de los grandes volúmenes de información que se generan a diario en las redes sociales por parte de los propios usuarios.</simpara>
</section>
</section>
</section>
<section xml:id="_aspectos_te_ricos">
<title>Aspectos teóricos</title>
<simpara>Aquí los aspectos teóricos.</simpara>
</section>
</chapter>
<chapter xml:id="_planificaci_n_y_gesti_n_del_proyecto">
<title>Planificación y gestión del proyecto</title>

</chapter>
<chapter xml:id="_an_lisis_del_sistema">
<title>Análisis del sistema</title>

</chapter>
<chapter xml:id="_desarrollo_e_implementaci_n_del_sistema">
<title>Desarrollo e implementación del sistema</title>

</chapter>
<appendix xml:id="_falcon_sistema_para_coleccionar_datos_de_la_api_streaming_de_twitter">
<title>Falcon, sistema para coleccionar datos de la API Streaming de Twitter</title>
<simpara>El primer paso en el desarrollo del proyecto, fue la creación de un sistema capaz de recolectar tuits con el objetivo de obtener material de entrenamiento sobre el que aplicar las diferentes estrategias planteadas.</simpara>
<simpara>Este sistema debía ser parametrizable, con el objetivo de poder configurar en cada ejecución el tipo de tuits que se querían obtener. En base a estos requisitos, se utilizó la <emphasis role="strong">API Streaming de Twitter</emphasis> (<link xlink:href="https://dev.twitter.com/docs/api/streaming">https://dev.twitter.com/docs/api/streaming</link>), la cual aporta dos características principales:</simpara>
<itemizedlist>
<listitem>
<simpara>Capacidad de obtener datos en tiempo real de Twitter de manera ininterrumpida</simpara>
</listitem>
<listitem>
<simpara>Capacidad de establecer filtros sobre el flujo de entrada:</simpara>
<itemizedlist>
<listitem>
<simpara>Filtro por idioma del tuit</simpara>
</listitem>
<listitem>
<simpara>Filtro por localización del tuit (mediante el uso de <emphasis>bounding boxes</emphasis>)</simpara>
</listitem>
</itemizedlist>

</listitem>
</itemizedlist>

<simpara>Para realizar la conexión entre el sistema desarrollado y la API de Twitter se utilizó la biblioteca <emphasis role="strong">Twitter4j</emphasis> (<link xlink:href="http://twitter4j.org/en/index.html">http://twitter4j.org/en/index.html</link>), la cual aunque originalmente está desarrollada para ser utilizada sobre Java, es perfectamente utilizable en Scala gracias a la compatibilidad entre ambos lenguajes mediante la Java Virtual Machine. La ventajas de utilizar una biblioteca construida sobre la API original de Twitter es que algunos de los problemas más habituales se solucionan a través de nuevas capas de abstracción:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Autenticación OAuth2 simplificada mediante clases propias de la biblioteca</simpara>
</listitem>
<listitem>
<simpara>La API de Twitter4J para utilizar el Streaming de Twitter permite aislar al desarrollador de la necesidad de mantener activa la comunicación HTTP manualmente para estar conectado al Streaming de Twitter.</simpara>
</listitem>
</orderedlist>

<figure>
<title>Comunicación entre un cliente y la API Streaming de Twitter (<link xlink:href="https://dev.twitter.com/docs/api/streaming">https://dev.twitter.com/docs/api/streaming</link>)</title>
  <mediaobject>
    <imageobject>
      <imagedata fileref="../assets/05development/twitter-streaming-api.png"/>
    </imageobject>
    <textobject><phrase>Modelo de comunicación entre un cliente y la API Streaming de Twitter</phrase></textobject>
  </mediaobject>
</figure>

<section xml:id="_almacenamiento_de_datos">
<title>Almacenamiento de datos</title>
<simpara>Uno de los puntos más importantes que planteó el sistema para recolectar tuits era en qué formato sería más adecuado serializar los datos obtenidos.</simpara>
<simpara>En un primer momento se barajó la posibilidad de utilizar el formato CSV, el cual permitiría acceder de manera rápida al número de tuits guardados y realizar operaciones sencillas en línea de comandos mediante operaciones <literal>grep</literal>. Esta decisión fue cancelada al realizar los primeros experimentos y comprobar como el guardado de ciertos datos en formato CSV presenta muchas dificultades para poder solventar todos los casos esquina que se presentan con la aparición de contenido complejo que pueda incluir comas, comillas y otros signos de puntuación (aún en el caso de utilizar bibliotecas especializadas como OpenCSV - <link xlink:href="http://opencsv.sourceforge.net/">http://opencsv.sourceforge.net/</link> ) combinados con caracteres extraños como Emoji (<link xlink:href="http://www.unicode.org/faq/emoji_dingbats.html">http://www.unicode.org/faq/emoji_dingbats.html</link>).</simpara>
<simpara>Como consecuencia de los resultados anteriores, y apoyado en el soporte nativo ofrecido por Scala, se utilizó XML como el lenguaje de marcado que mejor podría serializar y estructurar los datos obtenidos a través de Twitter4j. El siguiente fragmento de código permite ver lo sencillo que es serializar un objeto en Scala a XML mediante la utilización de literales:</simpara>
<programlisting language="scala" linenumbering="unnumbered">class Tweet(id:String, username: String, name:String, location: String, timezone: String, createdAt:String, latitude: String,
            longitude: String, text: String) {
  def toXML =
    &lt;tweet&gt;
      &lt;id&gt;
        {id}
      &lt;/id&gt;
      &lt;username&gt;
        {username}
      &lt;/username&gt;
      &lt;name&gt;
        {name}
      &lt;/name&gt;
      &lt;location&gt;
        {location}
      &lt;/location&gt;
      &lt;timezone&gt;
        {timezone}
      &lt;/timezone&gt;
      &lt;createdAt&gt;
        {createdAt}
      &lt;/createdAt&gt;
      &lt;latitude&gt;
        {latitude}
      &lt;/latitude&gt;
      &lt;longitude&gt;
        {longitude}
      &lt;/longitude&gt;
      &lt;text&gt;
        {text}
      &lt;/text&gt;
    &lt;/tweet&gt;
}</programlisting>

</section>
<section xml:id="_par_metros_del_sistema">
<title>Parámetros del sistema</title>
<simpara>Debido a que Falcon es un sistema sin ánimo de ejecutarse a través de una GUI, la manera de parametrizar la ejecución ha sido a través de una interfaz de línea de comandos. Para ello, se ha utilizado la biblioteca <emphasis role="strong">scopt</emphasis> (<link xlink:href="https://github.com/scopt/scopt">https://github.com/scopt/scopt</link>).</simpara>
<simpara>scopt permite parsear de manera sencilla los argumentos que se le pasan al programa en el momento de su ejecución. Para ello, simplemente hay que definir un objeto <literal>ScoptParser</literal> que contenga las reglas necesarias para especificar qué parámetros se esperan, qué tipo deben tener (<literal>String</literal>, <literal>Integer</literal>, <literal>Boolean</literal>) y si son requeridos u opcionales.</simpara>
<simpara>A continuación se muestra el <emphasis>usage</emphasis> de la aplicación:</simpara>
<screen>Falcon 1.0
Usage: Falcon [options]

  -l &lt;value&gt; | --language &lt;value&gt;
        Specifies the language, in ISO 639-1 format, for the tweets to collect.
  -s &lt;value&gt; | --stopwords &lt;value&gt;
        Specifies the file for the stopwords.
  -t &lt;value&gt; | --time-in &lt;value&gt;
        The time measure for collecting tweets (SECONDS, MINUTES, HOURS, DAYS).
  -n &lt;value&gt; | --timestamp &lt;value&gt;
        Units of time for collecting tweets.
  -o &lt;value&gt; | --output &lt;value&gt;
        The output filename where store the collection results.
  -c &lt;value&gt; | --credentials &lt;value&gt;
        Properties file with the Twitter credentials
  --coordinates-mandatory
        This flag indicates whether every tweet must have a geolocation tag associated or not.
  -b &lt;value&gt; | --bounding-boxes &lt;value&gt;
        Specifies the file which contains the bounding boxes.</screen>

<itemizedlist>
<listitem>
<simpara><literal>--time-in</literal> y <literal>--timestamp</literal>: permiten establecer al recolector un tiempo de ejecución representado en diferentes magnitudes.</simpara>
</listitem>
<listitem>
<simpara><literal>--output</literal>: nombre del fichero de salida.</simpara>
</listitem>
<listitem>
<simpara><literal>--coordinates-mandatory</literal>: su presencia indica si los tuits recolectados deben contener o no información geográfica adjunta.</simpara>
</listitem>
<listitem>
<simpara><literal>--language</literal>: idioma en el que se desean obtener los tuits.</simpara>
</listitem>
<listitem>
<simpara><literal>--stopwords</literal>: debido a restricciones de Twitter, es necesario proveer una lista de términos cuando se intenta realizar un filtrado por idioma. Con el objetivo de restringir lo mínimo posible el número de tuits a obtener, se provee una lista de <emphasis>stop words</emphasis> del idioma por el que se esté filtrando.</simpara>
</listitem>
<listitem>
<simpara><literal>--bounding-boxes</literal>: opcionalmente, se puede proveer un fichero <literal>XML</literal> que contiene los <emphasis>bounding boxes</emphasis> sobre los que se realizará el filtrado para aquellas ejecuciones que requieran tuits localizados en un área en concreto.</simpara>
</listitem>
<listitem>
<simpara><literal>--credentials</literal>: indica el fichero que contiene las credenciales que se utilizarán para conectarse con la API de Twitter y utilizar su servicio de Streaming.</simpara>
</listitem>
</itemizedlist>

<simpara>Un ejemplo de uso para obtener datos en español sobre los bounding boxes de España durante un día, podría ser algo como esto:</simpara>
<screen>falcon.jar -l es -s es_stop_words.txt -t DAYS -n 1 -o es_tweets_collection -c credentials.properties -b spain_bounding_boxes.xml</screen>

</section>
<section xml:id="_ejemplo_de_resultados">
<title>Ejemplo de resultados</title>
<simpara>Un ejemplo de los resultados obtenidos por el recolector sería el siguiente:</simpara>
<programlisting language="xml" linenumbering="unnumbered">&lt;tweets&gt;
  &lt;tweet&gt;
    &lt;username&gt;
      gaabriforner
    &lt;/username&gt;
    &lt;location&gt;
      Málaga
    &lt;/location&gt;
    &lt;timezone&gt;
      Athens
    &lt;/timezone&gt;
    &lt;createdAt&gt;
      2014-03-04 21:53
    &lt;/createdAt&gt;
    &lt;latitude&gt;
      -4.437747
    &lt;/latitude&gt;
    &lt;longitude&gt;
      36.7055494
    &lt;/longitude&gt;
    &lt;text&gt;
      y ante todo a echarle fuerza d voluntad y ganas para conseguir lo que quiero!!
    &lt;/text&gt;
  &lt;/tweet&gt;
&lt;/tweets&gt;</programlisting>

<important>
<title>Bug en Twitter4j</title>
<simpara>En fases posteriores del desarrollo del proyecto salió a la luz un bug por parte de la biblioteca Twitter4j que produce que la latitud y longitud de cada tuit se devuelvan de manera inversa. Por tanto, aquellos valores que se estaban considerando como latitud eran realmente la longitud y viceversa.</simpara>
<simpara>Este error no tuvo un gran impacto, puesto que fue fácilmente detectable y, una vez sabido, se tomaron las medidas adecuadas para tratar los datos correctamente.</simpara>
<simpara>A continuación se muestra el enlace donde se hace alusión al bug: <link xlink:href="https://groups.google.com/forum/#!topic/twitter4j/Kp-gqzBJIxE">https://groups.google.com/forum/#!topic/twitter4j/Kp-gqzBJIxE</link> el cual está presente tanto para la versión <literal>3.0.4-SNAPSHOT</literal> como en anteriores versiones.</simpara>
</important>

</section>
</appendix>
<appendix xml:id="_b2pick_aplicaci_n_web_para_obtener_emphasis_bounding_boxes_emphasis">
<title>b2pick, aplicación web para obtener <emphasis>bounding boxes</emphasis></title>

</appendix>
<appendix xml:id="_puma_sistema_de_an_lisis_de_datos">
<title>Puma, sistema de análisis de datos</title>
<simpara>Para el análisis de los datos recogidos mediante el recolector, fue necesario desarrollar un nuevo sistema preparado para trabajar con grandes volúmenes de datos y capaz de aplicar los algoritmos planteados en estudios anteriores para obtener vocabulario discriminativo. Este sistema fue bautizado con el nombre de <emphasis role="strong">Puma</emphasis>, como una manera rápida de poder referirse a él.</simpara>
<section xml:id="__qu_datos_se_deben_estudiar">
<title>¿Qué datos se deben estudiar?</title>
<simpara>Con el objetivo de poder encontrar vocabulario discriminativo, se comenzaron a descargar datos, durante 4 semanas, con las siguientes características:</simpara>
<variablelist>
<varlistentry>
<term>Semana 1</term>
<listitem>
<simpara>Tuits en español para todo el mundo</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Semana 2</term>
<listitem>
<simpara>Tuits en español localizados en España</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Semana 3</term>
<listitem>
<simpara>Tuits en inglés para todo el mundo</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Semana 4</term>
<listitem>
<simpara>Tuits en inglés localizados en UK</simpara>
</listitem>
</varlistentry>
</variablelist>

<simpara>La idea será comparar datasets con el mismo idioma pero de distintas localizaciones para obtener, mediante la aplicación de ciertos algoritmos, aquellos vocabularios más geográficamente geolocalizados.</simpara>
</section>
<section xml:id="_sistema_de_filtros">
<title>Sistema de filtros</title>
<simpara>Para poder comenzar a trabajar con los datos recopilados por el sistema Falcon, fue necesario definir qué tipo de información se quería extraer de cada tuit con el objetivo de realizar las operaciones necesarias sobre ella. Se definieron los siguientes términos como los más interesantes para encontrar vocabulario discriminativo:</simpara>
<itemizedlist>
<listitem>
<simpara>Menciones</simpara>
</listitem>
<listitem>
<simpara>Hashtags</simpara>
</listitem>
<listitem>
<simpara>Bigramas</simpara>
</listitem>
<listitem>
<simpara>Keywords</simpara>
</listitem>
<listitem>
<simpara>Keywords en el campo de localización del usuario</simpara>
</listitem>
</itemizedlist>

<simpara>Con el objetivo de poder generar un sistema flexible y personalizable, se diseñó un sistema de filtros que permita combinar la extracción de varios filtros por ejecución. Este diseño se basó en una implementación adaptada del patrón Decorator<footnote><simpara><link xlink:href="http://en.wikipedia.org/wiki/Decorator_pattern">http://en.wikipedia.org/wiki/Decorator_pattern</link></simpara></footnote>.</simpara>
<simpara>Debido a la naturaleza del problema, en este caso no tiene sentido que las operaciones de una extracción se basen en la extracción anterior, por tanto, en esta implementación personalizada, cada extracción se basa en el tuit original y acumula los resultados en una lista común.</simpara>
<figure>
<title>Representación del patrón Decorator que ilustra el sistema de filtros</title>
  <mediaobject>
    <imageobject>
      <imagedata fileref="../assets/appendixes/extractor-filter.png"/>
    </imageobject>
    <textobject><phrase>Representación del patrón Decorator que ilustra el sistema de filtros</phrase></textobject>
  </mediaobject>
</figure>

</section>
<section xml:id="_likelihood_ratio_test_y_likelihood_ratio_test_normalizado">
<title>Likelihood-ratio test y Likelihood-ratio test normalizado</title>

</section>
<section xml:id="_algoritmo_de_poda_y_control_del_uso_de_memoria">
<title>Algoritmo de poda y control del uso de memoria</title>

</section>
<section xml:id="_generaci_n_de_puntuaciones">
<title>Generación de puntuaciones</title>

</section>
<section xml:id="_caso_de_uso_y_flujo_completo_del_sistema_de_an_lisis">
<title>Caso de uso y flujo completo del sistema de análisis</title>
<simpara>Partiendo de dos conjuntos de datasets (uno global y otro local) con los siguientes tuits de ejemplo (se ha simplificado la estructura de los tuits ajustándolos a aquellas características importantes para ilustrar el ejemplo):</simpara>
<simpara><emphasis role="strong">LOCAL (ASTURIAS)</emphasis></simpara>
<screen>
username: cris_cerra
name: cristina cerra
location: Ribadesella; Gijón
latitude: -5.6653517
longitude: 43.5279244
tweet: No me disgusta economia de la empresa, pero tampoco entiendo a que fin lo tengo que estudiar yo…  Mas sinsentido imposible…


username: LorenaGS93
name: Lorena González
location: Moreda, Aller
latitude: -5.73753988
longitude: 43.168336
tweet: Conocemos lo que hacemos, pero desconocemos lo que podemos llegar a hacer

username: Luciantomil
name: ∴
location: Oviedo
latitude: -5.8362912
longitude: 43.3740172
tweet: Tanto quejarme de lo de espiar el #Wa y ahora me pasa a mi @LorenaGS93.
</screen>

<simpara><emphasis role="strong">GLOBAL (ESPAÑA SIN ASTURIAS)</emphasis></simpara>
<screen>
username: elenita9614
name: Elenaa
location: España, Madrid
latitude: -3.6598875
longitude: 40.4746948
tweet: Siempre en el amor hay un poco de locura pero en la locura hay un poco de razón.

username: xtian_herrero
name: Christian Herrero
location: Al sur del sur.
latitude: -5.15107127
longitude: 36.42808127
tweet: Copiando a mi amigo pacojarillo con su asamblea laminar. Me recuerda a #starwars #backlighting @… http://t.co/XQ5GGUrq9X

username: AlisTwittah
name: Ali
location:
latitude: -6.1914368
longitude: 36.4725925
tweet: VIVA EL BETIS
</screen>

<simpara>El algoritmo encargado de generar LLR recorrería cada uno de los dos ficheros aplicando los siguientes pasos:</simpara>
<simpara><emphasis role="strong">1.</emphasis> Para cada entidad Tweet se extraen aquellos términos estipulados por el usuario (Bigramas, Menciones, Hashtags, Keywords o Keywords en el campo de Localización)</simpara>
<simpara>A continuación se muestran los efectos de aplicar todos los filtros sobre uno de los tweets de prueba:</simpara>
<simpara><emphasis role="strong">ORIGINAL</emphasis></simpara>
<screen>username: Luciantomil
name: ∴
location: Oviedo
latitude: -5.8362912
longitude: 43.3740172
tweet: Tanto quejarme de lo de espiar el #Wa y ahora me pasa a mi @LorenaGS93.</screen>

<simpara><emphasis role="strong">PROCESADO</emphasis></simpara>
<screen>Menciones: @lorenags93
Hashtags: #wa
Keywords: tanto, quejarme, espiar, ahora, pasa
Bigramas: quejarme tanto, espiar tanto, ahora tanto, pasa tanto, espiar quejarme, ahora quejarme, pasa quejarme, ahora espiar, espiar pasa, ahora pasa
KW en Localización: oviedo</screen>

<simpara>Como se pueda observar, se aplica una normalización a cada tweet que implica transformar todo el contenido a minúsculas así como aplicar un filtro que elimina stop words o signos de puntuación. En el caso de los Bigramas, además, se realiza una ordenación por orden alfabético.</simpara>
<simpara><emphasis role="strong">2.</emphasis> Cada término extraído por tuit, se añade a un Hash que contabiliza su número de apariciones en total para ese dataset. Se ha creado un algoritmo de poda agresiva que permita controlar la cantidad de memoria utilizada por el proceso. Para ello, cuando el número de términos alcanza un umbral (configurado por el usuario) se realiza un truncado de los términos con menos apariciones del Hash en un porcentaje definido por el usuario. Para minimizar el impacto de la poda, se controla cual era la mayor de las frecuencias que tenían los términos que se eliminaron, de esa manera, los nuevos términos empezarán directamente en esa frecuencia mínima.</simpara>
<simpara><emphasis role="strong">3.</emphasis> Una vez ambos datasets tienen un Hash de resultados que contiene todos los términos con mayor número de apariciones, es necesario recorrer cada dataset para ver qué frecuencia tienen los términos del dataset local en el dataset global. Un caso que se puede presentar, es que ciertos términos aparezcan en el dataset local y <emphasis role="strong">no</emphasis> en el global, para ello, se utiliza un algoritmo en el que se buscan todos aquellos términos con la misma frecuencia en el dataset local <emphasis role="strong">pero</emphasis> que tengan correspondencia también en el dataset global y se calcula su media, aplicando este valor como frecuencia para el término sin correspondencia en el dataset global.</simpara>
<simpara>Por ejemplo, supongamos que estamos analizando un dataset local geolocalizado en Asturias, y aparece el término <emphasis>escanciar</emphasis> con una frecuencia de 10 apariciones. Después observamos que no existen correspondencia en el dataset global para ese término, por lo que buscamos en el dataset local otros términos con el mismo número de apariciones pero que <emphasis role="strong">sí</emphasis> tenga frecuencia asignada en el dataset global:</simpara>
<informaltable frame="all"
    rowsep="1" colsep="1">
  
  <tgroup cols="3">
    
    <colspec colname="col_1" colwidth="33*"/>
    
    <colspec colname="col_2" colwidth="33*"/>
    
    <colspec colname="col_3" colwidth="33*"/>
    
    
    <thead>
      
      <row>
        
        <entry align="left" valign="top">Término</entry>
        
        <entry align="left" valign="top">Local</entry>
        
        <entry align="left" valign="top">Global</entry>
        
      </row>
      
    </thead>
    
    <tbody>
      
      <row>
        
        <entry align="left" valign="top"><simpara>oviedo</simpara></entry>
        
        <entry align="left" valign="top"><simpara>10</simpara></entry>
        
        <entry align="left" valign="top"><simpara>4</simpara></entry>
        
      </row>
      
      <row>
        
        <entry align="left" valign="top"><simpara>sidra</simpara></entry>
        
        <entry align="left" valign="top"><simpara>10</simpara></entry>
        
        <entry align="left" valign="top"><simpara>3</simpara></entry>
        
      </row>
      
      <row>
        
        <entry align="left" valign="top"><simpara>culín</simpara></entry>
        
        <entry align="left" valign="top"><simpara>10</simpara></entry>
        
        <entry align="left" valign="top"><simpara>5</simpara></entry>
        
      </row>
      
    </tbody>
    
  </tgroup>
</informaltable>

<simpara>Como resultado, el término <emphasis>escanciar</emphasis> pasaría a tener: <literal>(4 + 3 + 5)/3 = 4</literal> como valor de frecuencia</simpara>
<simpara><emphasis role="strong">4.</emphasis> Para cada par de frecuencias, se aplica el logaritmo del test estadístico denominado: <emphasis role="strong">Likelihood ratio test</emphasis> (<link xlink:href="http://www.itl.nist.gov/div898/handbook/apr/section2/apr233.htm">http://www.itl.nist.gov/div898/handbook/apr/section2/apr233.htm</link>), el cual es capaz de devolver un valor aproximado del impacto que un determinado término tiene sobre el dataset global (indicando si es más o menos discriminativo). El agoritmo utilizado es una implementación normalizada desarrollada por el doctor Daniel Gayo-Avello.</simpara>
<simpara><emphasis role="strong">5.</emphasis> Una vez se obtiene el valor LLR para cada término, se guarda en un Hash de resultados que asocia <literal>Término -&gt; LLR</literal>. Por último, se serializa en un fichero TSV donde cada línea contiene un término concreto y su valor LLR asociado para esa pareja de datasets. Un ejemplo de fichero de salida sería el siguiente:</simpara>
<screen>52.096630093774245  principado asturias
57.068998944725266  gracies besin
63.80508052413473   héctor simón
63.80508052413473   asturias oviedo
63.80508052413473   oviedo principado
63.80508052413473   muches besin</screen>

<simpara>Los valores más positivos indican una mayor discriminación. En este caso, los resultados pertenecen a extraer bigramas sobre un conjunto de datasets entre los que se quería sacar los términos más discriminativos para la provincia de Asturias (a fecha de 17 de Mayo de 2014).</simpara>
<simpara><emphasis role="strong">6.</emphasis> Una vez con estos resultados, lo habitual es poner a descargar otras 24h de tuits de acuerdo al área global específicada y, después, recorrer los nuevos tuits obtenidos uno a uno para extraer todos sus términos y comprobar que puntuación LLR asignada tienen en los ficheros TSV generados en la versión anterior. De acuerdo al extracto de puntuaciones anterior, un tuit como el siguiente:</simpara>
<screen>Estoy con Héctor y Simón en Parque Principado de Oviedo. Tenía muches ganas de verlos. 1 besin.</screen>

<simpara>Tendría la siguiente puntuación:</simpara>
<screen>63.80508052413473 + 57.068998944725266 + 63.80508052413473 + 63.80508052413473 ~= 248,483</screen>

</section>
<section xml:id="_configuraci_n_del_sistema">
<title>Configuración del sistema</title>

</section>
</appendix>
<appendix xml:id="_tweetheat_mapa_de_calor_sobre_ficheros_tsv_de_puntuaci_n">
<title>TweetHeat, mapa de calor sobre ficheros TSV de puntuación</title>

</appendix>
<appendix xml:id="_entrenamiento_del_modelo_de_an_lisis_de_datos">
<title>Entrenamiento del modelo de análisis de datos.</title>
<simpara>Un objetivo muy importante del proyecto es obtener la habilidad de automatizar el sistema de inferir la localización para poder trabajar sobre datos que no tengan una etiqueta de geolocalización asociada. Para ello, es necesario recurrir a software especializado, denominado habitualmente como <emphasis role="strong">software de aprendizaje automático</emphasis>.</simpara>
<simpara>Vowpal Wabbit<footnote><simpara><link xlink:href="http://hunch.net/~vw/">http://hunch.net/~vw/</link></simpara></footnote> es un proyecto creado originalmente por <emphasis>Yahoo! Research</emphasis> que fue posteriormente continuado por <emphasis>Microsoft Research</emphasis>, consistente en la creación de un nuevo algoritmo de aprendizaje automático rápido y escalable que permita trabajar con grandes cantidades de datos. A partir de una serie de información específicamente preparada para ser consumida por Vowpal Wabbit, este es capaz de crear un propio modelo de datos que sirva como base de entrenamiento para después, mediante la aplicación de diversos algoritmos, predecir un resultado en base al conocimiento adquirido en ejecuciones anteriores.</simpara>
<simpara>Además de la velocidad y precisión de los resultados que puede ofrecer Vowpal Wabbit, una de sus características más importantes es la capacidad de funcionar como un demonio del sistema e ir aprendiendo <emphasis>en caliente</emphasis> a través de nuevos modelos de datos. Esto, a diferencia de otros software de aprendizaje automático, permite que el sistema pueda adquirir un conocimiento incremental y ofrecer mejores resultados a medida que pasa el tiempo.</simpara>
<section xml:id="_normalizaci_n_de_datos">
<title>Normalización de datos</title>
<simpara>Para aprovechar toda la potencia y velocidad que ofrece Vowpal Wabbit, el primer paso es crear ficheros de entrada que estén estructurados de acuerdo a un formato optimizado para el clasificador. En este caso, Vowpal Wabbit espera datos de entrada estructurados de la siguiente manera:</simpara>
<screen>[Label] [Importance [Tag]]|Namespace Features |Namespace Features ... |Namespace Features <co xml:id="CO1-1"/></screen>

<calloutlist>
  
  <callout arearefs="CO1-1">
    <para><link xlink:href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format">https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format</link></para>
    
  </callout>
  
</calloutlist>

<simpara>En el presente proyecto, esta estructura generaría datos de la siguiente manera:</simpara>
<screen>1 |Tweet @adrianzenb scl rainer wirth óscar amigos radio quillota
2 |Tweet @thomasuribe medellín colombia celebra día hombre
3 |Tweet @fvminajx @cursiperono mujer ruega punto</screen>

<simpara>Debido al dominio específico del problema de este proyecto, el sistema que debe realizar la traducción entre los ficheros de puntuación generados por el sistema Puma y los datos que espera recibir Vowpal Wabbit, debía de ser parametrizable para poder cubrir los siguientes dos casos:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Entrenar un modelo de clasificación binario donde sólo se indicara si un tuit pertenece o no a un conjunto de coordenadas (útil para modelos que se quieran utilizar para saber si un tuit en español pertenece o no a España).</simpara>
</listitem>
<listitem>
<simpara>Entrenar un modelo de clasificación multiclase donde se agruparan las clases en torno a un número de decimales para la latitud y longitud. Esto permite que se pueden considerar de la misma clase todos los tuits cuyas coordenadas con 3 decimales sean las mismas, permitiendo obtener predicciones con un grado de precisión de 10 kilómetros (en caso de agrupar por 2 decimales, la precisión sería de 100 y con 1, de 1000 kilómetros).</simpara>
</listitem>
</orderedlist>

<simpara>Para ello, se creo un sistema que mediante una interfaz de línea de comandos pudiese ser parametrizable mediante los siguientes argumentos:</simpara>
<screen>vw-input-translator 1.0
Usage: vw-input-translator [options] &lt;file&gt;

  --inout &lt;value&gt;
        Creates an input file for binary classification
  -d &lt;value&gt; | --decimals &lt;value&gt;
        Creates an input file for multi-class classification. Each sample will have the selected number of decimals on latitude and longitude coordinates
  &lt;file&gt;
        Source file</screen>

</section>
<section xml:id="_divisi_n_de_datos_en_conjuntos_de_entrenamiento_y_test">
<title>División de datos en conjuntos de entrenamiento y test</title>
<simpara>Con el objetivo de entrenar al clasificador, se desarrolló un script capaz de, a partir de los datos de entrada que recibiría Vowpal Wabbit, dividir el conjunto en dos para dedicar una parte al proceso de entrenamiento y otra a probar el modelo de datos generados.</simpara>
<simpara>Con el objetivo de generar dos conjuntos consistentes, la división se realizó en base a los usuarios, haciendo que un mismo usuario no pudiese formar parte de ambos grupos. Un 80% de los usuarios fue destinado al grupo de entrenamiento, mientras que el 20% restante fue a parar al conjunto de test.</simpara>
</section>
<section xml:id="_estudio_de_los_primeros_resultados_y_cambio_de_estrategia">
<title>Estudio de los primeros resultados y cambio de estrategia</title>

</section>
</appendix>
</book>