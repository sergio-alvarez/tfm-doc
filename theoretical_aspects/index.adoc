:imagesdir: ../assets

== Algoritmos y aspectos teóricos

=== Log Likelihood-Ratio test

El test Log Likelihood-Ratio (http://www.itl.nist.gov/div898/handbook/apr/section2/apr233.htm) es un método estadístico ampliamente utilizado en problemas donde se pretenden comparar dos conjuntos de datos a través de una serie de supuestos.

En el caso del presente proyecto, la idea principal es aplicar LLR sobre los dos conjuntos de datos que contienen, por un lado, los tuits que se han recogido para el área local sobre el que se quieren comenzar a inferir tuits, y por el otro, un conjunto de tuits localizado en un área que se podría entender como global respecto al conjunto de datos local (por lo general, las áreas globales son localizaciones donde se habla el mismo idioma que en el área local, pero abarca otros territorios).

En este caso, partimos del *supuesto* de que el área local es un caso especializado del área global, el cual se puede diferenciar por los términos que contiene. Sobre este supuesto, el test Log Likelihood-Ratio nos devolverá un valor con la probabilidad de que cada término del área local sea realmente discriminativo comparando su frecuencia también en el conjunto de datos globales. Cuanto más discriminativo sea el término, mayor será su valor LLR. Sin embargo, todos aquellos que no sirvan para diferenciar al conjunto de datos especializado, tendrán un valor negativo.

El valor LLR de cada término se utilizará para calcular la probabilidad de que los nuevos tuits pertenezcan o no al área local mediante un sumatorio de todos los valores para cada término que forma el tuit.

==== Log Likelihood-Ratio test normalizado

=== Términos discriminativos

Se consideran términos discriminativos aquellos que son capaces de aportar información muy geolocalizable de manera implícita. Un ejemplo son aquellas palabras muy propias de una localización en concreto, como el caso del término _sidra_ o _carbayu_, que con mucha probabilidad indican un contenido que ha sido generado en Asturias.

La estrategia planteada en este proyecto está basada en descubrir este tipo de términos a través de los tuits que los usuarios publican para una determinada región (país, estado, ciudad, etc.). Para ello, la premisa básica es aceptar que los términos más discriminativos tenderán a tener un epicentro muy significativo donde su frecuencia es muy elevada, para después, no tener apenas dispersión y ser muy poco frecuentes en el resto.

Con el objetivo de poder aplicar los algoritmos anteriores, será necesario trabajar siempre con dos datasets sobre los que establecer la comparativa. Por un lado, se trabajará con un conjunto de datos localizados en el área en concreto que se quiera analizar, y por otro lado, otro conjunto de datos que se establezcan en un área más global para ese mismo idioma.

A partir de ahí, el análisis de cada dataset extraerá los siguientes términos:

Menciones::
Se considerán menciones todos aquellos términos que comienzan con el literal `@`. En Twitter, se utilizan para hacer referencia a otro usuario en el contenido que se está publicando.

Hashtags::
Un hashtag es un término que comienza con el literal `#` y sirve para categorizar el contenido de un tuit. Un ejemplo claro es durante los partidos de fútbol del Fútbol Club Barcelona, donde los aficionados que se encuentran comentando el partido en Twitter, suelen acompañar cada publicación con el hashtag `#fcblive` de manera que clasifican manualmente el contenido de su tuit para poder ser agrupado en una misma conversación.

Bigramas:: En este proyecto, hemos considerado como bigramas todas aquellas combinaciones de 2 palabras que se puedan hacer con el contenido de un tuit. Al contrario que en algunos artículos de investigación anteriores donde sólo se consideran términos consecutivos, en este caso hemos realizado todas las combinaciones posibles para cada tuit.
+
Algunas consideraciones importantes sobre esto son:
+
* Se han eliminado todos aquellos bigramas que contienen 2 veces la misma palabra.
* Se han eliminado todos aquellos bigramas que contienen al menos una palabra vacía.
* Se han elminado todos aquellos bigramas con términos inferiores a 2 caracteres.
* Se han ordenado alfabéticamente todos los bigramas de acuerdo a las 2 palabras que contienen, facilitando así el control de bigramas repetidos.
+
Un ejemplo del tipo de bigramas que sacaríamos de analizar un tuit en nuestro proyecto sería:
+
____
Buenos días vamos a trabajar todo el día
____
+
Que generaría las siguientes combinaciones
+
----
(buenos días), (buenos vamos), (buenos trabajar), (buenos todo), (buenos día), (días vamos), (días trabajar), (días todo), (día días), (trabajar vamos), (todo vamos), (día vamos), (todo trabajar), (día trabajar), (día todo)
----
+
Como se puede observar, la generación de bigramas para cada tuit provoca una explosión de términos que fue necesario controlar (explicado en <<_utilización_de_algoritmos_de_poda>>) para evitar sobrepasar la memoria del sistema.

Keywords::

Las keywords son unigramas formandos, obviamente, por un único término, cuyo resultado se asemeja a realizar una tokenización sobre el tuit pero aplicando reglas que también se utilizaban en la extracción de bigramas (palabra vacía, longitud inferior a 2 caracteres, etc.).

Keywords en el campo de Localización::

Son el resultado de aplicar la extracción anterior sobre el campo de Localización del perfil del usuario.

==== Sistema de filtros

Con el objetivo de poder realizar las extracciones de los términos anteriores de forma flexible, se diseñó un pequeño sistema de filtros que ayudara a combinar varios filtros en una misma ejecución. La implementación de este sistema está basada en el patrón de diseño Decoratorfootnote:[http://en.wikipedia.org/wiki/Decorator_pattern], aunque con la diferencia de que en este caso, la extracción de cada filtro se realiza sobre el tuit original y no sobre el resultado de las extracciones de filtros anteriores (una «decoración» incremental no tendría sentido dado el dominio del problema).

.Representación del patrón Decorator que ilustra el sistema de filtros
image::appendixes/extractor-filter.png[Representación del patrón Decorator que ilustra el sistema de filtros, align="center"]

==== Utilización de algoritmos de poda
